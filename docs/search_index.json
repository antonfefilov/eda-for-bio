[
["index.html", "APS 135: Introduction to Exploratory Data Analysis with R Information and overview Aims How to use the book Getting help Updates", " APS 135: Introduction to Exploratory Data Analysis with R Dylan Z. Childs 2018-02-12 Information and overview This is the online course book for the Introduction to Exploratory Data Analysis with R component of (APS 135) module. You can view this book in any modern desktop browser, as well as on your phone or tablet device. The site is self-contained—it contains all the material you are expected to learn this year. Bethan Hindle is running the course this year. Please email her if you spot any problems. Aims You will be introduced to the R ecosystem. R is now very widely used by biologists and environmental scientists to access data, carry out interactive data analysis, build mathematical models and produce high quality figures. We will teach you a little basic R programming so that you are in a position to address these needs in future if you need to. You don’t have to become an expert programmer to have a successful career in science, but knowing a little bit about programming has become (almost) a prerequisite for doing biological research in the 21st century. You will learn how to begin using R to carry out data manipulation and visualisation. Designing good experiments, collecting data, and analysis are hard, and these activities often takes a great deal time and money. If you want to effectively communicate your hard-won, latest, greatest results, it is difficult to beat a good figure or diagram (conversely, if you want to be ignored, put everything into a boring table). R is really good at producing figures, so even if you end up just using it as a platform for visualising data, your time hasn’t been wasted. This book provides a foundation for learning statistics later on. If you want to be a biologist, particularly one involved in research, there is really no way to avoid using statistics. You might be able to dodge it by becoming a theoretician, but if that really is your primary interest you should probably being studying for a mathematics degree. For the rest of us who collect data, or at least analyse other people’s data, knowing about statistics is essential: it allows us to distinguish between real patterns (the “signal”) and chance variation (the “noise”). Topics The topics we will cover in this book are divided into three sections: The Getting Started with R block introduces the R language and the RStudio environment for working with R. We aim to run through much of what you need to know to start using R to improve your productivity. This includes some basic terminology, how to use R packages, and how to access help. As noted earlier, we are not trying to turn you into an expert programmer. That takes too long (not everyone enjoys programming, though many of you may be surprised to discover that you do in fact like it). By the end of this block you will know enough about R to begin learning the more practical material that follows. The Data Wrangling with R block aims to show you how to manipulate your data with R. The truth is that if you regularly work with data, a large amount of time will inevitably be spent getting data into the format you need. The informal name for this is “data wrangling”. This is a topic that is not often taught well to undergraduates, which is a shame, because mastering the art of data wrangling saves you a lot of time in the long run. This block will briefly cover two R packages to help you do this: dplyr and tidyr. We’ll learn how to get data into and out of R, makes subsets of important variables, create new variables, summarise your data, and so on. The Exploratory Data Analysis block is all about using R to help you understand and describe your data. The first step in any analysis after you have managed to wrangle the data into shape almost always involves some kind of visualisation and/or numerical summary (or at least that should be the next step if you are serious about getting your analysis right). In this block you will learn how to do this using one of the best plotting systems in R: ggplot2. We will review the different kinds of variables you might have to analyse, discuss the different ways you can describe them, both visually and with numbers, and learn how to explore relationships between variables. How to use the book This book covers all the material you need to get to grips with this year, some of which we will not have time to cover in the practicals. No one is expecting you to memorise everything in the book. It is designed to serve as a resource for you to refer to over the next 2-3 years (and beyond) as needed. However, you should aim to familiarise yourself with the content so that you know where to look for information or examples when needed. Try to understand the important concepts and then worry about the specific details. What should you be doing as you read about each topic? There is a lot of R code embedded in the book, most of which you can just copy and paste into RStudio and then run. You are strongly encouraged to do this when you first work through a topic. The best way to learn something like R is to use it actively, not just read about it. Experimenting with different code snippets by changing them is also a very good way to learn what they do. You can’t really break R (well you can, but it is quite hard), and working out why something does or does not work will help you learn to use it. Text, instructions, and explanations Normal text, instructions, explanations etc. are written in the same type as this document, we will tend to use bold for emphasis and italics to highlight specific technical terms when they are first introduced (italics will also crop up with Latin names from time to time, but this is unlikely to produce too much confusion!) At various points in the text you will come across text in different coloured boxes. These are designed to highlight stand-alone exercises or little pieces of supplementary information that might otherwise break the flow. There are three different kinds of boxes: This is an action box. We use these when we want to say something important. For example, we might be summarising a key learning outcome or giving you instructions to do something. This is a warning box. These contain a warning or a common “gotcha”. There are a number of common pitfalls that trip up new users of R. These boxes aim to highlight these and show you how to avoid them. It’s a good idea to pay attention to these. This is an information box. These aim to offer a not-too-technical discussion of how or why something works the way it does. You do not have to understand everything in these boxes to use R, but the information will help you understand how it works. R code and output in this book We will try to illustrate as many ideas as we can using snippets of real R code. Stand alone snippets will be formatted like this: tmp &lt;- 1 print(tmp) ## [1] 1 At this point it does not matter what the above actually means. You just need to understand how the formatting of R code in this book works. The lines that start with ## show us what R prints to the screen after it evaluates an instruction and does whatever was asked of it, that is, they show the output. The lines that do not start with ## show us the instructions, that is, they show us the input. So remember, the absence of ## shows us what we are asking R to do, otherwise we are looking at something R prints in response to these instructions. This typeface is used to distinguish R code within a sentence of text: e.g. “We use the mutate function to change or add new variables.” A sequence of selections from an RStudio menu is indicated as follows: e.g. File ▶ New File ▶ R Script File names referred to in general text are given in upper case in the normal typeface: e.g. MYFILE.CSV. Getting help You will learn various ways of finding help about R in this book. If you find yourself stuck at any point these should your first port of call. If you are still struggling, try the following, in this order: Google is your friend. One of the nice consequences of R’s growing popularity and the rise of blogging – take a look at R Bloggers for a flavour of R-specific blogs – is that the web is now packed full of useful tutorials and tips, many of which are aimed at beginners. One of the objectives of this book is turn you into a self sufficient useR. Learning how to solve your own R-related problems is an essential pre-requisite for this to happen. Solving your own problems will also help you learn how to use R more effectively. If an hour of Googling does not solve a problem, post a question on the APS 135 Facebook page. If you find something difficult, the chances are that someone else finds it difficult too. You are strongly encouraged to try to address one anothers’ problems via this page. Thinking through and explaining the answer to a question someone else has posed is a really good way of learning. Dylan will check the Facebook page from time to time, and will offer a solution if no-one else has suggested one. We would much prefer you to help each other though. We encourage you to try options 1 and 2 first. Nonetheless, on occasion Google may turn out not to be your friend and a post to the Facebook page might not elicit a satisfactory response. In these instances you are welcome to email Bethan with your query. You are unlikely to receive an answer at the weekend though. Updates We may occasionally decide to update the book in light of the results of comments and questions we receive from you. This is another reason why it is important for you to ask or post questions—it allows us to see where people are struggling. It is also a motivation for choosing to use a website rather than a static document—we can very easily adapt or extend the content to address problems as they arise. If we do update the book, we will let you know what has changed. "],
["get-up-and-running-with-r-and-rstudio.html", "Get up and running with R and RStudio What is R? What is RStudio (and why use it)? Working at the Console", " Get up and running with R and RStudio What is R? The answer to this question very much depends on who we ask. The geeky answer is something like this… R is a dialect of the S language, which was developed by John Chambers and colleagues at Bell Laboratories in the mid 1970s. It was designed to offer an interactive computing environment for statisticians and scientists to carry out data analysis. There are essentially two widely used versions of S (though others have started to appear), a commercial one called S-Plus, and the open source implementation known as R. S-Plus came first, and although it is still around, it is used less each year. Development of R was begun in the late 1990s by two academics, Ross Ihaka and Robert Gentleman, at the University of Auckland. Their motivation was to create an open source language to enable researchers in computational statistics to explore new ideas. That language quickly evolved into something that looked more and more S-like, which we now know as R (GNU R, to be overly precise). We could go on and on about the various features that R possesses. R is a functional programming language, it supports object orientation, etc etc… but these kinds of explanations are only helpful to someone who already knows about computer languages. It is useful to understand why so many people have turned to R to meet their data analysis needs. When a typical R user talks about “R” they are often referring to two things at once, the GNU R language and the ecosystem that exists around the language: R is all about data analysis. We can carry out any standard statistical analysis in R, as well as access a huge array of more sophisticated tools with impressive names like “structural equation model”, “random forests” and “penalized regression”. These days, when statisticians and computer scientists develop a new analysis tool, they often implement it in R first. This means a competent R user can always access the latest, cutting edge analysis tools. R also has the best graphics and plotting facilities of any platform. With sufficient expertise, we can make pretty much any type of figure we need (e.g. scatter plots, phylogenetic trees, spatial maps, or even volcanoes). In short, R is a very productive environment for doing data analysis. Because R is such a good environment for data analysis, a very large community of users has grown up around it. The size of this community has increased steadily since R was created, but this growth has really increased up in the last 5-10 years or so. In the early 2000s there were very few books about R and the main way to access help online was through the widely-feared R mailing lists. Now, there are probably hundreds of books about different aspects of R, online tutorials written by enthusiasts, and many websites that exist solely to help people learn R. The resulting ecosystem is vast, and though it can be difficult to navigate at times, when we run into an R-related problem the chances are that the answer is already written down somewhere1. R is not just about data analysis—though we will mostly use it this way. It is a fully-fledged programming language, meaning that once you become moderately proficient with it you can do things such as construct numerical simulation models, solve equations, query websites, send emails, access the foaas web service, and carry out many other tasks we don’t have time to write down. We won’t do any of this year or next but it is worth noting that R can do much more than just analyse data if we need it to. Getting and installing R R is open source, meaning anyone can download the source code – the collection of computer instructions that define R – and assuming they have enough time, energy and expertise, they are free to alter it as they please. Open source does not necessarily mean free, as in it costs £0 to download and use, but luckily R is free in this sense. If you’re working on the University managed desktops it should already have been installed and is ready for you to use. We encourage you to install a copy on your own laptop so that you can work at home, in the library, at a café, or wherever else you find you are productive. Do not use R on its own though. Use it in combination with the RStudio IDE discussed in the next section. In order to install R you need to download the appropriate installer from the Comprehensive R Archive Network (CRAN). We are going to use the “base distribution” as this contains everything you need to use R under normal circumstances. There is a single installer for Windows. On a Mac, it’s important to match the installer to the version of OS X. In either case, R uses a the standard install mechanism that should be familiar to anyone who has installed an application on their machine. There is no need to change the default settings—doing so will probably lead to problems later on. Go ahead and install R on your own computer now. You won’t be able to make much use of this book without it. After installing R it should be visible in the Programs menu on a Windows computer or in the Applications folder on a Mac. However, it would be a good idea to read the next section before launching R… What is RStudio (and why use it)? R and RStudio are not the same thing. We can run R without RStudio if we need to, but we cannot run RStudio without R. Remember that! R is essentially just a computer program that sits there and waits for instructions in the form of text. Those instructions can be typed in by a user like you or me, or they can be sent to it from another program. This means you can run R in a variety of different environments. The job of RStudio is to provide an environment that makes R a more pleasant and productive tool. One way to get a sense of why RStudio is a Very Good Thing is to look at what running R without it is like. The simplest way to run it on a Linux or Unix-based machine (like a Mac) is to use something called the Terminal. It’s well beyond the scope of this book to get into what this is, but in a nutshell, the Terminal provides a low-level, text-based way to interact with a computer. Here is what R looks like running inside a Terminal on a Mac: We can run R in much the same way on Windows using the “Command Prompt” if we need to. The key thing you need to take away from that screenshot is that running R like this is very “bare bones”. We typed the letter “R” in the Terminal and hit Enter to start R. It printed a little information as it started up and then presented us with “the prompt” (&gt;), waiting for input. This is where we type or paste in instructions telling R what to do. There is no other way to interact with it when we run R like this – no menus or buttons, just a lonely prompt. The developers of R on Windows PCs and Macs provide a slightly nicer way to work with R. When we download and install R for either of these two operating systems, in addition to the basic R program that we just saw running in a Terminal, we also get another program that acts as a Graphical User Interface (GUI) for R. This is the thing labelled “R” in the Programs menu on a Windows computer or the Applications folder on a Mac. If you launch the R GUI on your computer you will be presented with roughly the same thing on either a Windows PC or a Mac. There will be something called the Console, which is where you interact directly with R by typing things at the prompt (which looks like this: &gt;), and a few buttons and menus for managing common tasks. We will not go through these two GUIs in any more detail because we are not going to use them. We just need to know they exist so we don’t confuse them with RStudio. So what is RStudio? The first thing to note is that it is a different program from R. Remember that! RStudio is installed installed separately from R and occupies its own place in the Programs menu (Windows PC) or Applications folder (Mac). In one sense RStudio is just another Graphical User Interface for R which improves on the “bare bones” experience. However, it is a GUI on steroids. It is more accurate to describe it as an Integrated Development Environment (IDE). There is no all-encompassing definition of an IDE, but they all exist to make programmer’s lives easier by integrating various useful tools into a single piece of software. From the perspective of this book, there are four key features that we care about: The R interpreter—the thing that was running in the Terminal above—runs inside RStudio. It’s accessed via a window labelled Console. This is where we type in instructions we want to execute when we are working directly with R. The Console also shows us any output that R prints in response to these instructions. So if we just want the “bare bones” experience, we can still have it. RStudio provides facilities for working with R programs using something called a Source Code Editor. An R program ( also called a “script”)&quot; is just is a collection of instructions in the R language that have been saved to a text file. Nothing more! However, it is much easier to work with a script using a proper Source Code Editor than an ordinary text editor like Notepad. An good IDE like RStudio also gives you a visual, point-and-click means of accessing various language-specific features. This is a bit difficult to explain until we have have actually used some of these, but trust us, being able to do things like manage packages, set working directories, or inspect objects we’ve made simplifies day-to-day use of R. This especially true for new users. RStudio is cross-platform—it will run on a Windows PC, a Linux PC or a Mac. In terms of the appearance and the functionality it provides, RStudio is exactly the same on each of these platforms. If we learn to work with R via RStudio on a Windows PC, it’s no problem migrating to a Mac or Linux PC later on if we need to. This is a big advantage for those of us who work on multiple platforms. We’re only going to scratch the surface of what RStudio can do and there are certainly alternative bits of software that could meet our immediate needs. The reason for introducing a powerful tool like RStudio is because one day you may need to access things like debugging facilities, package building tools, repository management. RStudio makes it easy to use these advanced tools. Getting and installing RStudio RStudio is developed and maintained by a for-profit company called… RStudio. They make their money by selling software tools and services related to R and RStudio. The basic desktop version of RStudio is free to download and use though. It can be downloaded from the RStudio download page. The one to go for is the Open Source Edition of RStudio Desktop, not the commercial version of RStudio Desktop. RStudio installs like any other piece of software, so there’s nothing to configure after installation. If you haven’t already done it, go ahead and install RStudio Desktop on your own computer. You are going to need it. The anatomy of RStudio Once it’s installed RStudio is run like any other stand-alone application, via the Programs menu or the Applications folder on a Windows PC or Mac, respectively2. We’ll say this one last time—RStudio only works if we’ve also installed R. Here is how RStudio appears the first time it runs: There are three panes inside a single window, which we have labelled with red numbers. Each of these has a well-defined purpose. Let’s take a quick look at these: The large window on the left is the Console. We have already told you what this is for—the Console lets you know what R is doing and provides a mechanism to interact with R by typing instructions. All this happens at the prompt, &gt;. We will be working in the Console in a moment so won’t say any more about this here. The window at the top right contains two tabs. The first of these, labelled Environment, allows us to see all the different R objects we can access. There are also some buttons that help us to get data into and out of R. The second, labelled History, allows us to see a list of instructions we’ve previously sent to R. The buttons in this tab allow us to reuse or save these instructions. The window at the bottom right contains five tabs. The first, labelled Files, gives us a way to interact with the files and folders. The next tab, labelled Plots, is where any figures we produce are displayed. This tab also allows you to save your figures to file. The Packages tab is where we view, install and update packages used to extend the functionality of R. The Help tab is where you can access and display various different help pages. The Viewer is essentially an embedded web browser for working with interactive output—we won’t be using it in this course. Don’t be alarmed if RStudio looks different on your computer. There are a couple of reasons why this might be the case. First, the appearance of RStudio is highly customisable. Take a quick look at the Tools &gt; Global Options... window to see what we mean. Second, there is a fourth window that is sometimes be visible when we work with RStudio—the source code Editor we mentioned above. RStudio saves its state between different sessions, so if we have already messed about with RStudio’s appearance or left a script open last time we used it you will see these changes. RStudio will change over time Keep in mind that RStudio is very actively developed, which means features tend to appear or change over time. Consequently, if you update it regularly expect the odd thing to change here and there. This is generally a good thing—it usually means new features have been added—but it does require you to occasionally adjust to new additions. Working at the Console R was designed to be used interactively—it is what is known as an interpreted language, which we can interact with via something called a Command Line Interface (CLI). This is just a fancy way of saying that we can type an instructions to “do something” directly into the Console and those instructions will then be interpreted when we hit the Enter key. If our R expression does not contain any errors, R will then do something like read in some data, perform a calculation, make a figure, and so on. What actually happens obviously depends on what we ask it to do. Let’s briefly see what all this means by doing something very simple with R. Type 1 + 3 at the Console and hit the Enter key: 1+3 ## [1] 4 The first line above just reminds us what we typed into the Console. The line after that beginning with ## shows us what R printed to the Console after reading and evaluating our instructions. What just happened? We can ignore the [1] bit for now (the meaning of this will become clear later in the course). What are we left with – the number 2. The instruction we gave R was in effect “evaluate the expression 1 + 3”. R read this in, decided it was a valid R expression, evaluated the expression, and then printed the result to the Console for us. Unsurprisingly, the expression 1 + 3 is a request to add the numbers 1 and 3, and so R prints the number 4 to the Console. OK, that was not very exciting. In the next chapter we will start learning to use R to carry out more useful calculations. The important take-away from this is that this sequence of events—reading instructions, evaluating those instructions and printing their output—happens every time we type or paste something into the Console and hit Enter. The printing bit is optional by the way. Whether or not it happens depends on whether you decide to capture the output or not. Just remember, if R does not print anything to the Console it does not necessarily mean nothing has happened. Why do we keep using that word expression? It has a very specific meaning in computer science. The Wikipedia page says: An expression in a programming language is a combination of explicit values, constants, variables, operators, and functions that are interpreted according to the particular rules of precedence and of association for a particular programming language, which computes and then produces another value. That probably doesn’t make much sense, but it at least demonstrates why we don’t let computer scientists teach biologists about programming. In simple terms, an R expression is a small set of instructions written in human readable(ish) text that tell R to do something. That’s it. We could write “instructions” instead of “expressions” throughout this book but we may as well use the correct word. Whatever we call them, our aim is to learn how to combine sequences of expressions to Get Things Done in R. That’s what this book is about. The other big change is that R is finally starting to become part of the commercial landscape—learning how to use it can only improve your job prospects.↩ If you use a computer running Linux we assume you know what you are doing when it comes to installing and running software.↩ "],
["a-quick-introduction-to-r.html", "Chapter 1 A quick introduction to R 1.1 Using R as a big calculator 1.2 Storing and reusing results 1.3 How does assignment work? 1.4 Global environment 1.5 Naming rules and conventions", " Chapter 1 A quick introduction to R 1.1 Using R as a big calculator 1.1.1 Basic arithmetic The end of the Get up and running with R and RStudio chapter demonstrated that R can handle familiar arithmetic operations: addition, subtraction, multiplication, division. If we want to add or subtract a pair of numbers just place the + or - symbol in between two numbers, hit Enter, and R will read the expression, evaluate it, and print the result to the Console. This works exactly as we expect it to: 3 + 2 ## [1] 5 5 - 1 ## [1] 4 Multiplication and division are no different, though we don’t use x or ÷ for these operations. Instead, we use * and / to multiply and divide: 7 * 2 ## [1] 14 3 / 2 ## [1] 1.5 We can also exponentiate a numbers: raise one number to the power of another. We use the ^ operator to do this: 4^2 ## [1] 16 This raises 4 to the power of 2 (i.e. we squared it). In general, we can raise a number x to the power of y using x^y. Neither x or y need to be a whole numbers either. Arithmetic operations can also be combined into one expression. Assume we want to subtract 6 from 23. The expression to perform this calculation is: 2^3 - 6 ## [1] 2 \\(2^3=8\\) and \\(8-6=2\\). Simple enough, but what if we had wanted to carry out a slightly longer calculation that required the last answer to then be divided by 2? This is the wrong the way to do it: 2^3 - 6 / 2 ## [1] 5 The answer we were looking for is \\(1\\). So what happened? R evaluated \\(6/2\\) first and then subtracted this answer from \\(2^3\\). If that’s obvious, great. If not, it’s time to learn a bit about the order of precendence used by R. R uses a standard set of rules to decide the order in which arithmetic calculations feed into one another so that it can unambiguously evaluate any expression. It uses the same order as every other computer language, which thankfully is the same one we all learned in mathematics class at school. The order of precedence used is: exponents and roots (“taking powers”) multiplication and division additional and subtraction BODMAS and friends If you find it difficult to remember order of precedence used by R, there are a load of mnemonics that can to help. Pick one you like and remember that instead. In order to get the answer we were looking for we need to take control of the order of evaluation. We do this by enclosing grouping the necessary bits of the calculation inside parentheses (“round brackets”). That is, we place ( and ) either side of them. The order in which expressions inside different pairs of parentheses are evaluated follows the rules we all had to learn at school. The R expression we should have used is therefore: (2^3 - 6) / 2 ## [1] 1 We can use more than one pair of parentheses to control the order of evaluation in more complex calculations. For example, if we want to find the cube root of 2 (i.e. 21/3) rather than 23 in that last calculation we would instead write: (2^(1/3) - 6) / 2 ## [1] -2.370039 The parentheses around the 1/3 in the exponent are needed to ensure this is evaluated prior to being used as the exponent. 1.1.2 Problematic calculations Now is a good time to highlight how R handles certain kinds of awkward numerical calculations. One of these involves division of a number by 0. Some programming languages will respond to an attempt to do this with an error. R is a bit more forgiving: 1/0 ## [1] Inf Mathematically, division of a finite number by 0 equals A Very Large Number: infinity. R has a special built in data value that allows it to handle this kind of thing. This is Inf, which of course stands for “infinity”. The other special kind of value we sometimes run into can be generated by numerical calculations that don’t have a well-defined result. For example, it arises when we try to divide 0 or infinity by themselves: 0/0 ## [1] NaN The NaN in this result stands for Not a Number. R produces NaN because \\(0/0\\) is not defined mathematically: it produces something that is Not a Number. The reason we are pointing out Inf and NaN is not because we expect to use them. It’s important to know what they represent because they often arise as a result of a mistake somewhere in a program. It’s hard to track down such mistakes if we don’t know how Inf and NaN arise. That is enough about using R as a calculator for now. What we’ve seen—even though we haven’t said it yet—is that R functions as a REPL: a read-eval-print loop (there’s no need to remember this term). R takes user input, evaluates it, prints the results, and then waits for the next input. This is handy, because it means we can use it interactively, working through an analysis line-by-line. However, to use R to solve for complex problems we need to learn how to store and reuse results. We’ll look at this in the next section. Working efficiently at the Console Working at the Console soon gets tedious if we have to retype similar things over and over again. There is no need to do this though. Place the cursor at the prompt and hit the up arrow. What happens? This brings back the last expression sent to R’s interpreter. Hit the up arrow again to see the last-but-one expression, and so on. We go back down the list using the down arrow. Once we’re at the line we need, we use the left and right arrows to move around the expression and the delete key to remove the parts we want to change. Once an expression has been edited like this we hit Enter to send it to R again. Try it! 1.2 Storing and reusing results So far we’ve not tried to do anything remotely complicated or interesting, though we now know how to construct longer calculations using parentheses to control the order of evaluation. This approach is fine if the calculation is very simple. It quickly becomes unwieldy for dealing with anything more. The best way to see what we mean is by working through a simple example—solving a quadratic equation. Quadratic equations looks like this: \\(a + bx + cx^2 = 0\\). If we know the values of \\(a\\), \\(b\\) and \\(c\\) then we can solve this equation to find the values of \\(x\\) that ensure the left hand side equals the right hand side. Here’s the well-known formula for these solutions: \\[ x = \\frac{-b\\pm\\sqrt{b^2-4ac}}{2a} \\] Let’s use R to calculate these solutions for us. Say that we want to find the solutions to the quadratic equation when \\(a=1\\), \\(b=6\\) and \\(c=5\\). We just have to turn the above equation into a pair of R expressions: (-6 + (6^2 -4 * 1 * 5)^(1/2)) / (2 * 1) ## [1] -1 (-6 - (6^2 -4 * 1 * 5)^(1/2)) / (2 * 1) ## [1] -5 The output tells us that the two values of \\(x\\) that satisfy this particular quadratic equation are -1 and -5. What should we do if we now need to solve a different quadratic equation? Working at the Console, we could bring up the expressions we typed (using the up arrow) and then go through each of these, changing the numbers to match the new values of \\(a\\), \\(b\\) and \\(c\\). Editing individual expressions like this is fairly tedious, and more importantly, it’s fairly error prone because we have to make sure we substitute the new numbers at exactly the right positions. A partial solution to this problem is to store the values of \\(a\\), \\(b\\) and \\(c\\). We’ll see precisely why this is useful in a moment. First, we need to learn how to store results in R. The key to this is to use the assigment operator, written as a left arrow &lt;-. Sticking with our original example, we need to store the numbers 1, 6 and 5. We do this using three expressions, one after the another: a &lt;- 1 b &lt;- 6 c &lt;- 5 Notice that we don’t put a space between &lt; and -—R won’t like it if we try to add one. R didn’t print anything to screen, so what actually happened? We asked R to first evaluate the expression on the right hand side of each &lt;- (just a number in this case) and then assign the result of that evaluation instead of printing it. Each result has a name associated with it, which appears on the left hand side of the &lt;-. RStudio shortcut We use the assignment operator &lt;- all the time when working with R, and because it’s inefficient to have to type the &lt; and - characters over and over again, RStudio has a built in shortcut for typing the assignment operator: Alt + - . Try it. Move the curser to the Console, hold down the Alt key (‘Option’ on a Mac), and press the - sign key. RStudio will auto-magically add insert &lt;-. The net result of all this is that we have stored the numbers 1, 6 and 5 somewhere in R, associating them with the letters a, b and c, respectively. What does this mean? Here’s what happens if we type the letter a into the Console and hit Enter: a ## [1] 1 It looks the same as if we had typed the number 1 directly into the Console. The result of typing b or c is hopefully obvious. What we just did was to store the output that results from evaluating three separate R expressions, associating each a name so that we can access them again3. Whenever we use the assignment operator &lt;- we are telling R to keep whatever kind of value results from the calculation on the right hand side of &lt;-, giving it the name on the left hand side so that we can access it later. Why is this useful? Let’s imagine we want to do more than one thing with our three numbers. If we want to know their sum or their product we can now use: a + b + c ## [1] 12 a * b * c ## [1] 30 So once we’ve stored a result and associated it with a name we can reuse it wherever it’s needed. Returning to our motivating example, we can now calculate the solutions to the quadratic equation by typing these two expressions into the Console: (-b + (b^2 -4 * a * c)^(1/2)) / (2 * a) ## [1] -1 (-b - (b^2 -4 * a * c)^(1/2)) / (2 * a) ## [1] -5 Imagine we’d like to find the solutions to a different quadratic equation where \\(a=1\\), \\(b=5\\) and \\(c=5\\). We just changed the value of \\(b\\) here to keep things simple. To find our new solutions we have to do two things. First we change the value of the number associated with b… b &lt;- 5 …then we bring up those lines that calculate the solutions to the quadratic equation and run them, one after the other: (-b + (b^2 -4 * a * c)^(1/2)) / (2 * a) ## [1] -1.381966 (-b - (b^2 -4 * a * c)^(1/2)) / (2 * a) ## [1] -3.618034 We didn’t have to retype those two expressions. We could just use the up arrow to bring each one back to the prompt and hit Enter. This is much simpler than editing the expressions. More importantly, we are beginning to see the benefits of using something like R: we can break down complex calculations into a series of steps, storing and reusing intermediate results as required. 1.3 How does assignment work? It’s important to understand, at least roughly, how assignment works. The first thing to note is that when we use the assignment operator &lt;- to associate names and values, we informally refer to this as creating (or modifying) a variable. This is much less tedious than using words like “bind”, “associate”, value“, and”name&quot; all the time. Why is it called a variable? What happens when we run these lines: myvar &lt;- 1 myvar &lt;- 7 The first time we used &lt;- with myvar on the left hand side we created a variable myvar associated with the value 1. The second line myvar &lt;- 7 modified the value of myvar to be 7. This is why we refer to myvar as a variable: we can change the its value as we please. What happened to the old value associated with myvar? In short, it is gone, kaput, lost… forever. The moment we assign a new value to myvar the old one is destroyed and can no longer be accessed. Remember this. Keep in mind that the expression on the right hand side of &lt;- can be any kind of calculation, not just just a number. For example, if I want to store the number 1, associating it with answer, I could do this: answer &lt;- (1 + 2^3) / (2 + 7) That is a strange way to assign the number 1, but it illustrates the point. More generally, as along as the expression on the right hand side generates an output it can be used with the assignment operator. For example, we can create new variables from old variables: newvar &lt;- 2 * answer What happened here? Start at the right hand side of &lt;-. The expression on this side contained the variable answer so R went to see if answer actually exists in the global environment. It does, so it then substituted the value associated with answer into the requested calculation, and then assigned the resulting value of 2 to newvar. We created a new variable newvar using information associated with answer. Now look at what happens if we just copy a variable using the assignment operator: myvar &lt;- 7 mycopy &lt;- myvar At this point we have two variables, myvar and mycopy, each associated with the number 7. There is something very important going on here: each of these is associated with a different copy of this number. If we change the value associated with one of these variables it does not change the value of the other, as this shows: myvar &lt;- 10 myvar ## [1] 10 mycopy ## [1] 7 R always behaves like this unless we work hard to alter this behaviour (we never do this in this book). So remember, every time we assign one variable to another, we actually make a completely new, independent copy of its associated value. For our purposes this is a good thing because it makes it much easier to understand what a long sequence of R expressions will do. That probably doesn’t seem like an obvious or important point, but trust us, it is. 1.4 Global environment Whenever we associate a name with a value we create a copy of both these things somewhere in the computer’s memory. In R the “somewhere” is called an environment. We aren’t going to get into a discussion of R’s many different kinds of environments—that’s an advanced topic well beyond the scope of this book. The one environment we do need to be aware of though is the Global Environment. Whenever we perform an assignment in the Console the name-value pair we create (i.e. the variable) is placed into the Global Environment. The current set of variables are all listed in the Environment tab in RStudio. Take a look. Assuming that at least one variable has been made, there will be two columns in the Environment tab. The first shows us the names of all the variables, while the second summarises their values. The Global Environment is temporary By default, R will save the Global Environment whenever we close it down and then restore it in the next R session. It does this by writing a copy of the Global Environment to disk. In theory this means we can close down R, reopen it, and pick things up from where we left off. Don’t do this—it only increases the risk of making a serious mistake. Assume that when R and RStudio are shut down, everything in Global Environment will be lost. 1.5 Naming rules and conventions We don’t have to use a single letter to name things in R. The words tom, dick and harry could be used in place of a, b and c. It might be confusing to use them, but tom, dick and harry are all legal names as far as to R is concerned: A legal name in R is any sequence of letters, numbers, ., or _, but the sequence of characters we use must begin with a letter. Both upper and lower case letters are allowed. For example, num_1, num.1, num1, NUM1, myNum1 are all legal names, but 1num and _num1 are not because they begin with 1 and _. R is case sensitive—it treats upper and lower case letters as different characters. This means that num and Num are treated as distinct names. Forgetting about case sensitivity is a good way to create errors when using R. Try to remember that. Don’t begin a name with . We are allowed to begin a name with a ., but this usually is A Bad Idea. Why? Because variable names that begin with . are hidden from view in the Global Environment—the value it refers to exists but it’s invisible. This behaviour exists to allow R to create invisible variables that control how it behaves. This is useful, but it isn’t really meant to be used by the average user. Technically, this is called binding the name to a value. You really don’t need to remember this though.↩ "],
["scripts.html", "Chapter 2 Building scripts 2.1 Introduction 2.2 Writing scripts in RStudio 2.3 Running scripts in RStudio 2.4 Spotting problems", " Chapter 2 Building scripts 2.1 Introduction We’ve seen that using variables is useful because it enables us to break down a problem into a series of simpler steps. However, so far we’ve only been working in the Console. If we want to reuse a calculation when we’re working like this, we have to change a variable or two and then evaluate the expressions that do the job of solving an equation, make a graph, whatever. We also have to do all of this in the correct order, or things will not work as intended. We can see that working in the Console is not going to be practical most of the time. So what should we do? The answer is: put our sequence of R expressions into a text file, called a script. Calling it a script makes it sound a bit fancy and clever—“I spent all day debugging my script”. It is not. It is a boring text file that could be opened up in something like Notepad.exe. We just call it a script to signify the fact that the text contained in the file is a series of instructions telling our computer to do something. Working directly at the Console is the simplest way to use R, but we do not recommend working this way unless you only need to do something very simple that involves a handful of steps. For more complicated activities you should always store your instructions in a script. 2.2 Writing scripts in RStudio To open a new script in RStudio navigate to File &gt; New File &gt; R Script. This will open the new file in a fourth pane. This pane is the Source Code Editor we mentioned in the Get up and running with R and RStudio chapter. The name of the tab where this new file lives will be set to Untitled1 if we haven’t opened any other new R Scripts. Here is what RStudio looks like after we do this (we’ve highlighted the new pane with a red asterisk): When we work with a script we type the required sequence of R expressions into the Editor pane, not directly into the Console. This is important—if we mix and match mistakes happen. The worst of these is that we write a script that seems to work, only to find it is broken when we open it up and use it again later. This usually happens because we typed something into the Console that is needed to make the whole script run when we were preparing it, but then forget to put it into the script. Just don’t switch between using the Console and the Editor to avoid this. The easiest way to appreciate the benefits of using a script is to work with one. Here are a few lines of R code to copy-paste into the new Editor pane… a &lt;- 1 b &lt;- 6 c &lt;- 5 sqrt.b2ac &lt;- (b^2 -4 * a * c)^(1/2) (-b + sqrt.b2ac) / (2 * a) (-b - sqrt.b2ac) / (2 * a) …and here’s a partial screenshot of how the Editor pane might look (the details depend on how RStudio is set up): Notice that parts of the R code is formatted by colour. This is called syntax highlighting. Syntax highlighting is a must have feature of any Editor. In a nutshell, syntax is a bit like the grammar of a computer language. It is the set of rules that determine how we form valid expressions, assign variables, and so on. The purpose of syntax highlighting is to draw attention to different components of syntax. We can see that when we use the Cobalt highlighting option, RStudio sets the background to black and displays variables in white, parentheses and arithmetic operators in orange, and numbers in red. It doesn’t matter so much what the colours are. What matters is that we have a visual means to distinguish these different kinds of elements, making it much easier to read a script. Choose your own colour scheme The first thing you will probably notice is that this Editor looks a little different from yours. We said earlier that RStudio was highly customisable. What we did above was change the way it does something called syntax highlighting. You can do this by navigating to Tools &gt; Global Options…, selecting the Appearance button, and picking the Cobalt option under Editor theme (try it!). The other kind of elements RStudio has highlighted are in blue. We added these. They are called comments. Comments in R always start with a # symbol—this is called the “hash”&quot; symbol (also known as the ‘pound’ symbol to North Americans of a certain age). Lines that start with # are completely ignored by R. They exist only to allow us, the developers of a script, to add notes and explanations that remind us how it all works. Comments are important At this point we just want to emphasise that you should always use comments in your scripts to remind yourself what your R code is supposed to be doing. Use them liberally to help you understand the logic of each script you write. This is another “take our word”&quot; for it situation – if you do not use comments, then when you come back to your precious script in a few weeks/months/years time you will have no idea what it does. 2.3 Running scripts in RStudio The whole point of writing a script is ultimately to run it. The phrase “run our code” is shorthand for “send a number of R expressions to the R interpreter to be read and evaluated”. The latter is tedious to write (and read) over and over again, so we will just write “run your/my/our code”. We could run the code in the above script by copying and pasting it into the Console, but this is inefficient. Instead of relying on cut and paste, RStudio gives us different ways to run our code: There is a Run button at the top right of the Editor pane. As we might imagine, clicking on this will run some code. If we haven’t highlighted anything in the Editor, this runs whichever line the cursor is at, i.e. it runs just that one line. If we had highlighted a region inside the Editor, this button will run all of that in sequence. No one likes clicking buttons. Luckily, pressing Control+Enter (or Command+Enter on a Mac) does exactly the same thing as the Run button. It also uses the same rules to decide which bits of code to run or not4. Now that we know how to ‘run our code’, we can run every line in the script we just started. Here’s what should happen at the Console when we do this: a &lt;- 1 b &lt;- 6 c &lt;- 5 sqrt.b2ac &lt;- (b^2 -4 * a * c)^(1/2) (-b + sqrt.b2ac) / (2 * a) ## [1] -1 (-b - sqrt.b2ac) / (2 * a) ## [1] -5 This works exactly as though we typed or pasted the sequence of R expressions into the Console, hitting Enter each time we get to the end of a line. What this means is that we can use this script to find the solutions to any quadratic equation with ‘real roots’. All we have to do is edit the values assigned to a, b and c and then rerun the whole script. We can’t just rerun bits of it because everything is designed to work together, in sequence. Now that we have a script that does something a little bit useful we might wish to reuse it at some point. It’s just a text file, so we can save the script as we would any other file. We can do this using the familiar menu-based approach (File &gt; Save As...) or via the keyboard shortcut Control+S (or Command+S on a Mac). The only thing to keep in mind is that we must use the file extension .R or .r when we name the file, e.g. my_great_script.R. This is because RStudio uses the file extension to detect the fact that a file is an R script and not an ordinary text file. If we don’t do this, then next time we open up the file in RStudio we won’t be able to access the fancy Editor features like syntax highlighting, nor will we be able to send lines to the Console without using copy-paste. From now on always work with scripts. No more typing into the Console! 2.4 Spotting problems We may as well get something out of the way early on. It’s painfully easy to accidentally ask R to do something that contains an error of some kind. Mistakes happen all the time when writing R code—everyone does it. It’s not a problem when this happens. When it does though, it’s important to step back and work out what went wrong. 2.4.1 The dreaded + Be careful when highlighting code to run. RStudio will run exactly the text that is highlighted. If we start or finish the highlighted region in the middle of an expression then one of three things will usually happen. If we’re lucky we’ll generate an error because we ran an invalid partial expression. We say this is lucky because the error will at least be easy to spot. If we’re unlucky, we might end up running part of an expression that is itself a valid expression, but does not do what we had intended. This is harder to spot because it won’t generate an error, and it will probably create problems further down the line. The third outcome is that the Console will look something like this: What happened? Look carefully at the little snippet of R code we sent to the Console. It’s not a complete R expression, because it is missing a closing parenthesis: ). When R receives only part of an expression like this, which has correct syntax but is not complete, it sits and waits for the rest of the expression. This is what the + at the Console signifies. When we see this we have two options. We can manually type in the missing part of the expression and hit Enter, or (better) we can hit the Escape key to return you to the prompt &gt; and start again. The first option is rather error prone so we would generally prefer the latter. 2.4.2 Errors Here is an example of what happens at the Console when we generate an error: xyz + 2 ## Error in eval(expr, envir, enclos): object &#39;xyz&#39; not found In general terms, what happened is that R read in the instruction xyz + 2, tried to evaluate it, and found it could not. This is because the variable xyz does not exist, i.e. we never made a variable called xyz. Upon running into the error, R printed something to the screen to tell us we’ve made a mistake (“Error: object ‘xyz’ not found”). When this happens we say R has ‘thrown an error’. We know its an error because the message is in a warning colour (probably red or orange—it depends how RStudio is set up) and contains the word Error. The bit after the : is an attempt by R to tell us what went wrong. Always read the error messages. They will be incomprehensible at first, but they will eventually start to make more sense and become helpful (usually—sometimes they make no sense whatsoever, even to experienced users). This way of learning only works if we read the error messages in the first place though. We can also run lines of code via the Code &gt; Run Lines. This is easily the most inefficient method after cut and paste. It’s only there in case we forget our keyboard shortcut or cannot remember where the run button is.↩ "],
["using-functions.html", "Chapter 3 Using functions 3.1 Introduction 3.2 Functions and arguments 3.3 Evaluating arguments and returning results 3.4 Functions do not have “side effects” 3.5 Combining functions 3.6 Specifying function arguments", " Chapter 3 Using functions 3.1 Introduction Functions are a basic building block of any programming language. To use R effectively—even if our needs are very simple—we need to understand how to use functions. We are not aiming to unpick the inner workings of functions in this course5. The aim of this chapter is to explain what functions are for, how to use them, and how to avoid mistakes when doing so, without worrying too much about how they actually work. 3.2 Functions and arguments The job of each function in R is to carry out some kind of calculation or computation that would typically require many lines of R code to do “from scratch”. Functions allow us to reuse common computations while offering some control over the precise details of what actually happens. The best way to see what we mean by this is to see one in action. The round function is used to round one or more number(s) to a significant number of digits. To use it, we could type this into the Console and hit Enter: round(x = 3.141593, digits = 2) We have suppressed the output for now so that we can unpack things a bit first. Every time we use a function we always have to work with the same basic construct (there are a few exceptions, but we can ignore these for now). We start with the name of the function, that is, we use the name of the function as the prefix. In this case, the function name is round. After the function name, we need a pair of opening and closing parentheses. It is this combination of name and parentheses that that alerts R to fact that we are trying to use a function. Whenever we see name followed by opening and closing parentheses we’re seeing a function in action. What about the bits inside the parentheses? These are called the arguments of the function. That is a horrible name, but it is the one that everyone uses so we have to get used to it. Depending on how it was defined, a function can take zero, one, or more arguments. We will discuss this idea in more detail later in this section. In the simple example above, we used the round function with two arguments. Each of these was supplied as a name-value pair, separated by a comma. When working with arguments, name-value pairs occur either side of the equals (=) sign, with the name of the argument on the left hand side and the value it takes on the right hand side (notice that the syntax highlighter we used to make this website helpfully colours the argument names differently from the values). The name serves to identify which argument we are working with, and the value is the thing that controls what that argument does in the function. We refer to the process of associating argument names and values as “supplying the arguments” of the function (sometimes we also say “setting the arguments”). Notice the similarity between supplying function arguments and the assignment operation discussed in the last topic. The difference here is that name-value pairs are associated with the = symbol. This association is also temporary: it only lasts as long as it takes for the function to do whatever it does. Use = to assign arguments Do not use the assignment operator &lt;- inside the parentheses when working with functions. This is a “trust us” situation: you will end up in all kinds of difficulty if you do this. The names of the arguments that we are allowed to use are typically determined for us by the function. That is, we are not free to choose whatever name we like. We say “typically”, because R is a very flexible language and so there are certain exceptions to this simple rule of thumb. For now it is simpler to think of the names as constrained by the particular function we’re using. The arguments control the behaviour of a function. Our job as users is to set the values of these to get the behaviour we want. By now it is now probably fairly obvious what is going to happen when we used the round function like this at the Console: round(x = 3.141593, digits = 2) ## [1] 3.14 Remember, we said the round function rounds one or more numbers to a number of significant digits. The argument that specifies the focal number(s) is x; the second argument, digits, specifies the number of decimal places we require. Based on the supplied values of these arguments, 3.141593 and 2, respectively, the round function spits out a value of 3.14, which is then printed to the Console. If we had wanted to the answer to 3 significant digits we would use digits = 3. This is what we mean when we say the values of the supplied arguments controls the behaviour of the function. 3.3 Evaluating arguments and returning results Whenever R evaluates a function we refer to this action as “calling the function”. In our simple example, we called the round function with arguments x and digits (in this course we treat the phrases “use the function” and “call the function” as synonyms, as the former is more natural to new users). What we have just seen—although it may not be obvious—is that when we call functions they first evaluate their arguments, then perform some kind of action, and finally (optionally) return a value to us when they finish doing whatever it is they do We will discuss that word “return” in a moment. What do we mean by the word “evaluate” in this context? Take a look at this second example which uses round again: round(x = 2.3 + 1.4, digits = 0) ## [1] 4 When we call a function, what typically happens is that everything on the right hand side of an = is first evaluated, the result of this evaluation becomes associated with the corresponding argument name, and then the function does its calculations using the resulting name-value pairs. We say “typically” because other kinds of behaviours are possible—remember, R is a very flexible language—though for the purposes of this course we can assume that what we just wrote is always true. What happened above is that R evaluated 2.3 + 1.4, resulting in the number 3.7, which was then associated with the argument x. We set digits to 0 this time so that round just returns a whole number, 4. The important thing to realise is that the expression(s) on the right hand side of the = can be anything we like. This third example essentially equivalent to the last one: myvar &lt;- 2.3 + 1.4 round(x = myvar, digits = 0) ## [1] 4 This time we created a new variable called myvar and then supplied this as the value of the x argument. When we call the round function like this, the R interpreter spots the fact that something on the right hand side of an = is a variable and associates the value of this variable with x argument. As long as we have actually defined the numeric variable myvar at some point we can use it as the value of an argument. Keeping in mind what we’ve just learned, take a careful look at this example: x &lt;- 0 round(x = 3.7, digits = x) ## [1] 4 What is going on here? The key to understanding this is to realise that the symbol x is used in two different ways here. When it appears on the left hand side of the = it represents an argument name. When it appears on the right hand side it is treated as a variable name, which must have a value associated with it for the above to be valid. This is admittedly a slightly confusing way to use this function, but it is perfectly valid. The message here is that what matters is where things appear relative to the =, not the symbols used to represent them. We said at the beginning of this section that a function may optionally return a value to us when they finish complete their task. That word “return” is just jargon that refers to the process by which a function outputs a value. If we use a function at the Console this will be the value printed at the end. We can use this value in other ways too. For example, there is nothing to stop us combining function calls with the arithmetic operations: 2 * round(x = 2.64, digits = 0) ## [1] 6 Here the R interpreter first evaluates the function call, and then multiplies the value it returns by 2. If we want to reuse this value we have to assign the result of function call, for example: roundnum &lt;- 2 * round(x = 2.64, digits = 0) Using a function with &lt;- is really no different from the examples using multiple arithmetic operations in the last topic. The R interpreter starts on the right hand side of the &lt;-, evaluates the function call there, and only then assigns the value to roundnum. 3.4 Functions do not have “side effects” There is one more idea about functions and arguments that we really need to understand in order to avoid confusion later on. It relates to how functions modify their arguments, or more accurately, how they do not modify their arguments. Take a look at this example: myvar &lt;- 3.7 round(x = myvar, digits = 0) ## [1] 4 myvar ## [1] 3.7 We created a variable myvar with the value 3.7, rounded this to a whole number with round, and then printed the value of myvar. Notice that the value of myvar has not changed after using it as an argument to round. This is important. R functions typically do not alter the values of their arguments. Again, we say “typically” because there are ways to alter this behaviour if we really want to (yes, R is a very flexible language), but we will never ever do this. The standard behaviour—that functions do not alter their arguments—is what is meant by the phrase “functions do not have side effects”. If we had meant to round the value of myvar so that we can use this new value later on, we have to assign the result of function evaluation, like this: myvar &lt;- 3.7 myvar &lt;- round(x = myvar, digits = 0) In this example, we just overwrote the old value, but we could just as easily have created a new variable. The reason this is worth pointing out is that new users sometimes assume certain types of functions will alter their arguments. Specifically, when working with functions manipulate something called a data.frame, there is a tendency to assume that the function changes the data.frame argument. It will not. If we want to make use of the changes, rather than just see them printed to the Console, we need to assign the results. We can do this by creating a new variable or overwriting the old one. We’ll gain first hand experience of this in the Data Wrangling block. Remember, functions do not have side effects! New R users sometimes forget this and create all kinds of headaches for themselves. Don’t be that person. 3.5 Combining functions Up until now we have not tried to do anything very complicated in our examples. Using R to actually get useful work almost always involves multiple steps, very often facilitated by a number of different functions. There is more than one way to do this. Here’s a simple example that takes an approach we already know about: myvar &lt;- sqrt(x = 10) round(x = myvar, digits = 0) ## [1] 3 We calculated the square root of the number 10 and assigned the result to myvar, then we rounded this to a whole number and printed the result to the Console. So one way to use a series of functions in sequence is to assign a name to the result at each step and use this as an argument to the function in the next step. Here is another way to replicate the calculation in the previous example: round(x = sqrt(x = 10), digits = 0) ## [1] 3 The technical name for this is function composition. Another way of referring to this kind of expression is as a nested function call: we say that the sqrt function is nested inside the round function. The way to should read these constructs is from the inside out. The sqrt(x = 10) expression is on the right hand side of an = symbol, so this is evaluated first, the result is associated with the x argument of the round function, and only then does the round function do its job. There aren’t really any new ideas here. We have already seen that the R interpreter evaluates whatever is on the right hand side of the = symbol first before associating the resulting value with the appropriate argument name. However, nested function calls can be confusing at first so we need to see them in action. There’s nothing to stop us using multiple levels of nesting either. Take a look at this example: round(x = sqrt(x = abs(x = -10)), digits = 0) ## [1] 3 The abs function takes the absolute value of a number, i.e. removes the - sign if it is there. Remember, read nested calls from the inside out. In this example, first we took the absolute value of -10, then we took the square root of the resulting number (10), and then we rounded this to a whole number. Nested function calls are useful because they make our R code less verbose (we have to write less), but this comes at the cost of reduced readability. We aim to keep function nesting to a minimum in this book, but we will occasionally have to work with the nesting construct so have to understand it even if we don’t like using it. We’ll see a much-easier-to-read method for applying a series of functions in the Data Wrangling block. 3.6 Specifying function arguments We have only been working with functions that carry out mathematical calculations with numbers so far. We will see many more in this course as it unfolds. Some functions are designed to extract information about functions for us. For example, take a look at the args function: args(name = round) ## function (x, digits = 0) ## NULL We can see what args does: it prints a summary of the main arguments of a function to the Console (though it doesn’t always print all the available arguments). What can we learn from the summary of the round arguments? Notice that the first argument, x, is shown without an associated value, whereas the digits part of the summary is printed as digits = 0. The significance of this is that digits has a default value. This means that we can leave out digits when using the round function: round(x = 3.7) ## [1] 4 This is obviously the same result as we would get using round(x = 3.7, digits = 0). This is a very useful feature of R, as it allows us keep our R code concise. Some functions take a large number of arguments, many of which are defined with sensible defaults. Unless we need to change these default arguments, we can just ignore them when we call such functions. The x argument of round does not have a default, which means we have to supply a value. This is sensible, as the whole purpose of round is to round any number we give it. There is another way to simplify our use of functions. Take a look at this example: round(3.72, digits = 1) ## [1] 3.7 What does this demonstrate? We do not have to specify argument names, i.e. there is no need to specify argument names. In the absence of an argument name the R interpreter uses the position of the supplied argument to work out which name to associate it with. In this example we left out the name of the argument at position 1. This is where x belongs, so we end up rounding 3.71 to 1 decimal place. R is even more flexible than this, as it carries out partial matching on argument names: round(3.72, dig = 1) ## [1] 3.7 This also works because R can unambiguously match the argument I named dig to digits. Take note, if there were another argument to round that started with the letters dig this would have caused an error. We have to know our function arguments if we want to rely of partial matching. Be careful with your arguments Here is some advice. Do not rely on partial matching of function names. It just leads to confusion and the odd error. If you use it a lot you end up forgetting the true name of arguments, and if you abbreviate too much you create name matching conflicts. For example, if a function has arguments arg1 and arg2 and you use the partial name a for an argument, there is no way to know which argument you meant. We are pointing out partial matching so that you are aware of the behaviour. It is not worth the hassle of getting it wrong just to save on a little typing, so do not use it. What about position matching? This can also cause problems if we’re not paying attention. For example, if you forget the order of the arguments to a function and then place your arguments in the wrong place, you will either generate an error or produce a nonsensical result. It is nice not to have to type out the name = value construct all the time though, so our advice is to rely positional matching only for the first argument. This is a common convention in R, and it makes sense because it is often obvious what kind of information or data the first argument should carry, so the its name is redundant. At some point, if you really want to understand what happens when you use a function you will need to grapple with ideas like lazy evaluation, environments and scoping.↩ "],
["numeric-vectors.html", "Chapter 4 Numeric vectors 4.1 Introduction 4.2 Atomic vectors 4.3 Numeric vectors 4.4 Constructing numeric vectors 4.5 Named vectors 4.6 Generating sequences of numbers 4.7 Vectorised operations", " Chapter 4 Numeric vectors 4.1 Introduction The term “data structure” is computer science jargon for a particular way of organising data on our computers, so that it can be accessed easily and efficiently. Computer languages use many different kinds of data structures, but fortunately, we only need to learn about a couple of relatively simple ones to use R for data analysis. In fact, in this book only two kinds of data structure really matter: “vectors” and “data frames”. We’ll learn how to work with data frames in the next section of the book. The next three chapters will consider vectors. This chapter has two goals. First, we want to learn the basics of how to work with vectors. For example, we’ll see how “vectorised operations” may be used to express a repeated calculation. Second, we’ll learn how to construct and use numeric vectors to perform various calculations. Keep in mind that although we’re going to focus on numeric vectors, many of the principles we learn here can be applied to the other kinds of vectors considered later. 4.2 Atomic vectors A vector is a 1-dimensional object that contains a set of data values, which are accessible by their position: position 1, position 2, position 3, and so one. When people talk about vectors in R they’re often referring to atomic vectors6. An atomic vector is the simplest kind of data structure in R. There are a few different kinds of atomic vector, but the defining feature of each one is that it can only contain data of one type. An atomic vector might contain all integers (e.g. 2, 4, 6, …) or all characters (e.g. “A”, “B”, “C”), but it can’t mix and match integers and characters (e.g. “A”, 2, “C”, 5). The word “atomic” in the name refers to the fact that an atomic vector can’t be broken down into anything simpler—they are the simplest kind of data structure R knows about. Even when working with a single number we’re actually dealing with an atomic vector. It just happens to be of length one. Here’s the very first expression we evaluated in the first chapter: 1 + 1 ## [1] 2 Look at the output. What is that [1] at the beginning? We ignored it before because we weren’t in a position to understand its significance. The [1] is a clue that the output resulting from 1 + 1 is a atomic vector. We can verify this with the is.vector and is.atomic functions: x &lt;- 1 + 1 # what value is associated with x? x ## [1] 2 # is it a vector? is.vector(x) ## [1] TRUE # is it atomic? is.atomic(x) ## [1] TRUE This little exercise demonstrates an important point about R. Atomic vectors really are the simplest kind of data structure in R. Unlike many other languages there is simply no way to represent just a number. Instead, a single number must be stored as a vector of length one7. 4.3 Numeric vectors A lot of work in R involves numeric vectors. After all, data analysis is all about numbers. Here’s a simple way to construct a numeric vector: numeric(length = 50) ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [36] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 What happened? We made a numeric vector with 50 elements, each of which is the number 0. The word “element” is used to reference any object (a number in this case) that resides at a particular position in a vector. When we create a vector but don’t assign it to a name using &lt;- R just prints it to the Console for us. Notice what happens when the vector is printed to the screen. Since the length-50 vector can’t fit on one line, it was printed over two. At the beginning of each line there is a [X]: the X is a number that describes the position of the element shown at the beginning of a particular line. Different kinds of numbers Roughly speaking, R stores numbers in two different ways depending of whether they are whole numbers (“integers”) or numbers containing decimal points (“doubles” – don’t ask). We’re not going to worry about this distinction. Most of the time the distinction is fairly invisible to users so it is easier to just think in terms of numeric vectors. We can mix and match integers and doubles in R without having to worry too much about R is storing the numbers. If we need to check that we really have made a numeric vector we can use the is.numeric function to do this: # let&#39;s create a variable that is a numeric vector numvec &lt;- numeric(length = 50) # check it really is a numeric vector is.numeric(numvec) ## [1] TRUE This returns TRUE as expected. A value of FALSE would imply that numvec is some other kind of object. This may not look like the most useful function in the world, but sometimes we need functions like is.numeric to understand what R is doing or root out mistakes in our scripts. Keep in mind that when we print a numeric vector to Console R only prints the elements to 7 significant figures by default. We can see this by printing the built in constant pi to the Console: pi ## [1] 3.141593 The actual value stored in pi is actually much more precise than this. We can see this by printing pi again using the print function: print(pi, digits = 16) ## [1] 3.141592653589793 4.4 Constructing numeric vectors We just saw how to use the numeric function to make a numeric vector of zeros. The size of the vector is controlled by the length argument—we used length = 50 above to make a vector with 50 elements. This is arguably not a particularly useful skill, as we usually need to work vectors of particular values (not just 0). A very useful function for creating custom vectors is the c function. Take a look at this example: c(1.1, 2.3, 4.0, 5.7) ## [1] 1.1 2.3 4.0 5.7 The “c” in the function name stands for “combine”. The c function takes a variable number of arguments, each of which must be a vector of some kind, and combines these into a single, new vector. We supplied the c function with four arguments, each of which was a vector of length 1 (remember: a single number is treated as a length-one vector). The c function combines these to generate a vector of length 4. Simple. Now look at this example: vec1 &lt;- c(1.1, 2.3) vec2 &lt;- c(4.0, 5.7, 3.6) c(vec1, vec2) ## [1] 1.1 2.3 4.0 5.7 3.6 This shows that we can use the c function to concatenate (“stick together”) two or more vectors, even if they are not of length 1. We combined a length-2 vector with a length-3 vector to produce a new length-5 vector. Notice that we did not have to name the arguments in those two examples—there were no = involved. The c function is an example of one of those flexible functions that breaks the simple rules of thumb for using arguments that we set out earlier: it can take a variable number of arguments, and these arguments do not have predefined names. This behaviour is necessary for c to be of any use: in order to be useful it needs to be flexible enough to take any combination of arguments. Finding out about a vector R Sometimes it is useful to be able to find out a little extra information about a vector you are working with, especially if it is very large. Three functions that can extract some useful information about a vector for us are length, head and tail. Using a variety of different vectors, experiment with these to find out what they do. Make sure you use vectors that aren’t too short (e.g. with a length of at least 10). Hint: head and tail can be used with a second argument, n. 4.5 Named vectors What happens if we name the arguments to c when constructing a vector? Take a look at this: namedv &lt;- c(a = 1, b = 2, c = 3) namedv ## a b c ## 1 2 3 What happened here is that the argument names were used to define the names of elements in the vector we made. The resulting vector is still a 1-dimensional data structure. When it is printed to the Console the value of each element is printed, along with the associated name above it. We can extract the names from a named vector using the names function: names(namedv) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; Being able to name the elements of a vector is very useful because it enables us to more easily identify relevant information and extract the bits we need—we’ll see how this works in the next chapter. 4.6 Generating sequences of numbers The main limitation of the c function is that we have to manually construct vectors from their elements. This isn’t very practical if we need to construct very long vectors of numeric values. There are various functions that can help with this kind of thing though. These are useful when the elements of the target vector need to follow a sequence or repeating pattern. This may not appear all that useful at first, but repeating sequences are used a lot in R. 4.6.1 Generating sequences of numbers The seq function allows us to generate sequences of numbers. It needs at least two arguments, but there are several different ways to control the sequence produced by seq. The method used is determined by our choice of arguments: from, to, by, length.out and along.with. We don’t need to use all of these though—setting 2-3 of these arguments will often work: Using the by argument generates a sequence of numbers that increase or decrease by the requested step size: seq(from = 0, to = 12, by = 2) ## [1] 0 2 4 6 8 10 12 This is fairly self-explanatory. The seq function constructed a numeric vector with elements that started at 0 and ended 12, with successive elements increasing in steps of 2. Be careful when using seq like this. If the by argument does not lead to a sequence that ends exactly on the value of to then that value won’t appear in the vector. For example: seq(from = 0, to = 11, by = 2) ## [1] 0 2 4 6 8 10 We can generate a descending sequence by using a negative by value, like this: seq(from = 12, to = 0, by = -2) ## [1] 12 10 8 6 4 2 0 Using the length.out argument generates a sequence of numbers where the resulting vector has the length specified by length.out: seq(from = 0, to = 12, length.out = 6) ## [1] 0.0 2.4 4.8 7.2 9.6 12.0 Using the length.out argument will always produce a sequence that starts and ends exactly on the values specified by from and to (if we need a descending sequence we just have to make sure from is larger than to). Using the along.with argument allows us to use another vector to determine the length of the numeric sequence we want to produce: # make any any vector we like my_vec &lt;- c(1, 3, 7, 2, 4, 2) # use it to make a sequence of the same length seq(from = 0, to = 12, along.with = my_vec) ## [1] 0.0 2.4 4.8 7.2 9.6 12.0 It doesn’t matter what the elements of myvec are. The behaviour of seq is controlled by the length of myvec when we use along.with. It’s conventional to leave out the from and to argument names when using the seq function. For example, we could rewrite the first example above as: seq(0, 12, by = 2) ## [1] 0 2 4 6 8 10 12 When we leave out the name of the third argument its value is position matched to the by argument: seq(0, 12, 2) ## [1] 0 2 4 6 8 10 12 However, our advice is to explicitly name the by argument instead of relying on position matching, because this reminds us what we’re doing and will stop us forgetting about the different control arguments. If we do not specify values of either by, length.out and along.with when using seq the default behaviour is to assume we meant by = 1: seq(from = 1, to = 12) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 Generating sequences of integers that go up or down in steps of 1 is something we do a lot in R. Because of this, there is a special operator to generate these simple sequences—the colon, :. For example, to produce the last sequence again we use: 1:12 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 When we use the : operator the convention is to not leave spaces either side of it. 4.6.2 Generating repeated sequences of numbers The rep function is designed to replicate the values inside a vector, i.e. it’s short for “replicate”. The first argument (called x) is the vector we want to replicate. There are two other arguments that control how this is done. The times argument specifies the number of times to replicate the whole vector: # make a simple sequence of integers seqvec &lt;- 1:4 seqvec ## [1] 1 2 3 4 # now replicate *the whole vector* 3 times rep(seqvec, times = 3) ## [1] 1 2 3 4 1 2 3 4 1 2 3 4 All we did here was take a sequence of integers from 1 to 4 and replicate this end-to-end three times, resulting in a length-12 vector. Alternatively, we can use the each argument to replicate each element of a vector while retaining the original order: # make a simple sequence of integers seqvec &lt;- 1:4 seqvec ## [1] 1 2 3 4 # now replicate *each element* vector 3 times rep(seqvec, each = 3) ## [1] 1 1 1 2 2 2 3 3 3 4 4 4 This example produced a similar vector to the previous one. It contains the same elements and has the same length, but now the order is different. All the 1’s appear first, then the 2’s, and so on. Predictably, we can also use both the times and each arguments together if we want to: seqvec &lt;- 1:4 rep(seqvec, times = 3, each = 2) ## [1] 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4 The way to think about how this works is to imagine that we apply rep twice, first with each = 2, then with times = 3 (or vice versa). We can achieve the same thing using nested function calls, though it is much uglier: seqvec &lt;- 1:4 rep(rep(seqvec, each = 2), times = 3) ## [1] 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4 4.7 Vectorised operations All the simple arithmetic operators (e.g. + and -) and many mathematical functions are vectorised in R . What this means is that when we use a vectorised function it operates on vectors on an element-by-element basis. Take a look at this example to see what we mean by this: # make a simple sequence x &lt;- 1:10 x ## [1] 1 2 3 4 5 6 7 8 9 10 # make another simple sequence *of the same length* y &lt;- seq(0.1, 1, length = 10) y ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 # add them x + y ## [1] 1.1 2.2 3.3 4.4 5.5 6.6 7.7 8.8 9.9 11.0 We constructed two length-10 numeric vectors, called x and y, where x is a sequence from 1 to 10 and y is a sequence from 0.1 to 1. When R evaluates the expression x + y it does this by adding the first element of x to the first element of y, the second element of x to the second element of y, and so on, working through all 10 elements of x and y. Vectorisation is also implemented in the standard mathematical functions. For example, our friend the round function will round each element of a numeric vector: # make a simple sequence of non-integer values my_nums &lt;- seq(0, 1, length = 13) my_nums ## [1] 0.00000000 0.08333333 0.16666667 0.25000000 0.33333333 0.41666667 ## [7] 0.50000000 0.58333333 0.66666667 0.75000000 0.83333333 0.91666667 ## [13] 1.00000000 # now round the values round(my_nums, digits = 2) ## [1] 0.00 0.08 0.17 0.25 0.33 0.42 0.50 0.58 0.67 0.75 0.83 0.92 1.00 The same behaviour is seen with other mathematical functions like sin, cos, exp, and log. Each of these will apply the relevant function to each element of a numeric vector. Not all functions are vectorised. For example, the sum function takes a vector of numbers and adds them up: sum(my_nums) ## [1] 6.5 Although sum obviously works on a numeric vector it is not “vectorised” in the sense that it works element-by-element to return an output vector of the same length as its main argument. Many functions apply the vectorisation principle to more than one argument. Take a look at this example to see what we mean by this: # make a repeated set of non-integer values my_nums &lt;- rep(2 / 7, times = 6) my_nums ## [1] 0.2857143 0.2857143 0.2857143 0.2857143 0.2857143 0.2857143 # round each one to a different number of decimal places round(my_nums, digits = 1:6) ## [1] 0.300000 0.290000 0.286000 0.285700 0.285710 0.285714 We constructed a length 6 vector containing the number 2/7 (~ 0.285714) and then used the round function to round each element to a different number of decimal places. The first element was rounded to 1 decimal place, the second to two decimal places, and so on. We get this behaviour because instead of using a single number for the digits argument, we used a vector that is an integer sequence from 1 to 6. Vectorisation is not the norm R’s vectorised behaviour may seem like the “obvious” thing to do, but most computer languages do not work like this. In other languages we typically have to write a much more complicated expression to do something so simple. This is one of the reasons R is such a data analysis language: vectorisation allows us to express repetitious calculations in a simple, intuitive way. This behaviour can save us a lot of time. However, not every function treats its arguments in a vectorised way, so we always need to check (most easily, by experimenting) whether this behaviour is available before relying on it. The other common vector is called a “list”. Lists are very useful but we won’t cover them in this book↩ The same is true for things like sets of characters (&quot;dog&quot;, &quot;cat&quot;, &quot;fish&quot;, …) and logical values (TRUE or FALSE) discussed in the next two chapters.↩ "],
["other-kinds-of-vectors.html", "Chapter 5 Other kinds of vectors 5.1 Introduction 5.2 Character vectors 5.3 Logical vectors", " Chapter 5 Other kinds of vectors 5.1 Introduction The data we collect and analyse are often in the form of numbers, so it comes as no surprise that we work with numeric vectors a lot in R. Nonetheless, we also sometimes need to represent other kinds of vectors, either to represent different types of data, or to help us manipulate our data. This chapter introduces two new types of atomic vector to help us do this: character vectors and logical vectors. 5.2 Character vectors The elements of character vectors are what are known as a “character string” (or “string” if we are feeling lazy). The term “character string” refers a sequence of characters, such as “Treatment 1”, “University of Sheffield”, “Population Density”. A character vector is an atomic vector that stores an ordered collection of one or more character strings. If we want to construct a character vector in R we have to place double (&quot;) or single (') quotation marks around the characters. For example, we can print the name “Dylan” to the Console like this: &quot;Dylan&quot; ## [1] &quot;Dylan&quot; Notice the [1]. This shows that what we just printed is an atomic vector of some kind. We know it’s a character vector because the output is printed with double quotes around the value. We often need to make simple character vectors containing only one value—for example, to set the values of arguments to a function. The quotation marks are not optional—they tell R we want to treat whatever is inside them as a literal value. The quoting is important. If we try to do the same thing as above without the quotes we end up with an error: Dylan ## Error in eval(expr, envir, enclos): object &#39;Dylan&#39; not found What happened? When the interpreter sees the word Dylan without quotes it assumes that this must be the name of a variable, so it goes in search of it in the global environment. We haven’t made a variable called Dylan, so there is no way to evaluate the expression and R spits out an error to let us know there’s a problem. Longer character vectors are typically constructed to represent data of some kind. The c function is often a good starting point for this kind of thing: # make a length-3 character vector my_name &lt;- c(&quot;Dylan&quot;, &quot;Zachary&quot;, &quot;Childs&quot;) my_name ## [1] &quot;Dylan&quot; &quot;Zachary&quot; &quot;Childs&quot; Here we made a length-3 character vector, with elements corresponding to a first name, middle name, and last name. If we want to extract one or more elements from a character vector by their position Take note, this is not equivalent to the above : my_name &lt;- c(&quot;Dylan Zachary Childs&quot;) my_name ## [1] &quot;Dylan Zachary Childs&quot; The only element of this character vector is a single character string containing the first, middle and last name separated by spaces. We didn’t need to use the the c function here because we were only ever working with a length-1 character vector. i.e. we could have typed &quot;Dylan Zachary Childs&quot; and we would have ended up with exactly the same text printed at the Console. We can construct more complicated, repeating character vectors with rep. This works on character vectors in exactly the same way as it does on numeric vectors: c_vec &lt;- c(&quot;Dylan&quot;, &quot;Zachary&quot;, &quot;Childs&quot;) rep(c_vec, each = 2, times = 3) ## [1] &quot;Dylan&quot; &quot;Dylan&quot; &quot;Zachary&quot; &quot;Zachary&quot; &quot;Childs&quot; &quot;Childs&quot; &quot;Dylan&quot; ## [8] &quot;Dylan&quot; &quot;Zachary&quot; &quot;Zachary&quot; &quot;Childs&quot; &quot;Childs&quot; &quot;Dylan&quot; &quot;Dylan&quot; ## [15] &quot;Zachary&quot; &quot;Zachary&quot; &quot;Childs&quot; &quot;Childs&quot; Each element was replicated twice (each = 2), and then the whole vector was replicated three times (times = 3), end to end. What about the seq function? Hopefully it is fairly obvious that we can’t use this function to build a character vector. The seq function is designed to make sequences of numbers, from a starting value, to another. The notion of a sequence of character strings – for example, from &quot;Dylan&quot; to &quot;Childs&quot; – is meaningless. 5.3 Logical vectors The elements of logical vectors only take two values: TRUE or FALSE. Don’t let the simplicity of logical vectors fool you. They’re very useful. As with other kinds of atomic vectors the c and rep functions can be used to construct a logical vector: l_vec &lt;- c(TRUE, FALSE) rep(l_vec, times = 2) ## [1] TRUE FALSE TRUE FALSE Hopefully nothing about that output is surprising by this point. So why are logical vectors useful? Their allow us to represent the results of questions such as, “is x greater than y” or “is x equal to y”. The results of such comparisons may then be used to carry out various kinds of subsetting operations. Let’s first look at how we use logical vectors to evaluate comparisons. Before we can do that though we need to introduce relational operators. These sound fancy, but they are very simple: we use relational operators to evaluate the relative value of vector elements. Six are available in R: x &lt; y: is x less than y? x &gt; y: is x greater than y? x &lt;= y: is x less than or equal to y? x &gt;= y: is x greater than or equal to y? x == y: is x equal to y? x != y: is x not equal to y? The easiest way to understand how these work is to simply use them. We need a couple of numeric variables first: x &lt;- 11:20 y &lt;- seq(3, 30, by = 3) x ## [1] 11 12 13 14 15 16 17 18 19 20 y ## [1] 3 6 9 12 15 18 21 24 27 30 Now, if we need to evaluate and represent a question like, “is x greater than than y”, we can use either &lt; or &gt;: x &gt; y ## [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE The x &gt; y expression produces a logical vector, with TRUE values associated with elements in x are less than y, and FALSE otherwise. In this example, x is less than y until we reach the value of 15 in each sequence. Notice that relational operators are vectorised: they work on an element by element basis. They wouldn’t be much use if this were not the case. What does the == operator do? It compares the elements of two vectors to determine if they are exactly equal: x == y ## [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE The output of this comparison is true only for one element, the number 15, which is at the 5th position in both x and y. The != operator is essentially the opposite of ==. It identifies cases where two elements are not exactly equal. We could step through each of the different the other relational operators, but hopefully they are self-explanatory at this point (if not, experiment with them). = and == are not the same If we want to test for equivalence between the elements of two vectors we must use double equals (==), not single equals (=). Forgetting to do this == instead of = is a very common source of mistakes. The = symbol already has a use in R—assigning name-value pairs—so it can’t also be used to compare vectors because this would lead to ambiguity in our R scripts. Using = when you meant to use == is a very common mistake. If you make it, this will lead to all kinds of difficult-to-comprehend problems with your scripts. Try to remember the difference! "],
["extracting-subsets-of-vectors.html", "Chapter 6 Extracting subsets of vectors 6.1 Introduction 6.2 Subsetting by position 6.3 Subsetting with logical operators", " Chapter 6 Extracting subsets of vectors 6.1 Introduction At the beginning of the last chapter we said that an atomic vector is a 1-dimensional object that contains an ordered collection of data values. Why is this view of a vector useful? It means that we can extract a subset of elements from a vector once we know their position in that vector. There are two main ways to subset atomic vectors, both of which we’ll cover in this chapter. Whatever the method we use, subsetting involves a pair of opening and closing square brackets ([ and ]). These are always used together. 6.2 Subsetting by position We can use the [ construct with a vector to subset its elements directly using their position. Take a look at this example: numvec &lt;- c(7.2, 3.6, 2.9) numvec[2] ## [1] 3.6 The numvec variable is a length 3 numeric vector. In order to subset it and retain only the second element we used the [ ] construct with the number 2 inside, placing [2] next to the name of the vector. Notice that we do not place a space anywhere in this construct. We could, but this is not conventional. Remember, “the number 2” is in fact a numeric vector of length 1. This suggests we might be able to use longer vectors to extract more than one element: # make a numeric sequence my_vec &lt;- seq(0, 0.5, by = 0.1) my_vec ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 # make an indexing vector i &lt;- c(1, 3) # extract a subset of values my_vec[i] ## [1] 0.0 0.2 We first constructed a numeric vector of length 2 called i, which has elements 1 and 3. We then used this to extract his first and third value by placing i inside the [ ]. We didn’t have to carry out the subsetting operation in two steps. This achieves the same result: my_vec[c(1, 3)] ## [1] 0.0 0.2 Notice that when we subset a vector of a particular type, we get a vector of the same type back, e.g. subsetting a numeric vector produces another numeric vector. We can also subset a vector by removing certain elements. We use the - operator to do this. Here’s an example that produces the same result as the last example, but in a different way: my_vec[-c(2, 4, 5, 6)] ## [1] 0.0 0.2 The my_vec[-c(2, 4, 5, 6)] expression indicates that we want all the elements of c_vec except those that at position 2, 4, 5, and 6. We have learned how to use the [ construct with a numeric vector of integer(s) to subset the elements of vector by their position. This works exactly the same way with character vectors: c_vec &lt;- c(&quot;Dylan&quot;, &quot;Zachary&quot;, &quot;Childs&quot;) c_vec[1] ## [1] &quot;Dylan&quot; The c_vec variable is a length 3 character vector, with elements corresponding to his first, middle and last name. We used the [ ] construct with the number 1 inside, to extract the first element (i.e. the first name). Longer vectors can be used to extract more than one element, and we can use negative numbers to remove elements: # extract the first and third value c_vec[c(1, 3)] ## [1] &quot;Dylan&quot; &quot;Childs&quot; # drop the second value (equivalent) c_vec[-2] ## [1] &quot;Dylan&quot; &quot;Childs&quot; 6.3 Subsetting with logical operators Subsetting vectors by position suffers from one major drawback—we have to know where the elements we want sit in the vector. A second way to subset a vector makes use of logical vectors alongside [ ]. This is usually done using two vectors of the same length: the focal vector we wish to subset, and a logical vector that specifies which elements to keep. Here is a very simple example: i &lt;- c(TRUE, FALSE, TRUE) c_vec[i] ## [1] &quot;Dylan&quot; &quot;Childs&quot; This should be fairly self-explanatory. The logical vector method of subsetting works element-by-element, and an element in c_vec is retained wherever the corresponding element in i contains a TRUE; otherwise it is discarded. Why is this better than using position indexing? After all, using a vector of positions to subset a vector is much more direct. The reason is that we can use relational operators to help us select elements according to particular criteria. This is best illustrated with an example. We’ll start by defining two vectors of the same length: name &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;wren&quot;, &quot;pig&quot;, &quot;owl&quot;) name ## [1] &quot;cat&quot; &quot;dog&quot; &quot;wren&quot; &quot;pig&quot; &quot;owl&quot; type &lt;- c(&quot;mammal&quot;, &quot;mammal&quot;, &quot;bird&quot;, &quot;mammal&quot;, &quot;bird&quot;) type ## [1] &quot;mammal&quot; &quot;mammal&quot; &quot;bird&quot; &quot;mammal&quot; &quot;bird&quot; The first, name, is a character vector containing the common names of a few animals. The second, type, is another character vector whose elements denote the type of animal, in this case, a bird or a mammal. The vectors are arranged such that the information is associated via the position of elements in each vector (cats and dogs are mammals, a wren is a bird, and so on). Let’s assume that we want to create a subset of name that only contains the names of mammals. We can do this by creating a logical vector from type, where the values are TRUE when an element is equal to &quot;mammal&quot; and FALSE otherwise. We know how to do this using the == operator: i &lt;- type == &quot;mammal&quot; i ## [1] TRUE TRUE FALSE TRUE FALSE We stored the result in a variable called i. Now all we need to do is use with i inside the [ ] construct to subset name: name[i] ## [1] &quot;cat&quot; &quot;dog&quot; &quot;pig&quot; We did this the long way to understand the logic of subsetting vectors with logical operators. This is quite verbose though, and we usually combine the two steps into a single R expression: name[type == &quot;mammal&quot;] ## [1] &quot;cat&quot; &quot;dog&quot; &quot;pig&quot; We can use any of the relational operators to subset vectors like this. If we define a numeric variable that contains the mean mass (in grams) of each animal, we can use this to subset names according to the associated mean mass. For example, if we want a subset that contains only those animals where the mean mass is greater than 1kg we use: mass &lt;- c(2900, 9000, 10, 18000, 2000) name[mass &gt; 1000] ## [1] &quot;cat&quot; &quot;dog&quot; &quot;pig&quot; &quot;owl&quot; Just remember, this way of using information in one vector to create subsets of a second vector only works if the information in each is associated via the position of their respective elements. Keeping a bunch of different vectors organised like this is difficult and error prone. In the next block we’ll learn how to use something called a data frame and the dplyr package to make working with a collection of related vectors much easier. "],
["getting-help-1.html", "Chapter 7 Getting help 7.1 Introduction 7.2 Browsing the help system 7.3 Searching for help files 7.4 Navigating help files 7.5 Vignettes and demos", " Chapter 7 Getting help 7.1 Introduction R has a comprehensive built-in help system. This system is orientated around the base R functions and packages. Every good package comes with a set of help files. At a minimum these should provide information about the individual package functions and summaries of the data included with the package. They sometimes give descriptions of how different parts of the package should be used, and if we’re lucky, one or more “vignettes” that offer a practical demonstration of how to use a package. Other files are sometimes shipped with packages. For example, these might give an overview of the mathematical or computational theory a package relies on. We will not worry about these in this course. We may as well get something out of the way early on. The word “help” in the phrase “help file” is a bit of a misnomer. It is probably more accurate to say R has an extensive documentation system. The reason we say this is that the majority of help files are associated with functions, and these kinds of files are designed first and foremost to document how a particular function or group of functions are meant to be used. For example, they describe what kinds of arguments a function can take and what kind of objects it will return to us. Help files are also written in a very precise, painful-to-read manner. They contain a lot of jargon which can be hard for new R users to decipher. The take-home message is that R help files are aimed more at experienced users than novices. Their primary purpose is to carefully document the different elements of a package, rather than explain how a particular function or the package as whole should be used to achieve a given end. That said, help files often contain useful examples, and many package authors do try to make our life easier by providing functional demonstrations of their package (those “vignettes” we mentioned above are a vehicle for doing this). It’s important to try to get to grips with the built in help system. It contains a great deal of useful information which we need to really start using R effectively. The road to enlightenment is bumpy though. 7.2 Browsing the help system How do we access the help system? Help files are a little like mini web pages, which means we can navigate among them using hyperlinks. This makes it very easy to explore the help system. One way to begin browsing the help system uses the help.start function: help.start() If we type this now at the Console we should see the Package Index page open up in the Help tab of the bottom right pane in RStudio. This lists all the packages currently installed on a computer. We can view all the help files associated with a package by clicking on the appropriate link. For example, the functions that come with the base installation of R have a help file associated with them—click on the link to the R base package (base) to see these. Though we know about a few of these already, there are a lot of functions listed here. R is huge. The packages that come with the base R installation and those that we install separately from base R have their own set of associated help files. These can be viewed by following the appropriate link on the Package Index page. We will learn how to navigate these in a moment. Take note: it is up to the developer of a package to produce usable help files. Well-designed packages like dplyr and ggplot2 have an extensive help system that covers almost everything the package can do. This isn’t always the the case though, particularly with new or packages or packages that are not widely used. We will only ever use well-documented packages. Notice that the help browser has Forward, Back, and Home buttons, just like a normal web browser. If we get lost in the mire of help pages we can always navigate backward until we get back to a familiar page. However, for some reason the Home button does not take us to the same page as help.start. Clicking on the home button takes us to a page with three sections: The Manuals section looks like it might be useful for novice users. Unfortunately, it isn’t really. Even the “Introduction to R” manual is only helpful for someone with a bit of programming experience because it assumes we understand what terms like “data structure” and “data type” mean. It is worth reading this manual after gaining a bit of experience with R. The others manuals are more or less impenetrable unless the reader already knows quite a bit about computing in general. The Reference section is a little more helpful. The “Packages” link just takes us to the same page opened by help.start. From here we can browse the help pages on a package-specific basis. The “Search Engine &amp; Keywords” link takes us to a search engine page (no surprises there). We can use this to search for specific help pages, either by supplying a search term or by navigating through the different keywords. We’ll discuss the built-in search engine in the next subsection. The Miscellaneous Material section has a couple of potentially useful links. The “User Manuals” link lists any user manuals supplied by package authors. These tend to be aimed at more experienced users and the packages we will learn to use in this course do not provide them. However, it is worth knowing these exist as they are occasionally useful. The “Frequently Asked Questions” link is definitely worth reviewing at some point, but again, most of the FAQs are a little difficult for novice users to fully understand. 7.3 Searching for help files After browsing help files via help.start for a bit it quickly becomes apparent that this way of searching for help is not very efficient. Quite often we know the name of the function we need to use and all we want to do is open that particular help file. We can do this with the help function: help(topic = Trig) After we run this line RStudio opens up the help file for the trigonometry topic in the Help tab. This file provides information about the various trigonometric functions such as sin or cos. We’ll learn how to make sense of such help pages in the next subsection. For now, we just want to see how to use help. The help function needs a minimum of one argument: the name of the topic or function of interest. When we use it like this the help function searches across packages, looking for a help file whose name gives an exact match to the name we supplied. In this case, we opened the help file associated with the Trig topic. Most of the time we use the help function to find the help page for a specific function, rather than a general topic. This is fine if we can remember the name of the topic associated with different functions. Most of us cannot. Luckily, the help function will also match help pages by the name of the function(s) they cover: help(topic = sin) Here we searched for help on the sin function. This is part of the Trig topic so help(topic = sin) brings up the same page as the help(topic = Trig). There are several arguments of help that we can set to alter its behaviour. We will just consider one of these. By default, the help function only searches for files associated with the base functions or with packages that we have loaded in the current session with the library function. If we want to search for help on the mutate function—part of the dplyr package—but we haven’t run library(dplyr) in the current session this will fail: help(mutate) ## Help on topic &#39;mutate&#39; was found in the following packages: ## ## Package Library ## plyr /Library/Frameworks/R.framework/Versions/3.3/Resources/library ## dplyr /Library/Frameworks/R.framework/Versions/3.3/Resources/library ## ## ## Using the first match ... Instead, we need tell help where to look by setting the package argument: help(mutate, package = dplyr) It’s good practise to use help every time we’re struggling with a particular function. Even very experienced R users regularly forget how to use the odd function and have to dive into the help. It’s for this reason that R has a built in shortcut for help. This is accessed via ?. For example, instead of typing help(topic = sin) into the Console we can bring up the help page for the sin function by using ? like this: ?sin This is just a convenient shortcut that does the same thing as help. The only difference is that ? does not allow us to set arguments such as package. 7.4 Navigating help files Navigating a typical help file is a little daunting at first. These files can be quite long and they contain a lot of jargon. The help files associated with functions – the most common type – have a consistent structure though. There are a number of distinct sections, whose order is always the same. Wrestling with a help file is much easier if we at least understand what each section is for. After the title, there are eight sections we need to know about: Description gives us a short overview of what the function is meant to be used for. If the help page covers a family of related functions it gives a collective overview of all the functions. Always read this before diving into the rest of the help file. Usage shows how the function(s) are meant to be used. It lists each member of the family as well as their common arguments. The argument names are listed on their own if they have no default, or in name-value pairs, where the value gives the default used should we choose not to set it ourselves when we call the function. Arguments lists each of the allowed arguments, along with a short description of what they influence. This also tells us what what kind of data we are allowed to use with each argument, along with the allowable values (if this is relevant). Always read this section. Details describes precisely how the function(s) behave, often in painful, jargon-laden detail. It is usually the longest and hardest-to-comprehend section but is worth reading as it flags up common “gotchas”. We can sometimes get away with ignoring this section, but when we really want to understand a function we need to wade through it. Value explains kind of data structure or object a function returns to us when it finishes doing whatever it does. It’s often possible to guess what this will be from the type of function, but nonetheless, it usually a good idea to check whether our reasoning is correct. If it isn’t, we probably don’t understand the function yet. References just lists the key reference(s). These are worth digging out if we really need to know the ‘hows’ and ‘whys’ of a function. We can often skip this information. The one exception is if the function implements a particular statistical tool. In that case it’s sensible to go away and read the literature before trying to use it. See Also gives links to the help pages of related functions. These are usually functions that do something similar to the function of interest or are meant to be used in conjunction with it. We can often learn quite a bit about packages or related functions by following the links in this section. Examples provides one or more examples of how to use the function. These are stand-alone examples, so there’s nothing to stop us running them. This is often the most useful section of all. Seeing a function in action is a very good way to cut through the jargon and understand how it works. 7.5 Vignettes and demos The Oxford English Dictionary defines a vignette as, “A brief evocative description, account, or episode.” The purpose of a package vignette in R is to provide a relatively brief, practical account of one or more of its features. Not all packages come with vignettes, though many of the best thought out packages do. We use the vignette function to view all the available vignettes in Rstudio. This will open up a tab that lists each vignette under their associated package name along with a brief description. A package will often have more than one vignette. If we just want to see the vignettes associated with a particular package, we have to set the package argument. For example, to see the vignettes associated with dplyr we use: vignette(package = &quot;dplyr&quot;) Each vignette has a name (the “topic”) and is available either as a PDF or HTML file (or both). We can view a particular vignette by passing the vignette function the package and topic arguments. For example, to view the “data_frames” vignette in the dplyr package we would use: vignette(topic = &quot;data_frames&quot;, package = &quot;dplyr&quot;) The vignette function is fine, though it is usually more convenient to browse the list of vignettes inside a web browser. This allows us to open a particular vignette directly by clicking on its link, rather than working at the Console. We can use the browseVignettes function to do this: browseVignettes() This will open a page in our browser showing the vignettes we can view. As one should expect by now, we can narrow our options to a specific package by setting the package argument. In addition to vignettes, some packages also include one or more ‘demos’ (demonstrations). Demos are a little like vignettes, but instead of just opening a file for us to read, the demo function can actually runs a demonstration R scripts. We use the demo function (without any arguments) to list the available demos: demo() When we use the demo function like this it only lists the demos associated with packages that have been loaded in the current session (via library). If we want to see all the demos we can run we need to use the somewhat cryptic demo(package = .packages(all.available = TRUE)). In order to actually run a demo we use the demo function, setting the topic and package arguments. For example, to run the “colors” demo in the grDevices package we would use: demo(colors, package = &quot;grDevices&quot;, ask = FALSE) This particular demo shows off some of the pre-defined colours we might use to customise the appearance of a plot. We’ve suppressed the output though because so much is produced. "],
["packages.html", "Chapter 8 Packages 8.1 The R package system 8.2 Task views 8.3 Using packages", " Chapter 8 Packages 8.1 The R package system The R package system is probably the most important single factor driving increased adoption of R among quantitatively-minded scientists. Packages make it very easy to extend the basic capabilities of R. In his book about R packages Hadley Wickam says, Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. An R package is just a collection of folders and files in a standard, well-defined format. They bundle together computer code, data, and documentation in a way that is easy to use and share with other users. The computer code might all be R code, but it can also include code written in other languages. Packages provide an R-friendly interface to use this “foreign” code without the need to understand how it works. The base R distribution it comes with quiet a few pre-installed packages. These are “mature” packages that implement widely used statistical and plotting functionality. These base R packages represent a very small subset of the available R packages. The majority of these are hosted on a network of web servers around the world collectively know as CRAN. This network—known as a repository—is the same one we used to download the base R distribution in the Get up and running with R and RStudio chapter. CRAN stands for the Comprehensive R Archive Network, pronounced either “see-ran” or “kran”. CRAN is a fairly spartan web site, so it’s easy to navigate. When we navigate to CRAN we see about a dozen links of the right hand side of the home page. Under the Software section there is a link called Packages. Near the top of this page there is a link called Table of available packages, sorted by name that points to a very long list of all the packages on CRAN. The column on the left shows each package name, followed by a brief description of what the package does on the right. There are a huge number of packages here (over 12000 at the time of writing). 8.2 Task views A big list of packages presented like is overwhelming. Unless we already know the name of the package we want to investigate, it’s very hard to find anything useful by scanning the “all packages” table. A more user-friendly view of many R packages can be found on the Task Views page (the link is on the left hand side, under the section labelled CRAN). A Task View is basically a curated guide to the packages and functions that are useful for certain disciplines. The Task Views page shows a list of these discipline-specific topics, along with a brief description. The Environmentrics Task View maintained by Gavin Simpson contains information about using R to analyse ecological and environmental data. It is not surprising this Task View exists. Ecologists and environmental scientists are among the most enthusiastic R users. This view is a good place to start looking for a new package to support a particular analysis in a future project. The Experimental Design, Graphics, Multivariate, Phylogenetics, Spatial, Survival and Time Series Task Views all contain many useful packages for biologists and environmental scientists. 8.3 Using packages Two things need to happen in order for us to use a package. First, we need to ensure that a copy of the folders and files that make up the package are copied to an appropriate folder on our computer. This process of putting the package files into the correct location is called installing the package. Second, we need to load and attach the package for use in a particular R session. As always, the word “session” refers to the time between when we start up R and close it down again. It’s worth unpacking these two ideas a bit, because packages are a frequent source of confusion for new users: If we don’t have a copy of a package’s folders and files in the right format and the right place on our computer we can’t use it. This is probably fairly obvious. The process of making this copy is called installing the package. It is possible to manually install packages by going to the CRAN website, downloading the package, and then using various tools to install it. We won’t be using this approach though because it’s both inefficient and error prone. Instead, we’ll use built-in R functions to grab the package from CRAN and install it for us, all in one step. We don’t need to re-install a packages we plan to use every time we start a new R session. It is worth saying that again, there is no need to install a package every time we start up R / RStudio. Once we have a copy of the package on our hard drive it will remain there for us to use. The only exception to this rule is that a major update to R (not RStudio!) will sometimes require a complete re-install of the packages. This is because the R installer will not copy installed packages to the major new version of R. These major updates are fairly infrequent though, occurring perhaps every 1-2 years. Installing a package does nothing more than place a copy of the relevant files on our hard drive. If we actually want to use the functions or the data that comes with a package we need to make them available in our current R session. Unlike package installation this load and attach process as it’s known has to be repeated every time we restart R. If we forget to load up the package we can’t use it. 8.3.1 Viewing installed packages We sometimes need to check whether a package is currently installed. RStudio provides a simple, intuitive way to see which packages are installed on our computer. The Packages tab in the top right pane of RStudio shows the name of every installed package, a brief description (the same one seen on CRAN) and a version number. We can also manage our packages from this tab, as we are about to find out. There are also a few R functions that can be used to check whether a package is currently installed. For example, the find.package function can do this: find.package(&quot;MASS&quot;) ## [1] &quot;/Library/Frameworks/R.framework/Versions/3.3/Resources/library/MASS&quot; This either prints a “file path” showing us where the package is located, or returns an error if the package can’t be found. Alternatively, the function called installed.packages returns something called a data frame (these are discussed later in the book) containing a lot more information about the installed packages. 8.3.2 Installing packages R packages can be installed from a number of different sources. For example, they can be installed from a local file on a computer, from the CRAN repository, or from a different kind of online repository called Github. Although various alternatives to CRAN are becoming more popular, we’re only going to worry about installing packages that live on CRAN in this book. This is no bad thing—the packages that live outside CRAN tend to be a little more experimental. In order to install a package from an online repository like CRAN we have to first download the package files, possibly uncompress them (like we would a ZIP file), and move them to the correct location. All of this can be done at the Console using a single function: install.packages. For example, if we want to install a package called fortunes, we use: install.packages(&quot;fortunes&quot;) The quotes are necessary by the way. If everything is working—we have an active internet connection, the package name is valid, and so on—R will briefly pause while it communicates with the CRAN servers, we should see some red text reporting back what’s happening, and then we’re returned to the prompt. The red text is just letting us know what R is up to. As long as this text does not include the word “error”, there is usually no need to worry about it. There is nothing to stop us using install.packages to install more than one package at a time. We are going to use dplyr and ggplot2 later in the book. Since neither of these is part of the base R distribution, we need to download and install them from CRAN. Here’s one way to do this: pckg.names &lt;- c(&quot;dplyr&quot;, &quot;ggplot2&quot;) install.packages(pckg.names) There are a couple of things to keep in mind. First, package names are case sensitive. For example, fortunes is not the same as Fortunes. Quite often package installations fail because we used the wrong case somewhere in the package name. The other aspect of packages we need to know about is related to dependencies: some packages rely on other packages in order to work properly. By default install.packages will install these dependencies, so we don’t usually have to worry too much about them. Just don’t be surprised if the install.packages function installs more than one package when only one was requested. Install dplyr and ggplot2 We’re going to be using dplyr and ggplot2 packages later in the book. If they aren’t already installed on your computer (check with find.package), now is a good time to install them so they’re ready to use later. RStudio provides a way of interacting with install.packages via point-and-click. The Packages tab has an “Install”&quot; button at the top right. Clicking on this brings up a small window with three main fields: “Install from”, “Packages”, and “Install to Library”. We only need to work with the “Packages” field – the other two can be left at their defaults. When we start typing in the first few letters of a package name (e.g. dplyr) RStudio provides a list of available packages that match this. After we select the one we want and click the “Install” button, RStudio invokes install.packages with the appropriate arguments at the Console for us. Never use install.packages in scripts Because installing a package is a “do once” operation, it is almost never a good idea to place install.packages in a typical R script. A script may be run 100s of times as we develop an analysis. Installing a package is quite time consuming, so we don’t really want to do it every time we run our analysis. As long as the package has been installed at some point in the past it is ready to be used and the script will work fine without re-installing it. 8.3.3 Loading and attaching packages Once we’ve installed a package or two we’ll probably want to actually use them. Two things have to happen to access a package’s facilities: the package has to be loaded into memory, and then it has to attached to something called a search path so that R can find it. It is beyond the scope of this book to get in to “how” and “why” of these events. Fortunately, there’s no need to worry about these details, as both loading and attaching can be done in a single step with a function called library. The library function works exactly as we might expect it to. If we want to start using the fortunes package—which was just installed above—all we need is: library(&quot;fortunes&quot;) ## Warning: package &#39;fortunes&#39; was built under R version 3.3.2 Nothing much happens if everything is working as it should. R just returns us to the prompt without printing anything to the Console. The difference is that now we can use the functions that fortunes provides. As it turns out, there is only one, called fortune: fortune() ## ## Friends don&#39;t let friends use Excel for statistics! ## -- Jonathan D. Cryer (about problems with using Microsoft Excel for ## statistics) ## JSM 2001, Atlanta (August 2001) The fortunes package is either very useful or utterly pointless, depending on ones perspective. It dispenses quotes from various R experts delivered to the venerable R mailing list (some of these are even funny). Once again, if we really don’t like working in the Console RStudio can help us out. There is a small button next to each package listed in the Packages tab. Packages that have been loaded and attached have a blue check box next to them, whereas this is absent from those that have not. Clicking on an empty check box will load up the package. Try this. Notice that all it does is invoke library with the appropriate arguments for us (RStudio explicitly sets the lib.loc argument, whereas above we just relied on the default value). Don’t use RStudio for loading packages! We looked at how it works, because at some point most people realise they can use RStudio to load and attach packages. We don’t recommend using this route though. It’s much better to put library statements into a script. Why? Because if we rely on RStudio to load packages, we have to do this every time we want to run a script, and if we forget one we need, the script won’t work. This is another example of where relying on RStudio ultimately makes things more, not less, challenging. One last tip: we can use library anywhere, but typically the library expressions live at the very beginning of a script so that everything is ready to use later on. 8.3.4 An analogy The package system often confuses new users. The reason for this stems from the fact that they aren’t clear about what the install.packages and library functions are doing. One way to think about these is by analogy with smartphone “Apps”. Think of an R package as being analogous to a smartphone App— a package effectively extends what R can do, just as an App extends what a phone can do. When we want to try out a new App we have to first download it from an App store and install it on our phone. Once it has been downloaded, an App lives permanently on the phone (unless we delete it!) and can be used whenever it’s needed. Downloading and installing the App is something we only have to do once. Packages are no different. When we want to use an R package we first have to make sure it is installed on the computer. This is effectively what install.packages does: it grabs the package from CRAN (the “App store”) and installs it on our computer. Installing a package is a “do once” operation. Once we’ve installed it, we don’t need to install a package again each time we restart R. The package is sat on the hard drive ready to be used. In order to actually use an App which has been installed on a phone we open it up by tapping on its icon. This obviously has to happen every time we want to use the App. The package equivalent of opening a smartphone App is the “load and attach” operation. This is what library does. It makes a package available for use in a particular session. We have to use library to load the package every time we start a new R session if we plan to access the functions in that package: loading and attaching a package via library is a “do every time” operation. "],
["data-frames.html", "Chapter 9 Data frames 9.1 Introduction 9.2 Data frames 9.3 Exploring data frames 9.4 Extracting data from data frames 9.5 Final words", " Chapter 9 Data frames 9.1 Introduction We learned in the A quick introduction to R chapter that the word “variable” is used as short hand for any kind of named object. For example, we can make a variable called num_vec that refers to a simple numeric vector using: num_vec &lt;- 1:10 When a computer scientist talks about variables they’re usually referring to these sorts of name-value associations. However, the word “variable” has a second, more abstract meaning in the world of data analysis and statistics: it refers to anything we can control or measure. For example, if our data comes from an experiment, the data will typically involve variables whose values describe the experimental conditions (e.g. “control plots” vs. “fertiliser plots”) and the quantities we chose to measure (e.g. species biomass and diversity). These kinds of abstract variables are often called “statistical variables”. Statistical variables can be further broken down into a range of different types, such as numeric and categorical variables. We’ll discuss these later on in the Exploratory data analysis chapter. The reason we’re pointing out the dual meaning of the word “variable” here is because we need to work with both interpretations. The dual meaning can be confusing, but both meanings are in widespread use so we just have to get used to them. We’ll try to minimise confusion by using the phrase “statistical variable” when we are referring to data, rather than R objects. We’re introducing these ideas now because we’re going to consider a new type of data object in R: the data frame. Real world data analysis involves collections of data (“data sets”) that involve several related statistical variables. We’ve seen that an atomic vector can only be used to store one type of data such as a collection of numbers. This means a vector can be used to store a single statistical variable, How should we keep a large collection of variables organised? We could work with them separately but this is very error prone. Ideally, we need a way to keep related variables together. This is the problem that data frames are designed to manage. 9.2 Data frames Data frames are one of those R features that mark it out as a particularly good environment for data analysis. We can think of a data frame as table-like objects with rows and columns. They collect together different statistical variables, storing each of them as a different column. Related observations are all found in the same row. This will make more sense in a moment. Let’s consider the columns first. Each column is a vector of some kind. These are usually simple vectors containing numbers or character strings, though it is also possible to include more complicated vectors inside data frames. We’ll only work with data frames made up of relatively simple vectors in this book. The key constraint that a data frame applies is that each vector must have the same length. This is what gives a data frame it table-like structure. The simplest way to get a feel for data frames is to make one. Data frames are usually constructed by reading some external data into R, but for the purposes of learning about them it is better to build one from its component parts. We’ll make some artificial data describing a hypothetical experiment to do this. Imagine that we’ve conducted a small experiment to examine biomass and community diversity in six field plots. Three plots were subjected to fertiliser enrichment. The other three plots act as experimental controls. We could store the data describing this experiment in three vectors: trt (short for “treatment”) shows which experimental manipulation was used. bms (short for “biomass”) shows the total biomass measured at the end of the experiment. div (short for “diversity”) shows the number of species present at the end of the experiment. Here’s some R code to generate these three vectors (it doesn’t matter what the actual values are, they’re made up): trt &lt;- rep(c(&quot;Control&quot;,&quot;Fertilser&quot;), each = 3) bms &lt;- c(284, 328, 291, 956, 954, 685) div &lt;- c(8, 12, 11, 8, 4, 5) trt ## [1] &quot;Control&quot; &quot;Control&quot; &quot;Control&quot; &quot;Fertilser&quot; &quot;Fertilser&quot; &quot;Fertilser&quot; bms ## [1] 284 328 291 956 954 685 div ## [1] 8 12 11 8 4 5 Notice that the information about different observations are linked by their positions in these vectors. For example, the third control plot had a biomass of ‘291’ and a species diversity ‘11’. We can use the data.frame function to construct a data frame from one or more vectors. To build a data frame from the three vectors we created and print these to the Console, we use: experim.data &lt;- data.frame(trt, bms, div) experim.data ## trt bms div ## 1 Control 284 8 ## 2 Control 328 12 ## 3 Control 291 11 ## 4 Fertilser 956 8 ## 5 Fertilser 954 4 ## 6 Fertilser 685 5 Notice what happens when we print the data frame: it is displayed as though it has rows and columns. That’s what we meant when we said a data frame is a table-like structure. The data.frame function takes a variable number of arguments. We used the trt, bms and div vectors as arguments, resulting in a data frame with three columns. Each of these vectors has 6 elements, so the resulting data frame has 6 rows. The names of the vectors were used to name its columns. The rows do not have names, but they are numbered to reflect their position. The words trt, bms and div are not very informative. If we prefer to work with more informative column names—this is always a good idea—then we have to name the data.frame arguments: experim.data &lt;- data.frame(Treatment = trt, Biomass = bms, Diversity = div) experim.data ## Treatment Biomass Diversity ## 1 Control 284 8 ## 2 Control 328 12 ## 3 Control 291 11 ## 4 Fertilser 956 8 ## 5 Fertilser 954 4 ## 6 Fertilser 685 5 The new data frame contains the same data as the previous one but now the column names correspond to the names we chose. These names are better because they describe each variable using a human-readable word. Don’t bother with row names We can also name the rows of a data frame using the row.names argument of the data.frame function. We won’t bother to show an example of this though. Why? We can’t easily work with the information in row names so there’s not much point adding it. If we need to include row-specific data in a data frame it’s best to include an additional variable, i.e. an extra column. 9.3 Exploring data frames The first things we should do when presented with a new data set is explore its structure to understand what we’re dealing with. This is easy when the data is stored in a data frame. If the data set is reasonably small we can just print it to the Console. This is not very practical for even moderate-sized data sets though. The head and tail functions extract the first and last few rows of a data set, so these can be used to print part of a data set. The n argument controls the number of rows printed: head(experim.data, n = 3) ## Treatment Biomass Diversity ## 1 Control 284 8 ## 2 Control 328 12 ## 3 Control 291 11 tail(experim.data, n = 3) ## Treatment Biomass Diversity ## 4 Fertilser 956 8 ## 5 Fertilser 954 4 ## 6 Fertilser 685 5 The View function can be used to visualise the whole data set in a spreadsheet like view: View(experim.data) This shows the rows and columns of the data frame argument in a table- or spreadsheet-like format. When we run this in RStudio a new tab opens up with the experim.data data inside it. View only displays the data The View function is designed to allow us to display the data in a data frame as a table of rows and columns. We can’t change the data in any way with the View function. We can reorder the way the data are presented, but keep in mind that this won’t alter the underlying data. There are quite a few different R functions that will extract information about a data frame. The nrow and ncol functions return the number of rows and columns, respectively: nrow(experim.data) ## [1] 6 ncol(experim.data) ## [1] 3 The names function is used to extract the column names from a data frame: colnames(experim.data) ## [1] &quot;Treatment&quot; &quot;Biomass&quot; &quot;Diversity&quot; The experim.data data frame has three columns, so names returns a character vector of length three, where each element corresponds to a column name. There is also a rownames function if we need that too. The nrow, ncol, names and rownames functions each return a vector, so we can assign the result if we need to use it later. For example, if we want to extract and store the column names for any reason we could use varnames &lt;- names(experim.data). 9.4 Extracting data from data frames Data frames would not be much use if we could not extract and modify the data in them. In this section we will briefly review how to carry out these kinds of operations using basic R functions. 9.4.1 Extracting and adding a single variable A data frame is just a collection of variables stored in columns, where each column is a vector of some kind. There are several ways to extract these variables from a data frame. If we just want to extract a single variable we have two options. The first way of extracting a variable from a data frame uses a double square brackets construct, [[. For example, we extract the Biomass variable from our example data frame with the double square brackets like this: experim.data[[&quot;Biomass&quot;]] ## [1] 284 328 291 956 954 685 This prints whatever is in the Biomass column to the Console. What kind of object is this? It’s a numeric vector: is.numeric(experim.data[[&quot;Biomass&quot;]]) ## [1] TRUE A data frame really is nothing more than a collection of vectors. Notice that all we did was print the resulting vector to the Console. If we want to actually do something with this numeric vector we need to assign the result: bmass &lt;- experim.data$Biomass bmass^2 ## [1] 80656 107584 84681 913936 910116 469225 Here, we extracted the Biomass variable, assigned it to bmass, and then squared this. The value of Biomass variable inside the experim.data data frame is unchanged. Notice that we used &quot;Biomass&quot; instead of Biomass inside the double square brackets, i.e. we quoted the name of the variable. This is because we want R to treat the word “Biomass” as a literal value. This little detail is important! If we don’t quote the name then R will assume that Biomass is the name of an object and go in search of it in the global environment. Since we haven’t created something called Biomass, leaving out the quotes generates an error: experim.data[[Biomass]] ## Error in (function(x, i, exact) if (is.matrix(i)) as.matrix(x)[[i]] else .subset2(x, : object &#39;Biomass&#39; not found The error message is telling us that R can’t find a variable called Biomass in the global environment. On the other hand, this example does work: vname &lt;- &quot;Biomass&quot; experim.data[[vname]] ## [1] 284 328 291 956 954 685 This works because we first defined vname to be a character vector of length one, whose value is the name of a variable in experim.data. When R encounters vname inside the [[ construct it goes and finds the value associated with it and uses this value to determine the variable to extract. The second method for extracting a variable from a data frame uses the $ operator. For example, to extract the Biomass column from the experim.data data frame, we use: experim.data$Biomass ## [1] 284 328 291 956 954 685 We use the $ operator by placing the name of the data frame we want to work with on the left hand side and and the name of the column (i.e. the variable) we want to extract on the right hand side. Notice that this time we didn’t have to put quotes around the variable name when using the $ operator. We can do this if we want to—i.e. experim.data$&quot;Biomass&quot; also works—but $ doesn’t require it. Why is there more than one way to extract variables from a data frame? There’s no simple way to answer this question without getting into the details of how R represents data frames. The simple answer is that $ and [[ are not actually equivalent, even though they appear to do much the same thing. We’ve looked at the two extraction methods because they are both widely used. However, the $ method is a bit easier to read and people tend to prefer it for interactive data analysis tasks (the [[ construct tends to be used when we need a bit more flexibility). 9.4.2 Adding a variable to a data frame How do we add a new variable to an existing data frame? It turns out that the $ operator is also be used for this job by combining it with the assignment operator. Using it this way is fairly intuitive. For example, if we want to add a new (made up) variable called Elevation to experim.data, we do it like this: experim.data$Elevation &lt;- c(364, 294, 321, 358, 298, 312) This assigns some fake elevation data to a new variable in experim.data using the $ operator. The new variable is called Elevation because that was the name we used on the right hand side of $. This changes experim.data, such that it now contains four columns (variables): head(experim.data, n = 3) ## Treatment Biomass Diversity Elevation ## 1 Control 284 8 364 ## 2 Control 328 12 294 ## 3 Control 291 11 321 The [[ operator can also be used with &lt;- to add variables to a data frame. We won’t bother to show an example, as it works in exactly the same way as $ and we won’t be using the [[ method in this book. 9.4.3 Subsetting data frames What do we do if, instead of just extracting a single variable from a data frame, we need to select a subset of rows and/or columns? We use the single square brackets construct, [, to do this. There are two different ways we can use single square brackets, both of which involve the use of indexing vector(s) inside the [ construct. The first use of [ allows us to subset one or more columns while keeping all the rows. This works exactly as the [ does for vectors. Just think of columns as the elements of the data frame. For example, if we want to subset experim.data such that we are only left with the first and second columns (Treatment and Biomass), we can use a numeric indexing vector: experim.data[c(1:2)] ## Treatment Biomass ## 1 Control 284 ## 2 Control 328 ## 3 Control 291 ## 4 Fertilser 956 ## 5 Fertilser 954 ## 6 Fertilser 685 However, this is not a very good way to subset columns because we have to know the position of each variable. If for some reason we change the order of the columns, we have to update our R code accordingly. A better approach uses a character vector of column names inside the [: experim.data[c(&quot;Treatment&quot;, &quot;Biomass&quot;)] ## Treatment Biomass ## 1 Control 284 ## 2 Control 328 ## 3 Control 291 ## 4 Fertilser 956 ## 5 Fertilser 954 ## 6 Fertilser 685 The second use of [ ] is designed to allow us to subset rows and columns at the same time. We have to specify both the rows and the columns we require, using a comma (“,”) to separate a row and column index vector. This is easiest to understand with an example: # row index rindex &lt;- 1:3 # column index cindex &lt;- c(&quot;Treatment&quot;, &quot;Biomass&quot;) # subset the data frame experim.data[rindex, cindex] ## Treatment Biomass ## 1 Control 284 ## 2 Control 328 ## 3 Control 291 This example extracts a subset of experim.data corresponding to rows 1 through 3, and columns “Treatment” and “Biomass”. The rindex is a numeric vector of row positions, and cindex is a character vector of column names. This shows that rows and columns can be selected by referencing their position or their names. The rows are not named in experim.data, so we specified the positions. Storing the index vectors first is quite a long-winded way of subsetting a data frame. However, there is nothing to stop us doing everything in one step: experim.data[1:3, c(&quot;Treatment&quot;, &quot;Biomass&quot;)] ## Treatment Biomass ## 1 Control 284 ## 2 Control 328 ## 3 Control 291 If we need to subset just rows or columns we just leave out the appropriate index vector: experim.data[1:3, ] ## Treatment Biomass Diversity Elevation ## 1 Control 284 8 364 ## 2 Control 328 12 294 ## 3 Control 291 11 321 The absence of an index vector before/after the comma indicates that we want to keep every row/column. Here we kept all the columns but only the first three rows. Be careful with [ Subsetting with the [rindex, cindex] construct produces another data frame. This should be apparent from the way the last example was printed. This is usually how this construct works. We say usually, because subsetting just one column produces a vector. This is very unfortunate, as it produces unpredictable behaviour if we’re not paying attention. The [ construct works with three types of index vectors. We’ve just seen that the index vector can be a numeric or character type. The third approach uses a logical index vector. For example, we can subset the experim.data data frame, keeping just the rows where the Treatment variable is equal to “Control”, using: # make a logical index vector rindex &lt;- experim.data $ Treatment == &quot;Control&quot; rindex ## [1] TRUE TRUE TRUE FALSE FALSE FALSE # experim.data[rindex, ] ## Treatment Biomass Diversity Elevation ## 1 Control 284 8 364 ## 2 Control 328 12 294 ## 3 Control 291 11 321 Notice that we construct the logical rindex vector by extracting the Treatment variable with the $ operator and using the == operator to test for equality with “Control”. Don’t worry too much if that seems confusing. We combined many different ideas in that example. We’re going to learn a much more transparent way to achieve the same result in later chapters. 9.5 Final words We’ve seen how to extract/add variables and subset data frames using the $, [[ and [ constructs. The last example also showed that we can use a combination of relational operators (e.g. ==, != or &gt;=) and the square brackets construct to subset a data frame according to one or more criteria. There are also a number of base R functions that allow us to manipulate data frames in a slightly more intuitive way. For example, there is a function called transform that adds new variables and changes existing ones, and a function called subset to select variables and subset rows in a data frame according to the values of its variables. We’ve shown these approaches because they’re still used by many people. However, we will rely on the dplyr package to handle operations like subsetting and transforming data frame variables in this book. The dplyr package provides a much cleaner, less error prone framework for manipulating data frames, and can be used to work with similar kinds of objects that store data in consistent way. Before we can do that though, we need to learn a little bit about how to organise and import data into R. "],
["working-directories-and-data-files.html", "Chapter 10 Working directories and data files 10.1 Introduction 10.2 Data files: the CSV format 10.3 The working directory 10.4 Importing data with read.csv 10.5 Importing data with RStudio (Avoid this!) 10.6 Package data", " Chapter 10 Working directories and data files 10.1 Introduction R is able to access data from a huge range of different data storage formats and repositories. With the right tools, we can use R to pull in data from various data bases, proprietary storage formats (e.g. Excel), online web sites, or plain old text files. We aren’t going to evaluate the many packages and functions used to pull data into R—a whole book could be written about this topic alone. Instead, we’re going to examine the simplest method for data import: reading in data from a text file. We’ll also briefly look at how to access data stored in packages. 10.2 Data files: the CSV format Just about every piece of software that stores data in some kind of table-like structure can export those data to a CSV file. The CSV acronym stands for “Comma Separated Values”. CSV files are just ordinary text files. The only thing that makes them a CSV file is the fact that they store data in a particular format. This format is very simple: each row of a CSV file corresponds to a row in the data, and each value in a row (corresponding to a different column) is separated by a comma. Here is what the artificial data from the last chapter looks like in CSV format: ## &quot;trt&quot;,&quot;bms&quot;,&quot;div&quot; ## &quot;Control&quot;,284,8 ## &quot;Control&quot;,328,12 ## &quot;Control&quot;,291,11 ## &quot;Fertilser&quot;,956,8 ## &quot;Fertilser&quot;,954,4 ## &quot;Fertilser&quot;,685,5 The first line contains the variable names, with each name separated by a comma. It’s usually a good idea to include the variable names in a CSV file, though this is optional. After the variable names, each new line is a row of data. Values which are not numbers have double quotation marks around them; numeric values lack these quotes. Notice that this is the same convention that applies to the elements of atomic vectors. Quoting non-numeric values is actually optional, but reading CSV files into R works best when non-numeric values are in quotes because this reduces ambiguity. 10.2.1 Exporting CSV files from Excel Those who work with small or moderate data sets (i.e. 100s-1000s of lines) often use Excel to manage and store their data. There are good reasons for why this isn’t necessarily a sensible thing to do—for example, Excel has a nasty habit of “helpfully” formatting data values. Nonetheless, Excel is a ubiquitous and convenient tool for data management, so it’s important to know how to pull data into R from Excel. It is possible to read data directly from Excel into R, but this way of doing things can be error prone for an inexperienced user and requires us to use an external package (the readxl package is currently the best option). Instead, the simplest way to transfer data from Excel to R is to first export the relevant worksheet to a CSV file, and then import this new file using R’s standard file import tools. We’ll discuss the import tools in a moment. The initial export step is just a matter of selecting the Excel worksheet that contains the relevant data, navigating to Save As..., choosing the Comma Separated Values (.csv), and following the familiar file save routine. That’s it. After following this step our data are free of Excel and ready to be read into R. Always check your Excel worksheet Importing data from Excel can turn into a frustrating process if we’re not careful. Most problems have their origin in the Excel worksheet used to store the data, rather than R. Problems usually arise because we haven’t been paying close attention to a worksheet. For example, imagine we’re working with a very simple data set, which contains three columns of data and a few hundred rows. If at some point we accidentally (or even intentionally) add a value to a cell in the forth column, Excel will assume the fourth column is “real” data. When we then export the worksheet to CSV, instead of the expected three columns of data, we end up with four columns, where most of the fourth column is just missing information. This kind of mistake is surprisingly common and is a frequent source of confusion. The take-home message is that when Excel is used to hold raw data should be used to do just that—the worksheet containing our raw data should hold only that, and nothing else. 10.3 The working directory Before we start worrying about data import, we first need to learn a bit about how R searches for the files that reside on our computer’s hard drive. The key concept is that of the “working directory”. A “directory” is just another word for “folder”. The working directory is simply a default location (i.e. a folder) R uses when searching for files. The working directory must always be set, and there are standard rules that govern how this is chosen when a new R session starts. For example, if we start R by double clicking on an script file (i.e. a file with a “.R” extension), R will typically set the working directory to be the location of the R script. We say typically, because this behaviour can be overridden. There’s no need to learn the rules for how the default working directory is chosen, because we can always use R/RStudio to find out which folder is currently set as the working directory. Here are a couple of options: When RStudio first starts up, the Files tab in the bottom right window shows us the contents (i.e. the files and folders) of the working directory. Be careful though, if we use the file viewer to navigate to a new location this does not change the working directory. The getwd function will print the location to the working directory to the Console. It does this by displaying a file path. If you’re comfortable with file paths then the output of getwd will make perfect sense. If not, it doesn’t matter too much. Use the RStudio Files tab instead. Why does any of this matter? We need to know where R will look for files if we plan to read in data. Fortunately, it’s easy to change the working directory to a new location if we need to do this: Using RStudio, we can set the working directory via the Session &gt; Set Working Directory... &gt; Choose Directory... menu. Once this menu item is selected we’re presented with the standard file/folder dialogue box to choose the working directory. Alternatively, we can use a function called setwd at the Console, though once again, we have to be comfortable with file paths to use this. Using RStudio is easier, so we won’t demonstrate how to use setwd. 10.4 Importing data with read.csv Now that we know roughly how a CSV file is formatted, and where R will look for such files, we need to understand how to read them into R. The standard R function for reading in a CSV file is called read.csv. There are a few other options (e.g. read_csv from the readr package), but we’ll use read.csv because it’s part of the base R distribution, which means we can use it without relying on an external package. The read.csv function does one thing: given the location of a CSV file, it will read the data into R and return it to us as a data frame. There are a couple of different strategies for using read.csv. One is considered good practice and is fairly robust. The second is widely used, but creates more problems than it solves. We’ll discuss both, and then explain why the first strategy is generally better than the second. 10.4.0.1 Strategy 1—set the working directory first Remember, the working directory is the default location used by R to search for files. This means that if we set the working directory to be wherever our data file lives, we can use the read.csv function without having to tell R where to look for it. Let’s assume our data is in a CSV file called “my-great-data.csv”. We should be able to see “my-great-data.csv” in the Files tab in RStudio if the working directory is set to its location. If we can’t see it there, the working directory still needs to be set (e.g. via Session &gt; Set Working Directory... &gt; Choose Directory...). Once we’ve set the working directory to this location, reading the “my-great-data.csv” file into R is simple: my_data &lt;- read.csv(file = &quot;my-great-data.csv&quot;, stringsAsFactors = FALSE) R knows where to find the file because we first set the working directory to be the location of the file. If we forget to do this R will complain and throw an error. We have to assign the output a name so that we can actually use the new data frame (my_data in this example), otherwise all that will happen is the resulting data frame is read in and printed to the Console. 10.4.0.2 Strategy 2—use the full path to the CSV file If we are comfortable with “file paths” then read.csv can be used without bothering to set the working directory. For example, if we have the CSV file called “my-great-data.csv” on the in a folder called “r_data”, then on a Unix machine we might read it into a data frame using something like: my_data &lt;- read.csv(file = &quot;~/r_data/my-great-data.csv&quot;, stringsAsFactors = FALSE) When used like this, we have to give read.csv the full path to the file. This assumes of course that we understand how to construct a file path—the details vary depending on the operating system. 10.4.0.3 Why use the first strategy? Both methods get to the same end point: after running the code at the Console we should end up with an object called my_data in the global environment, which is a data frame containing the data in the “my-great-data.csv” file. So why should we prefer Many novice R users with no experience of programming struggle with file paths, leading to a lot of frustration and wasted time trying to specify them. The first method only requires us to set the working directory with RStudio and know the name of the file we want to read in. There’s no need to deal with file paths. The second strategy creates problems when we move our data around or work on different machines, as the file paths will need to be changed in each new situation. The first strategy is robust to such changes. For example, if we move all our data to a new location, we just have to set the working directory to the new location and our R code will still work. 10.5 Importing data with RStudio (Avoid this!) It is also possible to import data from a CSV file into R using RStudio. The steps are as follows: Click on the Environment tab in the top right pane of RStudio Select Import Dataset &gt; From Text File... Select the CSV file to read into R and click Open Enter a name (no spaces allowed) or stick with the default and click Import We’re only pointing out this method because new users are often tempted to use it—we do not recommend it. Why? It creates the same kinds of problems as the second strategy discussed above. All RStudio does generate the correct usage of a function called read_csv (from the readr package) and evaluate this at the Console. The code isn’t part of a script so we have to do this every time we want to work with the data file. It’s easy to make a mistake using this approach, e.g. by accidentally misnaming the data frame or reading in the wrong data. It may be tempting to copy the generated R code into a script. However, we still have the portability problem outlined above to deal with. Take our word for it. The RStudio-focussed way of reading data into R just creates more problems than it solves. Don’t use it. 10.6 Package data Remember what Hadley Wickam said about packages? “… include reusable R functions, the documentation that describes how to use them, and sample data.” Many packages come with one or more sample data sets. These are very handy, as they’re used in examples and package vignettes. We can use the data function to get R to list the data sets hiding away in packages: data(package = .packages(all.available = TRUE)) The mysterious .packages(all.available = TRUE) part of this generates a character vector with the names of all the installed packages in it. If we only use data() then R only lists the data sets found in a package called datasets, and in packages we have loaded and attached in the current R session using the library function. The datasets package is part of the base R distribution. It exists only to store example data sets. The package is automatically loaded when we start R, i.e. there’s no need to use library to access it, meaning any data stored in this package can be accessed every time we start R. We’ll use a couple of data sets in the datasets package later to demonstrate how to work with the dplyr and ggplot2 packages. "],
["dplyr-and-the-tidy-data-concept.html", "Chapter 11 dplyr and the tidy data concept 11.1 Introduction 11.2 The value of dplyr 11.3 Tidy data 11.4 A quick look at dplyr", " Chapter 11 dplyr and the tidy data concept 11.1 Introduction Data wrangling refers to the process of manipulating raw data into the format that we want it in, for example for data visualisation or statistical analyses. There are a wide range of ways we may want to manipulate our data, for example by creating new variables, subsetting the data, or calculating summaries. Data wrangling is often a time consuming process. It is also not the most interesting part of any analysis - we are interested in answering biological questions, not in formatting data. However, it is a necessary step to go through to be able to conduct the analyses that we’re really interested in. Learning how to manipulate data efficiently can save us a lot of time and trouble and is therefore a really important skill to master. 11.2 The value of dplyr The dplyr package has been carefully designed to make life easier to manipulate data frames and other kinds of similar objects. A key reason for its ease-of-use is that dplyr is very consistent in the way its functions work. For example, the first argument of the main dplyr functions is always an object containing our data. This consistency makes it very easy to get to grips with each of the main dplyr functions—it’s often possible to understand how one works by seeing one or two examples of its use. A second reason for favouring dplyr is that it is orientated around a few core functions, each of which is designed to do one thing well. The key dplyr functions are often referred to as “verbs”, reflecting the fact that they “do something” to data. For example: (1) select is used to obtain a subset of variables; (2) mutate is used to construct new variables; (3) filter is used to obtain a subset of rows; (4) arrange is used to reorder rows; and (5) summarise is used to calculate information about groups. We’ll cover each of these verbs in detail in later chapters, as well as a few additional functions such as rename and group_by. Apart from being easy to use, dplyr is also fast compared to base R functions. This won’t matter much for the small data sets we use in this book, but dplyr is a good option for large data sets. The dplyr package also allows us to work with data stored in different ways, for example, by interacting directly with a number of database systems. We won’t work with anything other than data frames (and the closely-related “tibble”) but it is worth knowing about this facility. Learning to use dplyr with data frames makes it easy to work with these other kinds of data objects. A dplyr cheat sheet The developers of RStudio have produced a very usable cheat sheat that summarises the main data wrangling tools provided by dplyr. Our advice is to download this, print out a copy and refer to this often as you start working with dplyr. 11.3 Tidy data dplyr will work with any data frame, but it’s at its most powerful when our data are organised as tidy data. The word “tidy” has a very specific meaning in this context. Tidy data has a specific structure that makes it easy to manipulate, model and visualise. A tidy data set is one where each variable is in only one column and each row contains only one observation. This might seem like the “obvious” way to organise data, but many people fail to adopt this convention. We aren’t going to explore the tidy data concept in great detail, but the basic principles are not difficult to understand. We’ll use an example to illustrate what the “one variable = one column” and “one observation = one row” idea means. Let’s return to the made-up experiment investigating the response of communities to fertilizer addition. This time, imagine we had only measured biomass, but that we had measured it twice over the course of the experiment. We’ll examine some artificial data for the experiment and look at two ways to organise it to help us understand the tidy data idea. The first way uses a separate column for each biomass measurement: ## Treatment BiomassT1 BiomassT2 ## 1 Control 284 324 ## 2 Control 328 400 ## 3 Control 291 355 ## 4 Fertilser 956 1197 ## 5 Fertilser 954 1012 ## 6 Fertilser 685 859 This often seems like the natural way to store such data, especially for experienced Excel users. However, this format is not tidy. Why? The biomass variable has been split across two columns (“BiomassT1” and “BiomassT2”), which means each row corresponds to two observations. We won’t go into the “whys” here, but take our word for it: adopting this format makes it difficult to efficiently work with data. This is not really an R-specific problem. This non-tidy format is sub-optimal in many different data analysis environments. A tidy version of the example data set would still have three columns, but now these would be: “Treatment”, denoting the experimental treatment applied; “Time”, denoting the sampling occasion; and “Biomass”, denoting the biomass measured: ## Treatment Time Biomass ## 1 Control T1 284 ## 2 Control T1 328 ## 3 Control T1 291 ## 4 Fertilser T1 956 ## 5 Fertilser T1 954 ## 6 Fertilser T1 685 ## 7 Control T2 324 ## 8 Control T2 400 ## 9 Control T2 355 ## 10 Fertilser T2 1197 ## 11 Fertilser T2 1012 ## 12 Fertilser T2 859 These data are tidy: each variable is in only one column, and each observation has its own unique row. These data are well-suited to use with dplyr. Always try to start with tidy data The best way to make sure your data set is tidy is to store in that format when it’s first collected and recorded. There are packages that can help convert non tidy data into the tidy data format (e.g. the tidyr package), but life is much simpler if we just make sure our data are tidy from the very beginning. 11.4 A quick look at dplyr We’ll finish up this chapter by taking a quick look at a few features of the dplyr package, before really drilling down into how it works. The package is not part of the base R installation, so we have to install it first via install.packages(&quot;dplyr&quot;). Remember, we only have to install dplyr once, so there’s no need to leave the install.packages line in script that uses the package. We do have to add library to the top of any scripts using the package to load and attach it: library(&quot;dplyr&quot;) We need some data to work with. We’ll use two data sets to illustrate the key ideas in the next few chapters: the iris data set in the datasets package and the storms data set in the nasaweather package. The datasets package ships with R and is loaded and attached at start up, so there’s no need to do anything to make iris available. The nasaweather package doesn’t ship with R so it needs to be installed via install.packages(&quot;nasaweather&quot;). Finally, we have to add library to the top of our script to load and attach the package: library(&quot;nasaweather&quot;) The nasaweather package is a bare bones data package. It doesn’t contain any new R functions, just data. We’ll be using the storms data set from nasaweather: this contains information about tropical storms in North America (from 1995-2000). We’re just using it as a convenient example to illustrate the workings of the dplyr, and later, the ggplot2 packages. 11.4.1 Tibble (tbl) objects The primary purpose of the dplyr package is to make it easier to manipulate data interactively. In order to facilitate this kind of work dplyr implements a special kind of data object known as a tbl (pronounced “tibble”). We can think of a tibble as a special data frame with a few extra whistles and bells. We can convert an ordinary data frame to a a tibble using the tbl_df function. It’s a good idea (though not necessary) to convert ordinary data frames to tibbles. Why? When a data frame is printed to the Console R will try to print every column and row until it reaches a (very large) maximum permitted amount of output. The result is a mess of text that’s virtually impossible to make sense of. In contrast, when a tibble is printed to the Console, it does so in a compact way. To see this, we can convert the iris data set to a tibble using tbl_df and then print the resulting object to the Console: # make a &quot;tibble&quot; copy of iris iris_tbl &lt;- tbl_df(iris) # print it iris_tbl ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows Notice that only the first 10 rows are printed. This is much nicer than trying to wade through every row of a data frame. 11.4.2 The glimpse function Sometimes we just need a quick, compact summary of a data frame or tibble. This is the job of the glimpse function from dplyr. The glimpse function is very similar to str: glimpse(iris_tbl) ## Observations: 150 ## Variables: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9,... ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1,... ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5,... ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1,... ## $ Species &lt;fctr&gt; setosa, setosa, setosa, setosa, setosa, setosa, ... The function takes one argument: the name of a data frame or tibble. It then tells us how many rows it has, how many variables there are, what these variables are called, and what kind of data are associated with each variable. This function is useful when we’re working with a data set containing many variables. "],
["working-with-variables.html", "Chapter 12 Working with variables 12.1 Introduction 12.2 Subset variables with select 12.3 Creating variables with mutate", " Chapter 12 Working with variables 12.1 Introduction This chapter will explore the the dplyr select and mutate verbs, as well as the related rename and transmute verbs. These four verbs are considered together because they all operate on the variables (i.e. the columns) of a data frame or tibble: The select function selects a subset of variables to retain and (optionally) renames them in the process. The mutate function creates new variables from preexisting ones and retains the original variables. The rename function renames one or more variables while keeping the remaining variable names unchanged. The transmute function creates new variables from preexisting ones and drops the original variables. 12.1.1 Getting ready A script that uses dplyr will typically start by loading and attaching the package: library(&quot;dplyr&quot;) Obviously we need to have first installed dplyr package for this to work. We’ll use the iris data set in the datasets package to illustrate the ideas in this chapter. The datasets package ships with R and is loaded and attached at start up, so there’s no need to do anything to make iris available. The iris data set is an ordinary data frame. Before we start working, it’s handy to convert this to a tibble so that it prints to the Console in a compact way: iris_tbl &lt;- tbl_df(iris) We gave the new tibble version a new name. We didn’t have to do this, but it will remind us that we’re working with tibbles. 12.2 Subset variables with select We use select to select variables from a data frame or tibble. This is typically used when we have a data set with many variables but only need to work with a subset of these. Basic usage of select looks like this: select(data_set, vname1, vname2, ...) take note: this is not an example we can run. This is a “pseudo code” example, designed to show, in abstract terms, how we use select: The first argument, data_set (“data object”), must be the name of the object containing our data. We then include a series of one or more additional arguments, where each one is the name of a variable in data_set. We’ve expressed this as vname1, vname2, ..., where vname1 and vname2 are names of the first two variables, and the ... is acting as placeholder for the remaining variables (there could be any number of these). It’s easiest to understand how a function like select works by seeing it in action. We select the Species, Petal.Length and Petal.Width variables from iris_tbl like this: select(iris_tbl, Species, Petal.Length, Petal.Width) ## # A tibble: 150 x 3 ## Species Petal.Length Petal.Width ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 0.2 ## 2 setosa 1.4 0.2 ## 3 setosa 1.3 0.2 ## 4 setosa 1.5 0.2 ## 5 setosa 1.4 0.2 ## 6 setosa 1.7 0.4 ## 7 setosa 1.4 0.3 ## 8 setosa 1.5 0.2 ## 9 setosa 1.4 0.2 ## 10 setosa 1.5 0.1 ## # ... with 140 more rows Hopefully nothing about this example is surprising or confusing. There are a few things to notice about how select works though: The select function is one of those non-standard functions we briefly mentioned in the Using functions chapter. This means the variable names should not be surrounded by quotes unless they have spaces in them (which is best avoided). The select function is just like other R functions: it does not have “side effects”. What this means is that it does not change the original iris_tbl. We printed the result produced by select to the Console, so we can’t access the new data set. If we need to access the result we have to assign it a name using &lt;-. The order of variables (i.e. the column order) in the resulting object is the same as the order in which they were supplied as arguments. This means we can reorder variables at the same time as selecting them if we need to. The select function will return the same kind of data object it is working on. It returns a data frame if our data was originally in a data frame and a tibble if it was a tibble. In this example, R prints a tibble because we had converted iris_tbl from a data frame to a tibble. It’s sometimes more convenient to use select to subset variables by specifying those we do not need, rather than specifying of the ones to keep. We use the - operator indicate that variables should be dropped. For example, to drop the Petal.Width and Petal.Length columns, we use: select(iris_tbl, -Petal.Width, -Petal.Length) ## # A tibble: 150 x 3 ## Sepal.Length Sepal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 5.1 3.5 setosa ## 2 4.9 3.0 setosa ## 3 4.7 3.2 setosa ## 4 4.6 3.1 setosa ## 5 5.0 3.6 setosa ## 6 5.4 3.9 setosa ## 7 4.6 3.4 setosa ## 8 5.0 3.4 setosa ## 9 4.4 2.9 setosa ## 10 4.9 3.1 setosa ## # ... with 140 more rows This returns a tibble with just the remaining variables:Sepal.Length, Sepal.Width and Species. The select function can also be used to grab (or drop) a set of variables that occur in a sequence next to one another. We specify a series of adjacent variables using the : operator. This must be used with two variable names, one on the left hand side and one on the right. When we use : like this, select will subset both those variables along with any others that fall in between them. For example, if we need the two Petal variables and Species, we use: select(iris_tbl, Petal.Length:Species) ## # A tibble: 150 x 3 ## Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 1.4 0.2 setosa ## 2 1.4 0.2 setosa ## 3 1.3 0.2 setosa ## 4 1.5 0.2 setosa ## 5 1.4 0.2 setosa ## 6 1.7 0.4 setosa ## 7 1.4 0.3 setosa ## 8 1.5 0.2 setosa ## 9 1.4 0.2 setosa ## 10 1.5 0.1 setosa ## # ... with 140 more rows The : operator can be combined with - if we need to drop a series of variables according to their position in a data frame or tibble: select(iris_tbl, -(Petal.Length:Species)) ## # A tibble: 150 x 2 ## Sepal.Length Sepal.Width ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 ## 6 5.4 3.9 ## 7 4.6 3.4 ## 8 5.0 3.4 ## 9 4.4 2.9 ## 10 4.9 3.1 ## # ... with 140 more rows The extra ( ) around are Petal.Length:Species important here — select will throw an error if we don’t include them. 12.2.1 Renaming variables with select and rename In addition to selecting a subset of variables, the select function can also rename variables at the same time. To do this, we have to name the arguments using =, placing the new name on the left hand side. For example, to select theSpecies, Petal.Length and Petal.Width variables from iris_tbl, but also rename Petal.Length and Petal.Width to Petal_Length and Petal_Width, we use: select(iris_tbl, Species, Petal_Length = Petal.Length, Petal_Width = Petal.Width) ## # A tibble: 150 x 3 ## Species Petal_Length Petal_Width ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 0.2 ## 2 setosa 1.4 0.2 ## 3 setosa 1.3 0.2 ## 4 setosa 1.5 0.2 ## 5 setosa 1.4 0.2 ## 6 setosa 1.7 0.4 ## 7 setosa 1.4 0.3 ## 8 setosa 1.5 0.2 ## 9 setosa 1.4 0.2 ## 10 setosa 1.5 0.1 ## # ... with 140 more rows Renaming variables is a common task when working with data frames and tibbles. What should we do if the only thing we would like to achieve is to rename a variables, rather than rename and select variables? The dplyr provides an additional function called rename for this purpose. This function renames certain variables while retaining all others. It works in a similar way to select. For example, to rename Petal.Length and Petal.Width to Petal_Length and Petal_Width, we use: rename(iris_tbl, Petal_Length = Petal.Length, Petal_Width = Petal.Width) ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal_Length Petal_Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows Notice that the rename function also preserves the order of the variables found in the original data. 12.3 Creating variables with mutate We use mutate to add new variables to a data frame or tibble. This is useful if we need to construct one or more derived variables to support an analysis. Basic usage of mutate looks like this: mutate(data_set, &lt;expression1&gt;, &lt;expression2&gt;, ...) Again, this is not an example we can run; it’s pseudo code that highlights in abstract terms how to use mutate. As always with dplyr, the first argument, data_set, should be the name of the object containing our data. We then include a series of one or more additional arguments, where each of these is a valid R expression involving one or more variables in data_set. We’ve have expressed these as &lt;expression1&gt;, &lt;expression2&gt;, where &lt;expression1&gt; and &lt;expression2&gt; represent the first two expressions, and the ... is acting as placeholder for the remaining expressions. Remember, this is not valid R code. It is just intended to demonstrate the general usage of mutate. To see mutate in action, let’s construct a new version of iris_tbl that contains a variable summarising the approximate area of sepals: mutate(iris_tbl, Sepal.Width * Sepal.Length) ## Warning: package &#39;bindrcpp&#39; was built under R version 3.3.2 ## # A tibble: 150 x 6 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows, and 1 more variables: `Sepal.Width * ## # Sepal.Length` &lt;dbl&gt; This creates a copy of iris_tbl with a new column called Sepal.Width * Sepal.Length (mentioned at the bottom of the printed output). Most of the rules that apply to select also apply to mutate: The expression that performs the required calculation is not surrounded by quotes. This makes sense, because an expression is meant to be evaluated so that it “does something”. It is not a value. Once again, we just printed the result produced by mutate to the Console, rather than assigning the result a name using &lt;-. The mutate function does not have side effects, meaning it does not change the original iris_tbl in any way. The select function returns the same kind of data object as the one it is working on: a data frame if our data was originally in a data frame, a tibble if it was a tibble. Creating a variable called something like Sepal.Width * Sepal.Length is not exactly ideal because it’s a difficult name to work with. The mutate function can name variables at the same time as they are created. We have to name the arguments using =, placing the name on the left hand side, to do this. Here’s how to use this construct to name the new area variable Sepal.Area: mutate(iris_tbl, Sepal.Area = Sepal.Width * Sepal.Length) ## # A tibble: 150 x 6 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Area ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 17.85 ## 2 4.9 3.0 1.4 0.2 setosa 14.70 ## 3 4.7 3.2 1.3 0.2 setosa 15.04 ## 4 4.6 3.1 1.5 0.2 setosa 14.26 ## 5 5.0 3.6 1.4 0.2 setosa 18.00 ## 6 5.4 3.9 1.7 0.4 setosa 21.06 ## 7 4.6 3.4 1.4 0.3 setosa 15.64 ## 8 5.0 3.4 1.5 0.2 setosa 17.00 ## 9 4.4 2.9 1.4 0.2 setosa 12.76 ## 10 4.9 3.1 1.5 0.1 setosa 15.19 ## # ... with 140 more rows We can create more than one variable by supplying mutate multiple (named) arguments: mutate(iris_tbl, Sepal.Area = Sepal.Width * Sepal.Length, Petal.Area = Petal.Width * Petal.Length, Area.Ratio = Petal.Area / Petal.Area) ## # A tibble: 150 x 8 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Area ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 17.85 ## 2 4.9 3.0 1.4 0.2 setosa 14.70 ## 3 4.7 3.2 1.3 0.2 setosa 15.04 ## 4 4.6 3.1 1.5 0.2 setosa 14.26 ## 5 5.0 3.6 1.4 0.2 setosa 18.00 ## 6 5.4 3.9 1.7 0.4 setosa 21.06 ## 7 4.6 3.4 1.4 0.3 setosa 15.64 ## 8 5.0 3.4 1.5 0.2 setosa 17.00 ## 9 4.4 2.9 1.4 0.2 setosa 12.76 ## 10 4.9 3.1 1.5 0.1 setosa 15.19 ## # ... with 140 more rows, and 2 more variables: Petal.Area &lt;dbl&gt;, ## # Area.Ratio &lt;dbl&gt; Notice that here we placed each argument on a new line, remembering the comma to separate arguments. There is nothing to stop us doing this because R ignores white space. This is useful though, because it allows us, the user, to makes long function calls easier to read by breaking them up on different lines. This last example reveals a nice feature of mutate: we can use newly created variables in further calculations. Here we constructed approximate sepal and petal area variables, and then used these to construct a third variable containing the ratio of these two quantities, Area.Ratio. 12.3.1 Transforming and dropping variables Occasionally we may want to construct one or more new variables, and then drop all other variables in the original dataset. The transmute function is designed to do this. It works exactly like mutate, but it has a slightly different behaviour: transmute(iris_tbl, Sepal.Area = Sepal.Width * Sepal.Length, Petal.Area = Petal.Width * Petal.Length, Area.Ratio = Petal.Area / Petal.Area) ## # A tibble: 150 x 3 ## Sepal.Area Petal.Area Area.Ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 17.85 0.28 1 ## 2 14.70 0.28 1 ## 3 15.04 0.26 1 ## 4 14.26 0.30 1 ## 5 18.00 0.28 1 ## 6 21.06 0.68 1 ## 7 15.64 0.42 1 ## 8 17.00 0.30 1 ## 9 12.76 0.28 1 ## 10 15.19 0.15 1 ## # ... with 140 more rows Here we repeated the previous example, but now only the new variables were retained in the resulting tibble. If we also want to retain one or more variables without altering them we just have to pass them as unnamed arguments. For example, if we need to retain species identity in the output, we use: transmute(iris_tbl, Species, Sepal.Area = Sepal.Width * Sepal.Length, Petal.Area = Petal.Width * Petal.Length, Area.Ratio = Petal.Area / Petal.Area) ## # A tibble: 150 x 4 ## Species Sepal.Area Petal.Area Area.Ratio ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 17.85 0.28 1 ## 2 setosa 14.70 0.28 1 ## 3 setosa 15.04 0.26 1 ## 4 setosa 14.26 0.30 1 ## 5 setosa 18.00 0.28 1 ## 6 setosa 21.06 0.68 1 ## 7 setosa 15.64 0.42 1 ## 8 setosa 17.00 0.30 1 ## 9 setosa 12.76 0.28 1 ## 10 setosa 15.19 0.15 1 ## # ... with 140 more rows "],
["working-with-observations.html", "Chapter 13 Working with observations 13.1 Introduction 13.2 Subset observations with filter 13.3 Reording observations with arrange", " Chapter 13 Working with observations 13.1 Introduction This chapter will explore the filter and arrange verbs. These are discussed together because they are used to manipulate observations (i.e. rows) of a data frame or tibble: The filter function extracts a subset of observations based on supplied conditions involving the variables in our data. The arrange function reorders the rows according to the values in one or more variables. 13.1.1 Getting ready We should start a new script by loading and attaching the dplyr package: library(&quot;dplyr&quot;) We’re going to use the storms data set in the nasaweather package this time. This means we need to load and attach the nasaweather package to make storms available: library(&quot;nasaweather&quot;) The storms data set is an ordinary data frame, so let’s convert it to a tibble so that it prints nicely: storms_tbl &lt;- tbl_df(storms) 13.2 Subset observations with filter We use filter to subset observations in a data frame or tibble containing our data. This is often done when we want to limit an analysis to a subset of observations. Basic usage of filter looks something like this: filter(data_set, &lt;expression1&gt;, &lt;expression1&gt;, ...) Remember, this is pseudo code (it’s not an example we can run). The first argument, data_set, must the name of the object containing our data. We then include one or more additional arguments, where each of these is a valid R expression involving one or more variables in data_set. Each expression must return a logical vector. We’ve expressed these as &lt;expression1&gt;, &lt;expression2&gt;, ..., where &lt;expression1&gt; and &lt;expression2&gt; represent the first two expressions, and the ... is acting as placeholder for the remaining expressions. To see how filter works in action, we’ll use it to subset observations in the storms_tbl dataset, based on two relational criteria: filter(storms_tbl, pressure &lt;= 960, wind &gt;= 100) ## # A tibble: 199 x 11 ## name year month day hour lat long pressure wind type ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Felix 1995 8 12 0 22.1 -57.8 955 100 Hurricane ## 2 Felix 1995 8 12 6 22.9 -59.0 943 110 Hurricane ## 3 Felix 1995 8 12 12 23.6 -60.2 932 115 Hurricane ## 4 Felix 1995 8 12 18 24.3 -61.0 929 120 Hurricane ## 5 Felix 1995 8 13 0 25.1 -61.6 930 115 Hurricane ## 6 Felix 1995 8 13 6 25.9 -61.9 937 105 Hurricane ## 7 Felix 1995 8 13 12 26.6 -62.3 942 100 Hurricane ## 8 Luis 1995 9 1 6 15.8 -42.6 958 105 Hurricane ## 9 Luis 1995 9 1 12 16.2 -43.6 950 115 Hurricane ## 10 Luis 1995 9 1 18 16.5 -44.7 948 115 Hurricane ## # ... with 189 more rows, and 1 more variables: seasday &lt;int&gt; In this example we’ve created a subset of storms_tbl that only includes observations where the pressure variable is less than or equal to 960 and the wind variable is greater than or equal to 100. Both conditions must be met for an observation to be included in the resulting tibble. The conditions are not combined as an either/or operation. This is probably starting to become repetitious, but there are a few features of filter that we should note: Each expression that performs a comparison is not surrounded by quotes. This makes sense, because the expression is meant to be evaluated to return a logical vector – it is not “a value”. As usual, the result produced by mutate in our example was printed to the Console. The mutate function did not change the original storms_tbl in any way (no side effects!). The filter function will return the same kind of data object it is working on: it returns a data frame if our data was originally in a data frame, and a tibble if it was a tibble. We can achieve the same result as the above example in a different way. This involves the &amp; operator: filter(storms_tbl, pressure &lt;= 960 &amp; wind &gt;= 100) ## # A tibble: 199 x 11 ## name year month day hour lat long pressure wind type ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Felix 1995 8 12 0 22.1 -57.8 955 100 Hurricane ## 2 Felix 1995 8 12 6 22.9 -59.0 943 110 Hurricane ## 3 Felix 1995 8 12 12 23.6 -60.2 932 115 Hurricane ## 4 Felix 1995 8 12 18 24.3 -61.0 929 120 Hurricane ## 5 Felix 1995 8 13 0 25.1 -61.6 930 115 Hurricane ## 6 Felix 1995 8 13 6 25.9 -61.9 937 105 Hurricane ## 7 Felix 1995 8 13 12 26.6 -62.3 942 100 Hurricane ## 8 Luis 1995 9 1 6 15.8 -42.6 958 105 Hurricane ## 9 Luis 1995 9 1 12 16.2 -43.6 950 115 Hurricane ## 10 Luis 1995 9 1 18 16.5 -44.7 948 115 Hurricane ## # ... with 189 more rows, and 1 more variables: seasday &lt;int&gt; Once again, we created a subset of storms_tbl that only includes observation where the pressure variable is less than or equal to 960 and the wind variable is greater than or equal to 100. However, rather than supplying pressure &lt;= 960 and wind &gt;= 100 as two arguments, we used a single R expression, combining them with the &amp;. We’re pointing this out because we sometimes need to subset on an either/or basis, and in those cases we have to use this second approach. For example: filter(storms_tbl, pressure &lt;= 960 | wind &gt;= 100) ## # A tibble: 266 x 11 ## name year month day hour lat long pressure wind type ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Felix 1995 8 12 0 22.1 -57.8 955 100 Hurricane ## 2 Felix 1995 8 12 6 22.9 -59.0 943 110 Hurricane ## 3 Felix 1995 8 12 12 23.6 -60.2 932 115 Hurricane ## 4 Felix 1995 8 12 18 24.3 -61.0 929 120 Hurricane ## 5 Felix 1995 8 13 0 25.1 -61.6 930 115 Hurricane ## 6 Felix 1995 8 13 6 25.9 -61.9 937 105 Hurricane ## 7 Felix 1995 8 13 12 26.6 -62.3 942 100 Hurricane ## 8 Felix 1995 8 13 18 27.4 -62.3 947 95 Hurricane ## 9 Felix 1995 8 14 0 28.2 -62.5 948 90 Hurricane ## 10 Felix 1995 8 14 6 29.0 -62.9 954 80 Hurricane ## # ... with 256 more rows, and 1 more variables: seasday &lt;int&gt; This creates a subset of storms_tbl that only includes observation where the pressure variable is less than or equal to 960 or the wind variable is greater than or equal to 100. We’re also not restricted to using some combination of relational operators such as ==, &gt;= or != when working with filter. The conditions specified in the filter function can be any expression that returns a logical vector. The only constraint is that the length of this logical vector has to equal the length of its input vectors. Here’s an example. The group membership %in% operator (part of base R, not dplyr) is used to determine whether the values in one vector occurs among the values in a second vector. It’s used like this: vec1 %in% vec2. This returns a vector where the values are TRUE if an element of vec1 is in vec2, and FALSE otherwise. We can use the %in% operator with filter to select to subset rows by the values of one or more variables: sub_storms_tbl &lt;- filter(storms_tbl, name %in% c(&quot;Roxanne&quot;, &quot;Marilyn&quot;, &quot;Dolly&quot;)) # print the output sub_storms_tbl $ name ## [1] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [8] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [15] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [22] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [29] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [36] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [43] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [50] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [57] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [64] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [71] &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; &quot;Marilyn&quot; ## [78] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [85] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [92] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [99] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [106] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [113] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [120] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; ## [127] &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Roxanne&quot; &quot;Dolly&quot; &quot;Dolly&quot; ## [134] &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; ## [141] &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; ## [148] &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; &quot;Dolly&quot; ## [155] &quot;Dolly&quot; 13.3 Reording observations with arrange We use arrange to reorder the rows of an object containing our data. This is sometimes used when we want to inspect a dataset to look for associations among the different variables. This is hard to do if they are not ordered. Basic usage of arrange looks like this: arrange(data_set, vname1, vname2, ...) Yes, this is pseudo-code. As always, the first argument, data_set, is the name of the object containing our data. We then include a series of one or more additional arguments, where each of these should be the name of a variable in data_set: vname1 and vname2 are names of the first two ordering variables, and the ... is acting as placeholder for the remaining variables. To see arrange in action, let’s construct a new version of storms_tbl where the rows have been reordered first by wind, and then by pressure: arrange(storms_tbl, wind, pressure) ## # A tibble: 2,747 x 11 ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Fran 1996 9 9 12 45.7 -72.3 1006 15 ## 2 Fran 1996 9 9 18 46.0 -71.1 1008 15 ## 3 Fran 1996 9 10 0 46.7 -70.0 1010 15 ## 4 Frances 1998 9 13 6 31.7 -96.9 1002 20 ## 5 Dean 1995 7 31 18 30.5 -96.5 1003 20 ## 6 Erin 1995 8 4 12 33.2 -89.7 1003 20 ## 7 Erin 1995 8 4 18 34.1 -90.2 1003 20 ## 8 Erin 1995 8 5 0 34.8 -90.2 1003 20 ## 9 Erin 1995 8 5 6 35.4 -90.1 1003 20 ## 10 Erin 1995 8 5 12 36.3 -89.8 1003 20 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; This creates a new version of storms_tbl where the rows are sorted according to the values of wind and pressure in ascending order – i.e. from smallest to largest. Since wind appears before pressure among the arguments, the values of pressure are only used to break ties within any particular value of wind. For the sake of avoiding any doubt about how arrange works, let’s quickly review its behaviour: The variable names used as arguments of arrange are not surrounded by quotes. The arrange function did not change the original iris_tbl in any way. The arrange function will return the same kind of data object it is working on. There isn’t much else we need to to learn about arrange. By default, it sorts variables in ascending order. If we need it to sort a variable in descending order, we wrap the variable name in the desc function: arrange(storms_tbl, wind, desc(pressure)) ## # A tibble: 2,747 x 11 ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Fran 1996 9 10 0 46.7 -70.0 1010 15 ## 2 Fran 1996 9 9 18 46.0 -71.1 1008 15 ## 3 Fran 1996 9 9 12 45.7 -72.3 1006 15 ## 4 Barry 1995 7 5 6 32.0 -72.0 1019 20 ## 5 Barry 1995 7 5 12 32.0 -72.0 1019 20 ## 6 Barry 1995 7 5 18 31.9 -72.0 1018 20 ## 7 Marilyn 1995 9 30 12 34.6 -49.3 1016 20 ## 8 Marilyn 1995 9 30 18 34.7 -50.0 1016 20 ## 9 Marilyn 1995 10 1 0 34.8 -50.5 1016 20 ## 10 Marilyn 1995 10 1 6 35.0 -51.0 1016 20 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; This creates a new version of storms_tbl where the rows are sorted according to the values of wind and pressure, in ascending and descending order, respectively. "],
["helper-functions.html", "Chapter 14 Helper functions 14.1 Introduction 14.2 Working with select 14.3 Working with mutate and transmute 14.4 Working with filter", " Chapter 14 Helper functions 14.1 Introduction There are a number of helper functions supplied by dplyr. Many of these are shown in the handy dplyr cheat sheat. This is a short chapter. We aren’t going to try to cover every single helper function here. Instead, we’ll highlight some of the more useful ones, and point out where the others tend to be used. We also assume that the storms_tbl and iris_tbl tibbles have already been constructed (look over the previous two chapters to see how this is done). 14.2 Working with select There are relatively few helper functions that can be used with select. The job of these functions is to make it easier to match variable names according to various criteria. We’ll look at the three simplest of these, but look at the examples in the help file for select and the cheat sheat to see what else is available. We can select variables according to the sequence of characters used at the start of their name with the starts_with function. For example, to select all the variables in iris_tbl that begin with the word “Petal”, we use: select(iris_tbl, starts_with(&quot;petal&quot;)) ## # A tibble: 150 x 2 ## Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.4 0.2 ## 2 1.4 0.2 ## 3 1.3 0.2 ## 4 1.5 0.2 ## 5 1.4 0.2 ## 6 1.7 0.4 ## 7 1.4 0.3 ## 8 1.5 0.2 ## 9 1.4 0.2 ## 10 1.5 0.1 ## # ... with 140 more rows This returns a table containing just Petal.Length and Petal.Width. As one might expect, there is also a helper function to select variables according to characters used at the end of their name. This is the ends_with function (no surprises here). To select all the variables in iris_tbl that end with the word “Length”, we use: select(iris_tbl, ends_with(&quot;length&quot;)) ## # A tibble: 150 x 2 ## Sepal.Length Petal.Length ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 1.4 ## 2 4.9 1.4 ## 3 4.7 1.3 ## 4 4.6 1.5 ## 5 5.0 1.4 ## 6 5.4 1.7 ## 7 4.6 1.4 ## 8 5.0 1.5 ## 9 4.4 1.4 ## 10 4.9 1.5 ## # ... with 140 more rows Notice that we have to quote the character string that we want to match against. This is not optional. However, the starts_with and ends_with functions are not case sensitive by default. For example, I passed starts_with the argument &quot;petal&quot; instead of &quot;Petal&quot;, yet it still selected variables beginning with the character string &quot;Petal&quot;. If we want to select variables on a case-sensitive basis, we need to set an argument ignore.case to FALSE in starts_with and ends_with. The last select helper function we will look at is called contains. This allows us to select variables based on a partial match anywhere in their name. Look at what happens if we pass contains the argument &quot;.&quot;: select(iris_tbl, contains(&quot;.&quot;)) ## # A tibble: 150 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5.0 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ... with 140 more rows This selects all the variables with a dot in their name. There is nothing to stop us combining the different variable selection methods. For example, we can use this approach to select all the variables whose names start with the word “Petal” or end with the word “Length”: select(iris_tbl, ends_with(&quot;length&quot;), starts_with(&quot;petal&quot;)) ## # A tibble: 150 x 3 ## Sepal.Length Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 1.4 0.2 ## 2 4.9 1.4 0.2 ## 3 4.7 1.3 0.2 ## 4 4.6 1.5 0.2 ## 5 5.0 1.4 0.2 ## 6 5.4 1.7 0.4 ## 7 4.6 1.4 0.3 ## 8 5.0 1.5 0.2 ## 9 4.4 1.4 0.2 ## 10 4.9 1.5 0.1 ## # ... with 140 more rows When we apply more than one selection criteria like this the select function returns all the variables that match either criteria, rather than just the set that meets all the criteria. 14.3 Working with mutate and transmute There are quite a few helper functions that can be used with mutate. These make it easier to carry out certain transformations that aren’t easy to do with base R functions. We won’t explore these here as they tend to be needed only in quite specific circumstances. However, in situations where we need to construct an unusual variable—for example, one that ranks the values of another variable—it’s always worth looking at the that handy cheat sheat to see what options might be available. 14.4 Working with filter There’s one dplyr helper function that works with filter that’s definitely worth knowing about: the between function. This is used to identify the values of a variable that lie inside a defined range: filter(storms_tbl, between(pressure, 960, 970)) ## # A tibble: 213 x 11 ## name year month day hour lat long pressure wind type ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Felix 1995 8 11 18 21.3 -56.5 965 90 Hurricane ## 2 Felix 1995 8 14 12 29.9 -63.4 962 80 Hurricane ## 3 Felix 1995 8 14 18 30.7 -64.1 962 75 Hurricane ## 4 Felix 1995 8 15 0 31.3 -65.1 962 75 Hurricane ## 5 Felix 1995 8 15 6 31.9 -66.2 964 75 Hurricane ## 6 Felix 1995 8 15 12 32.5 -67.4 968 70 Hurricane ## 7 Felix 1995 8 15 18 33.1 -68.8 965 70 Hurricane ## 8 Felix 1995 8 16 0 33.5 -70.1 963 70 Hurricane ## 9 Felix 1995 8 16 6 34.0 -71.3 966 70 Hurricane ## 10 Felix 1995 8 16 12 34.6 -72.4 968 70 Hurricane ## # ... with 203 more rows, and 1 more variables: seasday &lt;int&gt; This example filters the storms dataset such that only values of pressure between 960 and 970 are retained. We could do the same thing using some combination of &gt; or &lt;, but the between function makes things a bit easier to read. "],
["grouping-and-summarising-data.html", "Chapter 15 Grouping and summarising data 15.1 Summarising variables with summarise 15.2 Grouped operations using group_by 15.3 Removing grouping information", " Chapter 15 Grouping and summarising data This chapter will explore the summarise and group_by verbs. These two verbs are considered together because they are often used together, and their usage is quite distinct from the other dplyr verbs we’ve encountered: The group_by function adds information into a data object (e.g. a data frame or tibble), which makes subsequent calculations happen on a group-specific basis. The summarise function is a data reduction function calculates single-number summaries of one or more variables, respecting the group structure if present. 15.0.1 Getting ready We can start a new script by loading and attaching the dplyr package: library(&quot;dplyr&quot;) We’re going to use both the storms and iris data sets in the nasaweather and datasets packages, respectively. The datasets package ships is automatically loaded and attached at start up, so we need to make the nasaweather package available: library(&quot;nasaweather&quot;) Finally, let’s convert both data sets to a tibble so they print to the Console cleanly: storms_tbl &lt;- tbl_df(storms) iris_tbl &lt;- tbl_df(iris) 15.1 Summarising variables with summarise We use summarise to calculate summaries of variables in an object containing our data. We do this kind of calculation all the time when analysing data. In terms of pseudo-code, usage of summarise looks like this: summarise(data_set, &lt;expression1&gt;, &lt;expression2&gt;, ...) The first argument, data_set, must be the name of the data frame or tibble containing our data. We then include a series of one or more additional arguments, each of these is a valid R expression involving at least one variable in data_set. These are given by the pseudo-code placeholder &lt;expression1&gt;, &lt;expression2&gt;, ..., where &lt;expression1&gt; and &lt;expression2&gt; represent the first two expressions, and the ... is acting as placeholder for the remaining expressions. These expressions can be any calculation involving R functions. The only constraint is that they must generate a single value when evaluated. That last sentence was important. It’s easy to use summarise if can we remember one thing: summarise is designed to work with functions that take a vector as their input and return a single value (i.e. a vector of length one). Any calculation that does this can be used with summarise. The summarise verb is best understood by example. The R function called mean takes a vector of numbers (several numbers) and calculates their arithmetic mean (one number). We can use mean with summarise to calculate the mean of the Petal.Length and Petal.Width variables in iris_tbl like this: summarise(iris_tbl, mean(Petal.Length), mean(Petal.Width)) ## # A tibble: 1 x 2 ## `mean(Petal.Length)` `mean(Petal.Width)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.758 1.199333 Notice what kind of object summarise returns: it’s a tibble with only one row and two columns. There are two columns because we calculated two means, and there is one row containing these means. Simple. There are a few other things to note about how summarise works: As with all dplyr functions, the expression that performs the required summary calculation is not surrounded by quotes because it is an expression that it “does some calculations”. The order of the expression in the resulting tibble is the same as the order in which they were used as arguments. Even though the dimensions of the output object have changed, summarise returns the same kind of data object as its input. It returns a data frame if our data was originally in a data frame, or a tibble if it was in a tibble. Notice that summarise used the expressions to name the variables. Variable names like mean(Petal.Length) and mean(Petal.Width) are not very helpful. They’re quite long for one. More problematically, they contain special reserved characters like (, which makes referring to columns in the resulting tibble more difficult than it needs to be: # make a summary tibble an assign it a name iris_means &lt;- summarise(iris_tbl, mean(Petal.Length), mean(Petal.Width)) # extract the mean petal length iris_means$`mean(Petal.Length)` ## [1] 3.758 We have to place ‘back ticks’ (as above) or ordinary quotes around the name to extract the new column when it includes special characters. It’s better to avoid using the default names. The summarise function can name the new variables at the same time as they are created. Predictably, we do this by naming the arguments using =, placing the name we require on the left hand side. For example: summarise(iris_tbl, Mean_PL = mean(Petal.Length), Mean_PW = mean(Petal.Width)) ## # A tibble: 1 x 2 ## Mean_PL Mean_PW ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.758 1.199333 There are very many base R functions that can be used with summarise. A few useful ones for calculating summaries of numeric variables are: min and max calculate the minimum and maximum values of a vector. mean and median calculate the mean and median of a numeric vector. sd and var calculate the standard deviation and variance of a numeric vector. We can combine more than one function in a summarise expression as long as it returns a single number. This means we can do arbitrarily complicated calculations in a single step. For example, if we need to know the ratio of the mean and median values of petal length and petal width in iris_tbl, we use: summarise(iris_tbl, Mn_Md_PL = mean(Petal.Length) / median(Petal.Length), Mn_Md_PW = mean(Petal.Width) / median(Petal.Width)) ## # A tibble: 1 x 2 ## Mn_Md_PL Mn_Md_PW ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.863908 0.9225641 Notice that we placed each argument on a separate line in this example. This is just a style issue—we don’t have to do this, but since R doesn’t care about white space, we can use new lines and spaces to keep everything a bit more more human-readable. It pays to organise summarise calculations like this as they become longer. It allows us to see the logic of the calculations more easily, and helps us spot potential errors when they occur. 15.1.1 Helper functions There are a small number dplyr helper functions that can be used with summarise. These generally provide summaries that aren’t available directly using base R functions. For example, the n_distinct function is used to calculate the number of distinct values in a variable: summarise(iris_tbl, Num.PL.Vals = n_distinct(Petal.Length)) ## # A tibble: 1 x 1 ## Num.PL.Vals ## &lt;int&gt; ## 1 43 This tells us that there are 43 unique values of Petal.Length. We won’t explore any others here. The handy cheat sheat is worth looking over to see what additional options are available. 15.2 Grouped operations using group_by Performing a calculation with one or more variables over the whole data set is useful, but very often we also need to carry out an operation on different subsets of our data. For example, it’s probably more useful to know how the mean sepal and petal traits vary among the different species in the iris_tbl data set, rather than knowing the overall mean of these traits. We could calculate separate means by using filter to create different subsets of iris_tbl, and then using summary on each of these to calculate the relevant means. This would get the job done, but it’s not very efficient and very soon becomes tiresome when we have to work with many groups. The group_by function provides a more elegant solution to this kind of problem. It doesn’t do all that much on its own though. All the group_by function does is add a bit of grouping information to a tibble or data frame. In effect, it defines subsets of data on the basis of one or more grouping variables. The magic happens when the grouped object is used with a dplyr verb like summarise or mutate. Once a data frame or tibble has been tagged with grouping information, operations that involve these (and other) verbs are carried out on separate subsets of the data, where the subsets correspond to the different values of the grouping variable(s). Basic usage of group_by looks like this: group_by(data_set, vname1, vname2, ...) The first argument, data_set (“data object”), must be the name of the object containing our data. We then have to include one or more additional arguments, where each of these is the name of a variable in data_set. I have expressed this as vname1, vname2, ..., where vname1 and vname2 are names of the first two variables, and the ... is acting as placeholder for the remaining variables. As usual, it’s much easier to understand how group_by works once we’ve seen it in action. We’ll illustrate group_by by using it alongside summarise with the storms_tbl data set. We’re aiming to calculate the mean wind speed for every type of storm. The first step is to use group_by to add grouping information to storms_tbl: group_by(storms_tbl, type) ## # A tibble: 2,747 x 11 ## # Groups: type [4] ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Allison 1995 6 3 0 17.4 -84.3 1005 30 ## 2 Allison 1995 6 3 6 18.3 -84.9 1004 30 ## 3 Allison 1995 6 3 12 19.3 -85.7 1003 35 ## 4 Allison 1995 6 3 18 20.6 -85.8 1001 40 ## 5 Allison 1995 6 4 0 22.0 -86.0 997 50 ## 6 Allison 1995 6 4 6 23.3 -86.3 995 60 ## 7 Allison 1995 6 4 12 24.7 -86.2 987 65 ## 8 Allison 1995 6 4 18 26.2 -86.2 988 65 ## 9 Allison 1995 6 5 0 27.6 -86.1 988 65 ## 10 Allison 1995 6 5 6 28.5 -85.6 990 60 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; Compare this to the output produced when we print the original storms_tbl data set: storms_tbl ## # A tibble: 2,747 x 11 ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Allison 1995 6 3 0 17.4 -84.3 1005 30 ## 2 Allison 1995 6 3 6 18.3 -84.9 1004 30 ## 3 Allison 1995 6 3 12 19.3 -85.7 1003 35 ## 4 Allison 1995 6 3 18 20.6 -85.8 1001 40 ## 5 Allison 1995 6 4 0 22.0 -86.0 997 50 ## 6 Allison 1995 6 4 6 23.3 -86.3 995 60 ## 7 Allison 1995 6 4 12 24.7 -86.2 987 65 ## 8 Allison 1995 6 4 18 26.2 -86.2 988 65 ## 9 Allison 1995 6 5 0 27.6 -86.1 988 65 ## 10 Allison 1995 6 5 6 28.5 -85.6 990 60 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; There is almost no change in the printed information—group_by really doesn’t do much on its own. The main change is that the tibble resulting from the group_by operation has a little bit of additional information printed at the top: Groups: type [4]. The Groups: type part of this tells us that the tibble is grouped by the type variable and nothing else. The [4] part tells us that there are 4 different groups. The only thing group_by did was add this grouping information to a copy of storms_tbl. The original storms_tbl object was not altered in any way. If we actually want to do anything useful useful with the result we need to assign it a name so that we can work with it: storms_grouped &lt;- group_by(storms_tbl, type) Now we have a grouped tibble called storms_grouped, where the groups are defined by the values of type. Any operations on this tibble will now be performed on a “by group” basis. To see this in action, we use summarise to calculate the mean wind speed: summarise(storms_grouped, mean.wind = mean(wind)) ## # A tibble: 4 x 2 ## type mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Extratropical 40.06068 ## 2 Hurricane 84.65960 ## 3 Tropical Depression 27.35867 ## 4 Tropical Storm 47.32181 When we used summarise on an ungrouped tibble the result was a tibble with one row: the overall global mean. Now the resulting tibble has four rows, one for each value of type: The type variable in the new tibble tells us what these values are; the mean.wind variable shows the mean wind speed for each value. 15.2.1 More than one grouping variable What if we need to calculate summaries using more than one grouping variable? The workflow is unchanged. Let’s assume we want to know the mean wind speed and atmospheric pressure associated with each storm type in each year. We first make a grouped copy of the data set with the appropriate grouping variables: # group the storms_tbl data by storm year + assign the result a name storms_grouped &lt;- group_by(storms_tbl, type, year) # storms_grouped ## # A tibble: 2,747 x 11 ## # Groups: type, year [24] ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Allison 1995 6 3 0 17.4 -84.3 1005 30 ## 2 Allison 1995 6 3 6 18.3 -84.9 1004 30 ## 3 Allison 1995 6 3 12 19.3 -85.7 1003 35 ## 4 Allison 1995 6 3 18 20.6 -85.8 1001 40 ## 5 Allison 1995 6 4 0 22.0 -86.0 997 50 ## 6 Allison 1995 6 4 6 23.3 -86.3 995 60 ## 7 Allison 1995 6 4 12 24.7 -86.2 987 65 ## 8 Allison 1995 6 4 18 26.2 -86.2 988 65 ## 9 Allison 1995 6 5 0 27.6 -86.1 988 65 ## 10 Allison 1995 6 5 6 28.5 -85.6 990 60 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; We grouped the storms_tbl data by type and year and assigned the grouped tibble the name storms_grouped. When we print this to the Console we see Groups: type, year [24] near the top, which tells us that the tibble is grouped by two variables with 24 unique combinations of values. We then calculate the mean wind speed and pressure of each storm type in each year: summarise(storms_grouped, mean_wind = mean(wind), mean_pressure = mean(pressure)) ## # A tibble: 24 x 4 ## # Groups: type [?] ## type year mean_wind mean_pressure ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Extratropical 1995 38.67521 995.2991 ## 2 Extratropical 1996 40.42105 991.2526 ## 3 Extratropical 1997 38.94737 999.7632 ## 4 Extratropical 1998 42.92208 990.6104 ## 5 Extratropical 1999 38.86364 992.0909 ## 6 Extratropical 2000 39.68254 996.7619 ## 7 Hurricane 1995 81.99187 969.6016 ## 8 Hurricane 1996 85.50336 969.1275 ## 9 Hurricane 1997 80.39474 976.3947 ## 10 Hurricane 1998 87.04142 972.4260 ## # ... with 14 more rows This calculates mean wind speed and atmospheric pressure for different combination of type and year. The first line shows us that the mean wind speed and pressure associated with extra-tropical storms in 1995 was 38.7 mph and 995 millibars, the second line shows us that the mean wind speed and pressure associated with extra-tropical storms in 1995 was 40.4 mph and 991 millibars, and so on. There are 24 rows in total because there were 24 unique combinations of type and year in the original storms_tbl. 15.2.2 Using group_by with other verbs The summarise function is the only dplyr verb we’ll use with grouped tibbles in this book. However, all the main verbs alter their behaviour to respect group identity when used with tibbles with grouping information. When mutate or transmute are used with a grouped object they still add new variables, but now the calculations occur “by group”. Here’s an example using transmute: # group the storms data by storm name + assign the result a name storms_grouped &lt;- group_by(storms_tbl, name) # create a data set &#39;mean centred&#39; wind speed variable transmute(storms_grouped, wind_centred = wind - mean(wind)) ## Adding missing grouping variables: `name` ## # A tibble: 2,747 x 2 ## # Groups: name [79] ## name wind_centred ## &lt;chr&gt; &lt;dbl&gt; ## 1 Allison -14.393939 ## 2 Allison -14.393939 ## 3 Allison -9.393939 ## 4 Allison -4.393939 ## 5 Allison 5.606061 ## 6 Allison 15.606061 ## 7 Allison 20.606061 ## 8 Allison 20.606061 ## 9 Allison 20.606061 ## 10 Allison 15.606061 ## # ... with 2,737 more rows In this example we calculated the “group mean-centered” version of the wind variable. The new wind_centred variable contains the difference between the wind speed and the mean of whichever storm type is associated with the observation. 15.3 Removing grouping information On occasion it’s necessary to remove grouping information from a data object. This is most often done when working with “pipes” (the topic of the next chapter) when we need to revert back to operating on the whole data set. The ungroup function removes grouping information: ungroup(storms_grouped) ## # A tibble: 2,747 x 11 ## name year month day hour lat long pressure wind ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Allison 1995 6 3 0 17.4 -84.3 1005 30 ## 2 Allison 1995 6 3 6 18.3 -84.9 1004 30 ## 3 Allison 1995 6 3 12 19.3 -85.7 1003 35 ## 4 Allison 1995 6 3 18 20.6 -85.8 1001 40 ## 5 Allison 1995 6 4 0 22.0 -86.0 997 50 ## 6 Allison 1995 6 4 6 23.3 -86.3 995 60 ## 7 Allison 1995 6 4 12 24.7 -86.2 987 65 ## 8 Allison 1995 6 4 18 26.2 -86.2 988 65 ## 9 Allison 1995 6 5 0 27.6 -86.1 988 65 ## 10 Allison 1995 6 5 6 28.5 -85.6 990 60 ## # ... with 2,737 more rows, and 2 more variables: type &lt;chr&gt;, ## # seasday &lt;int&gt; Looking at the top right of the printed summary, we can see that the Group: part is now gone—the ungroup function effectively created a copy of storms_grouped that is identical to the original storms_tbl tibble. "],
["building-pipelines.html", "Chapter 16 Building pipelines 16.1 Why do we need ‘pipes’?", " Chapter 16 Building pipelines This chapter will introduce something called the pipe operator: %&gt;%. We don’t often use the various dplyr verbs in isolation. Instead, starting with our raw data, they are combined in a sequence to prepare the data for further analysis (e.g. making a plot, calculating summaries, fitting a statistical model, and so on). The function of the pipe operator is to make the data wrangling part of such a workflow as transparent as possible. 16.1 Why do we need ‘pipes’? We’ve seen that carrying out calculations on a per-group basis can be achieved by grouping a tibble, assigning this a name, and then applying the summarise function to the new tibble. For example, if we need the mean wind speed for every storm recorded in storms_tbl, we could use: # 1. make a grouped copy of the storms data storms_grouped &lt;- group_by(storms_tbl, name) # 2. calculate the mean wind speed for each storm summarise(storms_grouped, mean.wind = mean(wind)) ## # A tibble: 79 x 2 ## name mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Alberto 63.04598 ## 2 Alex 35.38462 ## 3 Allison 44.39394 ## 4 Ana 32.10526 ## 5 Arlene 39.03846 ## 6 Arthur 35.22727 ## 7 Barry 39.76190 ## 8 Bertha 60.00000 ## 9 Beryl 36.11111 ## 10 Bill 50.55556 ## # ... with 69 more rows There’s nothing wrong with this way of doing things. However, this approach to building up an analysis is quite verbose—especially if an analysis involves more than a couple of steps—because we have to keep storing intermediate steps. It also tends to clutter the global environment with lots of data objects we don’t need. One way to make things more concise is to use a nested function call (we examined these in the Using functions chapter), like this: summarise(group_by(storms_tbl, type), mean.wind = mean(wind)) ## # A tibble: 4 x 2 ## type mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Extratropical 40.06068 ## 2 Hurricane 84.65960 ## 3 Tropical Depression 27.35867 ## 4 Tropical Storm 47.32181 Here we placed the group_by function call inside the list of arguments to summarise. Remember, you have to read nested function calls from the inside out to understand what they are doing. This is exactly equivalent to the previous example. We get the same result without having to store intermediate data. However, there are a couple of good reasons why this approach is not advised: Experienced R users probably don’t mind this approach because they’re used to nested functions calls. Nonetheless, no reasonable person would argue that nesting functions inside one another is intuitive. Reading outward from the inside of a large number of nested functions is hard work. Even for experienced R users, using function nesting is a fairly error prone approach. For example, it’s very easy to accidentally put an argument or two on the wrong side of a closing ). If we’re lucky this will produce an error and we’ll catch the problem. If we’re not, we may just end up with nonsense in the output. There’s a third option for combing several functions that has the dual benefit of keeping our code concise and readable, while avoiding the need to clutter the global environment with intermediate objects. This third approach involves something called the “pipe” operator: %&gt;% (no spaces allowed). This isn’t part of base R though. Instead, it’s part of a package called magrittr. but there’s no need to install this if we’re using dplyr because dplyr imports it for us. The %&gt;% operator has become very popular in recent years. The main reason for this is because it allows us to specify a chain of function calls in a (reasonably) human readable format. Here’s how we write the previous example using the pipe operator %&gt;%: storms_tbl %&gt;% group_by(., type) %&gt;% summarise(., mean.wind = mean(wind)) ## # A tibble: 4 x 2 ## type mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Extratropical 40.06068 ## 2 Hurricane 84.65960 ## 3 Tropical Depression 27.35867 ## 4 Tropical Storm 47.32181 How do we make sense of this? Every time we see the %&gt;% operator it means the following: take whatever is produced by the left hand expression and use it as an argument in the function on the right hand side. The . serves as a placeholder for the location of the corresponding argument. This means we can understand what a sequence of calculations is doing by reading from left to right, just as we would read the words in a book. This example says, take the storms_tbl data, group it by type, then take the resulting grouped tibble and apply the summarise function to it to calculate the mean of wind. It is exactly the same calculation we did above. When using the pipe operator we can often leave out the . placeholder. Remember, this signifies the argument of the function on the right of %&gt;% that is associated with the result from on left of %&gt;%. If we choose to leave out the ., the pipe operator assumes we meant to slot it into the first argument. This means we can simplify our example even more: storms_tbl %&gt;% group_by(type) %&gt;% summarise(mean.wind = mean(wind)) ## # A tibble: 4 x 2 ## type mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Extratropical 40.06068 ## 2 Hurricane 84.65960 ## 3 Tropical Depression 27.35867 ## 4 Tropical Storm 47.32181 This is why the first argument of a dplyr verb is always the data object. This convention ensures that we can use %&gt;% without explicitly specifying the argument to match against. Remember, R does not care about white space, which means we can break a chained set of function calls over several lines if it becomes too long: storms_tbl %&gt;% group_by(type) %&gt;% summarise(mean.wind = mean(wind)) ## # A tibble: 4 x 2 ## type mean.wind ## &lt;chr&gt; &lt;dbl&gt; ## 1 Extratropical 40.06068 ## 2 Hurricane 84.65960 ## 3 Tropical Depression 27.35867 ## 4 Tropical Storm 47.32181 In fact, many dplyr users always place each part of a pipeline onto a new line to help with overall readability. Finally, when we need to assign the result of a chained functions we have to break the left to right rule a bit, placing the assignment at the beginning: new_data &lt;- storms_tbl %&gt;% group_by(type) %&gt;% summarise(mean.wind = mean(wind)) (Actually, there is a rightward assignment operator, -&gt;, but let’s not worry about that) Why is %&gt;% called the ‘pipe’ operator? The %&gt;% operator takes the output from one function and “pipes it” to another as the input. It’s called ‘the pipe’ for the simple reason that it allows us to create an analysis ‘pipeline’ from a series of function calls. Incidentally, if you Google the phrase “magritt pipe” you’ll see that magrittr is a very clever name for an R package. One final piece of advice: learn how to use the %&gt;% method of chaining together functions. Why? Because it’s the simplest and cleanest method for doing this, many of the examples in the dplyr help files and on the web use it, and the majority of people carrying out real world data wrangling with dplyr rely on piping. "],
["exploratory-data-analysis.html", "Chapter 17 Exploratory data analysis 17.1 Introduction 17.2 Statistical variables and data 17.3 Populations, samples and distributions 17.4 Relationships", " Chapter 17 Exploratory data analysis 17.1 Introduction Exploratory data analysis (EDA) was promoted by the statistician John Tukey in his 1977 book, “Exploratory Data Analysis”. The broad aim of EDA is to help us formulate and refine hypotheses that will lead to informative analyses or further data collection. The core objectives of EDA are: to suggest hypotheses about the causes of observed phenomena, to guide the selection of appropriate statistical tools and techniques, to assess the assumptions on which statistical analysis will be based, to provide a foundation for further data collection. EDA involves a mix of both numerical and visual methods of analysis. Statistical methods are sometimes used to supplement EDA, but its main purpose is to facilitate understanding before diving into formal statistical modelling. Even if we think we already know what kind of analysis we need to pursue, it’s always a good idea to explore a data set before diving into the analysis. At the very least, this will help us to determine whether or not our plans are sensible. Very often it uncovers new patterns and insights. In this chapter we’re going to examine some basic concepts that underpin EDA: 1) classifying different types of data, and 2) distinguishing between populations and samples. This will set us up to learn how to explore our data in later chapters. 17.2 Statistical variables and data In the Data frames chapter we pointed out that the word “variable” is typically used to mean one of two things. In the context of programming, a variable is a name-value association that we create when we run some code. Statisticians use the word in a quite different way. To them, a variable is any characteristic or quantity that can be measured, classified or experimentally controlled. Much of statistics is about quantifying and explaining the variation in such quantities as best we can. Biomass, species richness, population density, infection status, body size, and individual fitness are all examples of statistical variables we encounter in the organismal sciences. We can think of these as statistical variables because their values vary between different observations. For example, ‘annual fitness’—measured as the number of offspring produced—is a variable that differs both among the organisms in a population and over the life cycle of a given individual. There are different ways to describe statistical variables according to the manner in which they can be analysed, measured, or presented. While not the most exciting topic, it’s important to be clear about what kind of variables we’re dealing with because this determines how we should visualise the data, and later, how we might analyse it statistically. Statisticians have thought long and hard about how we should classify variables. The distinctions can be very subtle, but we’ll only consider two fairly simple classification schemes. 17.2.1 Numeric vs. categorical variables Numeric variables have values that describe a measurable quantity as a number, like ‘how many’ or ‘how much’. Numeric variables are also called quantitative variables; the data collected containing numeric variables are called quantitative data. Numeric variables may be further described as either continuous or discrete: Continuous numeric variable: Observations can take any value between a certain set of real numbers, i.e. numbers represented with decimals. This set is typically either “every possible number” (e.g. the change in population density can be positive or negative, and very large or very small) or “all the positive numbers” (e.g. biomass may be very large or very small, but it is strictly positive). Examples of continuous variables include body mass, age, time, and temperature. Though in theory continuous variables may admit any number in the set of possible numbers, in practice the values given to an observation may be bounded and can only include values as small as the measurement protocol allows. Elephants are very large, but they never get as big as a passenger jet, and trying to measure the mass of an elephant at a precision of a few grams is probably not practical. Discrete numeric variable: Observations can take a value based on a count from a set of whole values; e.g. 1, 2, 3, 4, 5, and so on. A discrete variable cannot take the value of a fraction between one value and the next closest value. Examples of discrete variables include the number of individuals in a population, number of offspring produced (‘reproductive fitness’), and number of infected individuals in an experiment. All of these are measured as whole units. Discrete variables are very common in the biological and environmental sciences. We love to count things… Categorical variables have values that describe a characteristic of a data unit, like ‘what type’ or ‘which category’. Categorical variables fall into mutually exclusive (in one category or in another) and exhaustive (include all possible options) categories. Therefore, categorical variables are qualitative variables and tend to be represented by a non-numeric value. The data collected for a categorical variable are qualitative data. Categorical variables may be further described as ordinal or nominal: Ordinal variable: Observations can take a value that can be logically ordered or ranked. The categories associated with ordinal variables can be ranked higher or lower than another, but do not necessarily establish a numeric difference between each category. Examples of ordinal categorical variables include academic grades (i.e. A, B, C), size class of a plant (i.e. small, medium, large) and behaviour. Nominal variable: Observations can take a value that is not able to be organised in a logical sequence. Examples of nominal categorical variables include sex, business type, eye colour, religion and brand. Don’t use numbers to classify categorical variables Be careful when classifying variables. It is dangerous to assume that just because a numerical scheme has been used to describe it, a variable it must not be categorical – there is nothing to stop someone using numbers to describe a categorical variable (e.g. Male = 1, Female = 2, Hermaphrodite = 3). We can use numbers to describe categories, but it isn’t very sensible. It’s much clearer to use a non-numeric recording scheme (e.g. Male = “M”, Female = “F”, Hermaphrodite = “H”) to record categorical variables. 17.2.2 Ratio vs. interval scales A second way of classifying numeric variables (not categorical variables) relates to the scale they are measured on. The measurement scale is important because it determines how things like differences, ratios, and variability are interpreted. Interval scale: This allows for the degree of difference between data items, but not the ratio between them. This kind of scale does not have unique and non-arbitrary zero value. However, we can compare the ratio of differences on an interval scale though. A good example of an interval scale is date, which we measure relative to an arbitrary epoch (e.g. AD). We cannot really say that 2000 AD is twice as long as 1000 AD. We can talk about the amount of time that has passed between two dates though, i.e. it does make sense to say that twice as much time has passed since the epoch in 2000 AD versus 1000 AD. Ratio scale: This scale does possess a meaningful zero value. It takes its name from the fact that a measurement on this scale represents a ratio between a measure of the magnitude of a quantity and a unit of the same kind. What this means in simple terms is that it is meaningful to say that something is “twice as …” as something else when working with a variable measured on a ratio scales. Ratio scales most often appear when we work with physical quantities. For example, we can say that one tree is twice as big as another, or that one elephant has twice the mass of another, because length and mass are measured on ratio scales. Keep in mind that the distinction between ratio and interval scales is a property of the scale of measurement, not the thing being measured. For example, when we measure temperature in º C we’re working on an interval scale defined relative to the freezing and boiling temperatures of water under standard conditions. It doesn’t make any sense to say that 30º C is twice as hot as 15º C. However, if we measured the same two temperatures on the Kelvin scale, it is meaningful to say that 303.2K is 1.05 times hotter than 288.2K. This is because the Kelvin scale is relative to a true zero: absolute zero. 17.3 Populations, samples and distributions When we collect data of any kind, we are working with a sample of objects (e.g. trees, insects, fields) from a wider population. We usually want to know something about the wider population, but since it’s impossible to study every member of the population, we study the properties of one or more samples instead. The problem with samples is that they are ‘noisy’. If we were repeat the same data collection protocol more than once we should expect to end up with a different sample each time, even if the wider population never changes. This results purely from chance variation in the sampling of different units. Picking apart the relationship between samples and populations is the basis of much of statistics. This topic is best dealt with in a dedicated statistics book, so we won’t develop these ideas in much detail here. The reason we mention the distinction between a population and a sample is because EDA is primarily concerned with properties of samples—it aims to characterise the sample in hand without trying to say too much about the wider population from which it is derived. When we talk about “exploring a variable” what we are really doing is exploring is the sample distribution of that variable. What is this? The sample distribution is a statement about the frequency with which different values occur in a particular sample. Imagine we took a sample of undergraduates and measured their height. The majority of students would be round about 1.7m tall, even though there would obviously be some variation among students. Men would tend to be slightly taller than women, and very small or very tall people would be rare. We know from experience that no one in this sample would be over 3 meters tall. These are all statements about a (hypothetical) sample distribution of undergraduate heights. Our goal when exploring the sample distribution of a variable is to answer questions such as, What are the most common values of the variable; and How much do observations differ from one another? Rather than simply describing these properties in verbal terms, as we did above, we want to describe in a more informative way. There are two ways to go about this: Calculate descriptive statistics. Descriptive statistics are used to quantify the basic features of a sample distribution. They provide simple summaries about the sample that can be used to make comparisons and draw preliminary conclusions. For example, we often use ‘the mean’ to summarise the ‘most likely’ values of a variable in a sample. Construct graphical summaries. Descriptive statistics are not much use on their own, for the simple reason that a few numbers can’t capture every aspect of a sample distribution. Graphical summaries are an ideal complement to descriptive statistics because they allow us to present lots of information about a sample distribution in one place and in a manner that is easy for people to understand. 17.4 Relationships So far we’ve been thinking about samples of one statistical variable. However, a sample may involve more than one variable. Moreover, data analysis is usually concerned with the relationships among two or more variables. These relationships might involve the same (e.g. numeric vs. numeric) or different types of variable (e.g. numeric vs. categorical). In either case, EDA is used to understand how the values of one variable depend on those of the other. Just as with single variable analyses, we use both descriptive statistics and graphical summaries to explore such relationships. "],
["introduction-to-ggplot2.html", "Chapter 18 Introduction to ggplot2 18.1 The anatomy of ggplot2 18.2 A quick introduction to ggplot2 18.3 Increasing the information density", " Chapter 18 Introduction to ggplot2 One of the main reasons data analysts turn to R is for its strong data visualisation capabilities. The R ecosystem includes many different packages that support data visualisation. The three that are used widely used are: 1) the base graphics system, which uses the graphics package; 2) the lattice package; and 3) the ggplot2 package. Each of these has its own strengths and weaknesses: Base graphics is part of base R—it’s available immediately after we start R. It’s very flexible and allows us to construct more or less any plot we like. This flexibility comes at a cost though. It’s quite easy to get up and running with the base R graphics—there are functions like plot and hist for making commonly-used figures—but building complex figures is time consuming. We have to write a lot of R code to prepare even moderately complex plots, there are a huge number of graphical parameters to learn, and many of the standard plotting functions are inconsistent in the way they work. The lattice package was developed by Deepayan Sarkar to implement ideas of Bill Cleveland in his 1993 book, Visualizing Data. The package implements something called Trellis graphics, a very useful approach for graphical exploratory data analysis. Trellis graphics are designed to help us visualise complicated, multiple variable relationships. The lattice package has many “high level” functions (e.g. xyplot) to make this process easy, but still retains much of the fine-grained control that characterises the standard graphics system. The lattice package is very powerful but can be hard to learn. The ggplot2 package was developed by Hadley Wickham to implement some of the ideas in a book called The Grammar of Graphics by Wilkinson (2005). It produces Trellis-like graphics, but is quite different from lattice in the way it goes about this. It uses its own mini-language to define graphical objects, adopting the language of Wilkinson’s book to define these. It takes a little while to learn the basics, but once these have been mastered these it’s very easy to produce sophisticated plots with very little R code. The downside of working with ggplot2 is that it isn’t as flexible as base graphics. We aren’t going to survey all three of these plotting systems. There isn’t space, and in any case, it’s possible to meet most data visualisation needs by becoming proficient with just one of them. This book focuses on ggplot2. In many ways the ggplot2 package hits the ‘sweet spot’ between standard graphics and lattice. It allows us to produce complex visualisations without the need to write lines and lines of R code, but is still flexible enough to allow us to tweak the appearance of a figure so that it meets our specific needs. 18.1 The anatomy of ggplot2 The easiest way to learn ggplot2 is by seeing it in action. Before we dive in it’s worth surveying the essential features of the ggplot2 ‘grammar’. Most of this is fairly abstract and won’t make sense on first reading. This is fine. Abstract ideas like ‘aesthetics’ and ‘geoms’ will start to make sense as we work through a range of different examples in the next few chapters. ggplot2 is designed to reflect Wilkinson’s grammar of graphics. The ggplot2 version of this grammar revolves around the idea of layers. The underlying idea is that we construct a visualisation in a structured manner by defining one or more of these layers, which together with a few other components define a complete ggplot2 object. Each layer may have its own data or it may share its data with another layer, and each layer displays its data in a specific way. The resulting ggplot2 object is defined by a combination of: a default data set along with a set of mappings from variables to aesthetics, one or more layers, each comprising a number of components, one scale for each aesthetic mapping, a coordinate system for the plot, a faceting specification that tells ggplot2 how to define a multi-panel plot. We’ll skim over each of these in turn before moving onto the business of actually using ggplot2. 18.1.1 Layers Each layer in a ggplot2 plot may have five different components, though we don’t necessarily have to specify all of these: The data. At a minimum, every plot needs some data. Unlike base R graphics, ggplot2 always accepts data in one format, an R data frame (or tibble). Each layer can be associated with it’s own data set. However, we don’t have explicitly add data to each layer. When we choose not to specify the data set for a layer ggplot2 will try use the default data if it has been defined. A set of aesthetic mappings. These describe how variables in the data are associated with the aesthetic properties of the layer. Commonly encountered aesthetic properties include position (x and y locations), colour, and size of the objects on a plot. Each layer can be associated with it’s own unique aesthetic mappings. When we choose not to specify these for a layer ggplot2 will use the defaults if they were defined. A geometric object, called a ‘geom’. The geom tells ggplot2 how to actually represent the layer—they refer to the objects we can actually see on a plot, such as points, lines or bars. Each geom only works with a particular of aesthetic mappings. We always have to define at least one geom when using ggplot2. A stat. These take the raw data in the data frame and transform in some useful way. A stat allows us to produce summaries of our raw data. We won’t use them in this book because we can produce the same kinds of figures by first processing our data with dplyr. Nonetheless, the stat facility is one of the things that makes ggplot2 particularly useful for exploratory data analysis. A position adjustment. These apply small tweaks to the position of layer elements. These are most often used in plots like bar plots where we need to define how the bars are plotted, but they can occasionally be useful in other kinds of plots. 18.1.2 Scales The scale part of a ggplot2 object controls how the data is mapped to the aesthetic attributes. A scale takes the data and converts it into something we can perceive, such as an x/y location, or the colour and size of points in a plot. A scale must be defined for every aesthetic in a plot. It doesn’t make sense to define an aesthetic mapping without a scale because there is no way for ggplot2 to know how to go from the data to the aesthetics without one. Scales operate in the same way on the data in a plot. If we include several layers they all have to use the same scale for the shared aesthetic mappings. This behaviour is sensible because it ensures that the information that is displayed is consistent. If we choose not to explicitly define a scale for an aesthetic ggplot2 will use a default. Very often this will be a ‘sensible’ choice, which means we can get quite a long way with ggplot2 without ever really understanding scales. We won’t worry too much about them, though we will take a brief look at a few of the more common options. 18.1.3 Coordinate system A ggplot2 coordinate system takes the position of objects (e.g. points and lines) and maps them onto the 2d plane that a plot lives on. Most people are already very familiar with the most common coordinate system (even if they didn’t realise it). The Cartesian coordinate system is the one we’ve all been using ever since we first constructed a graph with paper and pencil at school. All the most common statistical plots use this coordinate system so we won’t consider any others in this book. 18.1.4 Faceting The idea behind faceting is very simple. Faceting allows us to break a data set up into subsets according to the unique values of one or two variables, and then produce a separate plot for each subset. The result is a multipanel plot, where each panel shares the same layers, scales, etc. The data is the only thing that varies from panel to panel. The result is a kind of ‘Trellis plot’, similar to those produced by the lattice package. Faceting is a very powerful tool that allows us to slice up our data in different ways and really understand the relationship between different variables. Together with aesthetic mappings, faceting allows us to summarise relationships among 4-6 variables in a single plot. 18.2 A quick introduction to ggplot2 Now that we’ve briefly reviewed the ggplot2 grammar we can start learning how to use it. The package uses this grammar to define a sort of mini-language within R, using functions to specify components like aesthetics and geoms, which are combined with data to define a ggplot2 graphics object. Once we’ve constructed a suitable object we can use it to display our graphic on the computer screen or save in a particular graphics format (e.g. PDF, PNG, JPEG, and so on). Rather than orientating this introduction around each of the key functions we’re going to develop a simple example to help us see how ggplot2 works. Many of the key ideas about how ggplot2 works can be taken away from this one example, so it’s definitely worth investing the time to understand it, i.e. use the example understand how the different ggplot2 functions are related to the grammar outlined above. Our goal is to produce a scatter plot. The scatter plot is one of the most commonly used visualisation tools in the EDA toolbox. It’s designed to show how one numeric variable is related to another. A scatter plot uses horizontal and vertical axes (the ‘x’ and ‘y’ axes) to visualise pairs of related observations as a series of points in two dimensions. We’ll use the storms data from the nasaweather package to construct the scatter plot. The questions we want to explore are: 1) what is the relationship between wind speed (wind) and atmospheric pressure (pressure); 2) and how does this vary among (year) and within (seasday) years? That is, we want to investigate how wind speed depends on atmospheric pressure, and how this relationship varies over time. 18.2.1 Making a start To begin working with a graphical object we have to first set up a basic skeleton to build on. This is the job of the ggplot function. We can build an empty object by using ggplot without any arguments: plt &lt;- ggplot() summary(plt) ## data: [x] ## faceting: &lt;ggproto object: Class FacetNull, Facet&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map: function ## map_data: function ## params: list ## render_back: function ## render_front: function ## render_panels: function ## setup_data: function ## setup_params: function ## shrink: TRUE ## train: function ## train_positions: function ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet&gt; Here’s what just happened: we constructed the skeleton object, assigned it to a variable called plt, and then used the summary function to inspect the result. When first learning about ggplot2, it’s a good idea to use summary on various objects to get an idea of their structure. It’s quite ‘verbose’, but the important parts of the output are near the top, before the faceting: part. In this case, the ‘important part’ is basically empty. This tells us that there are no data, aesthetic mapping, layers, etc associated with plt. All we did was set up an empty object. ggplot2 vs. ggplot Notice that the while package is called ggplot2, the actual function that does the work of setting up the skeleton graphical object is called ggplot. Try not to mix them up—this is a common source of errors. How can we improve on this? We should add a default data set, which we do by passing the name of a data frame or dplyr tibble to ggplot. Let’s try this with storms: plt &lt;- ggplot(storms) summary(plt) ## data: name, year, month, day, hour, lat, long, pressure, wind, ## type, seasday [2747x11] ## faceting: &lt;ggproto object: Class FacetNull, Facet&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map: function ## map_data: function ## params: list ## render_back: function ## render_front: function ## render_panels: function ## setup_data: function ## setup_params: function ## shrink: TRUE ## train: function ## train_positions: function ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet&gt; Notice that when we summarise the resulting object this time we see that the variables inside storms (name, year, month, etc) now comprise the data inside our plt object. The next step is to add a default aesthetic mapping to our graphical object. Remember, these describe how variables in the data are mapped to the aesthetic properties of the layer(s). One way to think about aesthetic mappings is that they define what kind of relationships our plot will describe. Since we’re making a scatter plot we need to define mappings for positions on the ‘x’ and ‘y’ axes. We want to investigate how wind speed depends on atmospheric pressure, so we need to associate wind with the y axis and pressure with the x axis. We define an aesthetic mapping with the aes function (‘aesthetic mapping’). One way to do this is like this: plt &lt;- plt + aes(x = pressure, y = wind) This little snippet of R code may look quite strange at first glance. There are a couple things to take away from this: We ‘add’ the aesthetic mapping to the plt object using the + operator. This has nothing to do with arithmetic. The ggplot2 package uses some clever programming tricks to redefine the way + works with its objects so that it can be used to combine them. This is nice because it makes building up a plot from the components of the grammar very natural. The second thing to notice is that an aesthetic mapping is defined by one or more name-value pairs, specified as arguments of aes. The names on the left hand side of each = refer to the properties of our graphical object (e.g. the ‘x’ and ‘y’ positions). The values on right hand side refer to variable names in the data set that we want to associate with these properties. Notice that we overwrote the original plt object with the updated version using the assignment operator. We could have created a distinct object, but there’s usually no advantage to do this. Once again, we should inspect the result using summary: summary(plt) ## data: name, year, month, day, hour, lat, long, pressure, wind, ## type, seasday [2747x11] ## mapping: x = pressure, y = wind ## faceting: &lt;ggproto object: Class FacetNull, Facet&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map: function ## map_data: function ## params: list ## render_back: function ## render_front: function ## render_panels: function ## setup_data: function ## setup_params: function ## shrink: TRUE ## train: function ## train_positions: function ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet&gt; As hoped, the data (data:) from the original plt are still there, but now we can also see that two default mappings (mapping:) have been defined for the x and y axis positions. We have successfully used the ggplot and aes functions to set up a graphical object with both default data and aesthetic mappings. Any layers that we now add will use these unless we choose to override them by specifying different options. In order to produce a plotable version of plt we now need to specify a layer. This will tell ggplot2 how to visualise the data. Remember, each layer has five different components: data, aesthetic mappings, a geom, a stat and a position adjustment. Since we’ve already set up the default data and aesthetic mappings, there’s no need to define these again—ggplot2 will use the defaults if we leave them out of the definition. This leaves the geom, stat and position adjustment. What kind of geom do we need? A scatter plots allow us to explore a relationship as a series of points. We need to add a layer that uses the point geom. What about the stat and position? These are difficult to explain (and understand) without drilling down into the details of how ggplot2 works. The important insight is that both the stat and the position adjustment components change our data in some way before plotting it. If we want to avoid having ggplot2 do anything to our data, the key word is ‘identity’. We use this as the value when we want ggplot2 to plot our data ‘as is’. We’re going examine the easy way to add a layer in a moment. However we’ll start with a long-winded approach first, because this reveals exactly what happens whenever we build a ggplot2 object. The general function for adding a layer is simply called layer. Here’s how it works in its most basic usage: plt &lt;- plt + layer(geom = &quot;point&quot;, stat = &quot;identity&quot;, position = &quot;identity&quot;) All we did here was take the plt object, add a layer to it with the layer function, and then overwrite the old version of plt. Again, we add the new component using the + symbol. We passed three arguments to the layer function to… define the geom: the name of this argument was geom and the value assigned to it was &quot;point&quot;. define the stat: the name of this argument was stat and the value assigned to it was &quot;identity&quot;. define the position adjustment : the name of this argument was position and the value assigned to it was &quot;identity&quot;. Let’s review the structure of the resulting graphical object one last time to see what we’ve achieved: summary(plt) ## data: name, year, month, day, hour, lat, long, pressure, wind, ## type, seasday [2747x11] ## mapping: x = pressure, y = wind ## faceting: &lt;ggproto object: Class FacetNull, Facet&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map: function ## map_data: function ## params: list ## render_back: function ## render_front: function ## render_panels: function ## setup_data: function ## setup_params: function ## shrink: TRUE ## train: function ## train_positions: function ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet&gt; ## ----------------------------------- ## geom_point: na.rm = FALSE ## stat_identity: na.rm = FALSE ## position_identity The text above the ----- line is the same as before. It summarises the default data and the aesthetic mapping. The text below this summarises the layer we just added. It tells us that this layer will use the points geom (geom_point), the identity stat (stat_identity), and the identity position adjustment (position_identity). Now plt has everything it needs to actually render a figure. How do we do this? We just ‘print’ the object: print(plt) That’s it (finally)! We have used produced a scatter plot showing how wind speed depends of atmospheric pressure. This clearly shows that higher wind speeds are associated with lower pressure systems. That wasn’t really why we made this plot though—we wanted to see how the ggplot2 functions are related to its grammar. Here’s a quick summary of what we did: # step 1. set up the skeleton object with a default data set plt &lt;- ggplot(storms) # step 2. add the default aesthetic mappings plt &lt;- plt + aes(x = pressure, y = wind) # step 3. specify the layer we want to use plt &lt;- plt + layer(geom = &quot;point&quot;, stat = &quot;identity&quot;, position = &quot;identity&quot;) # step 4. render the plot print(plt) Don’t use this workflow! It’s possible to construct any ggplot2 visualisation using the workflow outlined in this subsection. It isn’t recommended though. The workflow adopted here was selected to reveal how the grammar works, rather than for its efficiency. A more concise, standard approach to using ggplot2 is outlined next. Use this for real world analysis. 18.2.2 The standard way of using ggplot2 The ggplot2 package is quite flexible, which means we can arrive at a particular visualisation in a number of different ways. To keep life simple, we’re going to adopt one consistent work flow for the remainder of this book. This won’t reveal the full array of ggplot2 tricks, but it is sufficient to enable us to construct a wide range of standard visualisations. To see it in action, we’ll make exactly the same wind speed vs. atmospheric pressure scatter plot again, only this time, we’ll use a few short cuts. We began building our ggplot2 object by setting up a skeleton object with a default data set and then added the default aesthetic mappings. There is a more concise way to achieve the same result: plt &lt;- ggplot(storms, aes(x = pressure, y = wind)) summary(plt) ## data: name, year, month, day, hour, lat, long, pressure, wind, ## type, seasday [2747x11] ## mapping: x = pressure, y = wind ## faceting: &lt;ggproto object: Class FacetNull, Facet&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map: function ## map_data: function ## params: list ## render_back: function ## render_front: function ## render_panels: function ## setup_data: function ## setup_params: function ## shrink: TRUE ## train: function ## train_positions: function ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet&gt; In this form the aes function is used inside ggplot, i.e. it supplies a second argument to ggplot. This approach is the most commonly used approach for setting up a graphical object with default data and aesthetic mappings. We will use it from now on. The next step is to add a layer. We just saw that the layer function can be used to construct a layer from its component parts. However, ggplot provides a number of functions that add layers according to the type of geom they use. They look like this: geom_NAME, where NAME stands for the name of the different possible geoms. An alternative to the last line is therefore: plt &lt;- plt + geom_point() We didn’t have to specify the stat or the position adjustment components of the layer because the geom_NAME functions all choose sensible defaults, though these can be overridden if needed, but 90% of the time there’s no need to do this. This way of defining a layer is much simpler and less error prone than the layer method. We will use the geom_NAME method from now on. There’s one last trick we need to learn to use ggplot2 efficiently. We’ve been building a plot object in several steps, giving the intermediates the name plt, and then manually printing the object to display it when it’s ready. This is useful if we want to make different versions of the same plot. However, we very often just want to build the plot and display it in one go. This is done by combining everything with + and printing the resulting object directly: ggplot(storms, aes(x = pressure, y = wind)) + geom_point() That’s it! As we have seen, there’s a lot going on underneath this, but this small snippet of R code contains everything ggplot2 needs to construct and display the simple scatter plot of wind speed against atmospheric pressure. 18.3 Increasing the information density We introduced the example by saying that we were interested in the relationship between wind speed, atmospheric pressure, observation year, and the time of year. So far we’ve only examined the first two. We’ll finish this chapter by exploring the two main approaches for increasing the information in visualisation to investigate the relationship with the remaining two variables. 18.3.1 Using additional aesthetics How can we learn about relationship of these two variables to time of year (seasday)? We need to include information in the seasday variable in our scatter plot somehow. There are different ways we might do this, but the basic trick is to map the seasday variable to a new aesthetic. We need to change the way we are using aes. One option is to map the seasday to the point colours so that the colour of the points correspond to the time of year: ggplot(storms, aes(x = pressure, y = wind, colour = seasday)) + geom_point() Notice that ggplot2 automatically adds a legend to the figure to help us interpret it. A colour scale is not much use without a legend. Points are now coloured according to whether they are associated with early (dark blue) or late (light blue) observations. There’s a hint that lower intensity storms tend to be at the beginning an end of the storm season, but it’s hard to be sure because there is so much overplotting—i.e. many points are in the same place. We could no doubt improves on this visualisation, but nonetheless, it illustrates the important concept: we can add information to a plot by mapping additional variables to new aesthetics. There is nothing to stop us using different aesthetics if we wanted to squeeze even more information into this plot. For example, we could map the storm type variable (type) to the point shape if we wanted, using shape = type inside aes. However, this graph is already a bit too crowded, so this might not be too helpful in this instance. 18.3.2 Using facets What if we want to see how the wind speed and pressure relationship might vary among years? One way to do this is to make a separate scatter plot for each year. We don’t have to do this manually though. We can use the faceting facility of ggplot2 instead. This allows us to break up our data set up into subsets according to the unique values of one or two variables and produce a separate plot for each subset, but without having to write much R code. Faceting operates on the whole figure so we can’t apply it by changing the properties of a layer. Instead, we have to use a new function to add the faceting information. Here’s how we split things up by year using the facet_wrap function: ggplot(storms, aes(x = pressure, y = wind, colour = seasday)) + geom_point() + facet_wrap(~ year, nrow = 2, ncol = 3) The first argument of facet_wrap (~ year) says to split up the data set according to the values of year. The nrow and ncol arguments just specify how to split the panels across rows and columns of the resulting plot. Notice that the panels share the same scales for the ‘x’ and ‘y’ axes. This makes it easy to compare information. The plot indicates that the wind speed – pressure relationship is more or less invariant across years, and that perhaps 1997 and 2000 were not such bad storm years compared to the others. This isn’t really surprising. The occurrence of tropical storms is somewhat stochastic, but the laws of atmospheric physics don’t change from one year to the next! Don’t forget the ~ We have to include the ~ symbol at the beginning of the ~ year part of the facet_wrap specification. Trust us, the faceting won’t work without it. The ~ specifies something called a ‘formula’. The main job of a formula is to specify relationships among variables. These are usually used in R’s statistical modelling functions (not covered in this book), but they occasionally pop up in other places. "],
["customising-plots.html", "Chapter 19 Customising plots 19.1 Working with layer specific geom properties 19.2 Working with layer specific position adjustments 19.3 Working with plot specific scales 19.4 Adding titles and labels 19.5 Themes", " Chapter 19 Customising plots The default formatting used by ggplot2 is generally fine for exploratory purposes. In fact, although they aren’t universally popular, the defaults are carefully chosen to ensure that the information in a plot is easy to discern. These choices are a little unconventional though. For example, published figures usually use a white background. For this reason, we often need to change the appearance of a plot once we’re ready to include it in a report. Our aim in this chapter is to learn a little bit about the underlying logic of how to customise ggplot2. We aren’t going to attempt to cover the many different permutations. Instead, we’ll focus on the main principles underlying the different routes to customisation. We’ll build on these as we review a range of different visualisations in later chapters. Using the storms data once again, we’ll work on improving the simple scatter plot initially produced in the Introduction to ggplot2 chapter: # 1. make the storms data available library(nasaweather) # 2. plot wind speed against atmospheric pressure ggplot(storms, aes(x = pressure, y = wind)) + geom_point() 19.1 Working with layer specific geom properties What do we do if we if we need to change the properties of a geom? We’re using the point geom at the moment. How might we change the colour or size of points in our scatter plot? It’s quite intuitive—we set the appropriate arguments in the geom_point function. Let’s rebuild our example, this time setting the colour, size and transparency of points: ggplot(storms, aes(x = pressure, y = wind)) + geom_point(colour = &quot;steelblue&quot;, size = 1.5, alpha = 0.3) The point colour is set with the colour argument. There are many ways to specify colours in R, but if we only need to specify a few the simplest is to use a name R recognises. The point size is specified with the size argument. The baseline is 1, and so here we increased the point size by assigning this a value of 1.5. Finally, we made the points somewhat transparent by setting the value of the alpha argument to be less than 1. In graphical systems the ‘alpha channel’ essentially specifies transparency of something—a value of 0 is taken to mean ‘completely invisible’ and a value of 1 means ‘completely opaque’. Built-in colours in R There is nothing special about “steelblue” other than the fact that it is colour name ‘known’ to R. There are over 650 colour names built-in to R. To see them, we can use a function called colours to print these to the Console. Try it. There are other arguments—such as fill and shape—that can be used to adjust the way the points are rendered. We won’t look at these here. The best way to learn how these work is to simply experiment with them. The key message to take away from this little customisation example is this: if we want to set the properties of a geom in a particular layer, we do so by specifying the appropriate arguments in the geom_NAME function that defines that layer. We do not change the arguments passed to the ggplot function. How should we format ggplot2 code? Take another look at that last example. Notice that we split the ggplot2 definition over two lines, placing each function on its own line. The whole thing will still be treated as a single expression when we do this because each line, apart from the last one, ends in a +. R doesn’t care about white space. As long as we leave the + at the end of each line R will consider each new line to be part of the same definition. Splitting the different parts of a graphical object definition across lines like this is a very good idea. It makes everything more readable and helps us spot errors. This way of formatting ggplot2 code is pretty much essential once we start working with complex plots. We will always use this convention from now on. 19.1.1 The relationship between aesthetics and geom properties. We’ve seen that we can introduce new information into a plot by setting up additional aesthetics. In the previous chapter we added the information about the time of year an observation was made by mapping the seasday variable to the colour aesthetic. Let’s try to add this to our new figure: ggplot(storms, aes(x = pressure, y = wind, colour = seasday)) + geom_point(colour = &quot;steelblue&quot;, size = 1.5, alpha = 0.3) This doesn’t seem to have worked as hoped as the resulting scatter plot looks exactly like the previous one, i.e. all the points are the same colour. What went wrong? We’re still setting the colour argument of geom_point. When we add a layer, any layer-specific properties that we set will override the aesthetic mappings. We need to remove the colour = &quot;steelblue&quot; from inside geom_point to remedy this: ggplot(storms, aes(x = pressure, y = wind, colour = seasday)) + geom_point(size = 1.5, alpha = 0.3) That’s what we were aiming for. The points are now coloured again according to how they are associated with early (dark blue) or late (light blue) season observations. The key message to take away from this example is this: if we decide to change the properties of the geom in a particular layer, we will override any aesthetic mappings that conflict with our choice of customisation. 19.2 Working with layer specific position adjustments What else might we do to make the plot a little easier to read? Wind speed is only measured to the nearest 5 mph, which is causing many points to be plotted on top of one another. One option to solve this problem is to randomly shuffle the vertical position of each point a little to avoid this over-plotting. This is called ‘jittering’. We do this by specifying a position adjustment in our layer. Remember, position adjustments are part of individual layers, not the whole plot. Here’s one way to do this: ggplot(storms, aes(x = pressure, y = wind, colour = seasday)) + geom_point(alpha = 0.3, size = 1.5, position = position_jitter(w = 0, h = 4)) We used the position_jitter function to associate the necessary information with the position argument of geom_point. The w (’w’idth) and h (’h’eight) arguments of the position_jitter function specify how much to jitter points in the x and y directions. The resulting plot is a little easier to read. The key message to take away from this second customisation example is this: every layer has its own position adjustment, which we can change by setting the position argument inside the geom_NAME function that defines the corresponding layer. Just keep in mind that, very often, there’s no need to mess about with position adjustments (the defaults are fine). 19.3 Working with plot specific scales Let’s look at a different way to tweak our plot. So far we have been focussing on customisations that apply in a layer specific manner (geom properties and position adjustments). A second class of customisation applies to the whole plot. Specifically, this new type of customisation applies to the scales used in the plot. Here’s what we said about scales in the last chapter: The scale controls how the data is mapped to the aesthetic attributes. A scale takes the data and converts it into something we can perceive, such as an x/y location, or the colour and size of points in a plot. A scale must be defined for every aesthetic in a plot. Every aesthetic has a scale associated with it. We adjust ‘how the data is mapped to the aesthetic attributes’ by changing some aspect of the corresponding scale. This will seem very abstract at first. As always it’s best understood by example. We’re going to adjust the scale associated y axis aesthetic (‘y’). Specifically, we want to increase the number of ‘guides’ (the horizontal lines inside the plot) and their accompanying labels. Here is how we place guides at 20, 40, 60, 80, etc, on the y axis: ggplot(storms, aes(x = pressure, y = wind, colour = seasday)) + geom_point(alpha = 0.3, size = 1.5, position = position_jitter(w = 0, h = 4)) + scale_y_continuous(breaks = seq(20, 160, by = 20)) What’s going on here? The functions that adjust a scale all have the general form scale_XX_YY. The XX bit in the name must reference the relevant aesthetics, while the YY part refers to the kind of scale we want to define. The aesthetic we wanted to alter was the y axis. It turns out (though it probably wasn’t obvious) that this is a continuous scale because wind is a numeric variable. This means we had to use the scale_y_continuous function to tweak the y axis (there is a scale_x_continuous function for altering the x axis). The breaks argument just takes a vector containing a numeric sequence and uses this to specify where the guides should be drawn. Scales are the hardest aspect of ggplot2 to get to grips with. For one, there are a lot of them—type scale_ at the Console and hit the tab key to see how many there are. Each of them can take a variety of different arguments. Luckily, the defaults used by ggplot2 are often good enough that we can arrive at a good plot without having to manipulate the scales. We’ll take a look at a few more options as we progress through different visualisations. The key message to take away from this third customisation example is this: every aesthetic mapping has a scale associated with it. If we want to change how the information associated with an aesthetic is displayed we should change the corresponding scale. For example, if we want to change the way point colours are associated with the seasday variable we have to use one of the scale_colour_YY functions. 19.4 Adding titles and labels What else might we like to tweak? Look at the x and y axis labels. These are just the names of the data variables used to define the aesthetic mapping. These labels aren’t too bad in this case, but they could be more informative. We know “wind” stands for “wind speed”, but someone reading this figure may not realise this immediately. There are also no units – generally a big no-no for serious figures. Here is how to set the axis labels: ggplot(storms, aes(x = pressure, y = wind, colour = seasday)) + geom_point(alpha = 0.3, size = 1.5, position = position_jitter(w = 0, h = 4)) + scale_y_continuous(breaks = seq(20, 160, by = 20)) + xlab(&quot;Atmospheric Pressure (mbar)&quot;) + ylab(&quot;Wind Speed (mph)&quot;) The axes labels are a feature of the whole plot. They do not belong to a particular layer. This is why we don’t alter axis labels by passing arguments to the function that built a layer (geom_point in this case). Instead, we use the xlab and ylab functions to set the x and y labels, respectively, using + to add them to our graphical object. If we need to add a title to a graph we can use the ggtitle function in the same way. The labs function provides a more flexible alternative to xlab and ylab. It’s more flexible because labs can be used to change the label of every aesthetic in the plot. For example, if we want to set the labels of the x and y axes, and the label associated with seasday, we use: ggplot(storms, aes(x = pressure, y = wind, colour = seasday)) + geom_point(alpha = 0.3, size = 1.5, position = position_jitter(w = 0, h = 4)) + scale_y_continuous(breaks = seq(20, 160, by = 20)) + labs(x = &quot;Atmospheric Pressure (mbar)&quot;, y = &quot;Wind Speed (mph)&quot;, colour = &quot;Day of \\nSeason&quot;) We snuck one last trick into that last example. Notice that the “Day of Season” label is split over two lines. We did this by inserting the special sequnce \\n into the label text. The \\n inside a quoted label tells R to start a new line. 19.5 Themes The final route to customisation we’ll consider concerns something called the ‘theme’ of a plot. We haven’t considered the ggplot2 theme system at all yet. In simple terms, ggplot2 themes deal with the visual aspects of a plot that aren’t directly handled by adjusting geom properties or scales, i.e. the ‘non-data’ parts of a plot. This includes features such as the colour of the plotting region and the grid lines, whether or not those grid lines are even displayed, the position of labels, the font used in labels, and so on. The ggplot2 theme system is extremely powerful. Once we know how to use it, we can set up a custom theme to meet our requirements and then apply it as needed with very little effort. However, it’s not an entirely trivial thing to learn about because there are so many components of every plot. Fortunately there are a range of themes built into ggplot2 that are easily good enough for producing publication ready figures. Let’s assume we have made a plot object final_plt containing all the information and data formatting we want: final_plt &lt;- ggplot(storms, aes(x = pressure, y = wind, colour = seasday)) + geom_point(alpha = 0.3, size = 1.5, position = position_jitter(w = 0, h = 4)) + scale_y_continuous(breaks = seq(20, 160, by = 20)) + labs(x = &quot;Atmospheric Pressure (mbar)&quot;, y = &quot;Wind Speed (mph)&quot;, colour = &quot;Day of \\nSeason&quot;) Here’s how to use the built in themes to alter the themes used to plot the final_plt object: final_plt + theme_bw() In this example we use + with the theme_bw function to use the built in ‘black and white’ theme. This removes the grey background that so many people dislike in ggplot2. There aren’t that many themes built into ggplot2—type theme_ at the Console and hit the tab key to see the others. One popular alternative to theme_bw is the ‘classic’ theme, via theme_classic: final_plt + theme_classic(base_size = 15) This produces a very stripped down plot that’s much closer to those produced by the base graphics system. Notice that we did one more thing here: we set the base_size argument to 15 to increase the size of all the text in the figure (the default is 11). This can be used with any theme_XX function to quickly change the relative size of all the text in a plot. "],
["exploring-one-numeric-variable.html", "Chapter 20 Exploring one numeric variable 20.1 Understanding numerical variables 20.2 Graphical summaries 20.3 Descriptive statistics", " Chapter 20 Exploring one numeric variable This chapter will consider how to go about exploring the sample distribution of a numeric variable. Using the storms data from the nasaweather package (remember to load and attach the package), we’ll review some basic descriptive statistics and visualisations that are appropriate for numeric variables. 20.1 Understanding numerical variables We’ll work with the wind and pressure variables in storms to illustrate the key ideas. Wind speed and atmospheric pressure are clearly numeric variables. We can say a bit more. They are both numeric variables that are measured on a ratio scale because zero really is zero: it makes sense to say that 20 mph is twice as fast as 10 mph and 1000 mbar exerts twice as much pressure on objects as 500 mbar. Are these continuous or discrete variables? Think about the possible values that wind speed and atmospheric pressure can take. A wind speed and atmospheric pressure of 40.52 mph and 1000.23 mbar are perfectly reasonable values, so fundamentally, these are continuous variables. The simplest way to understand our data, if not the most effective, is to view it in its raw form. We can always use the View function to do this in RStudio. However, since this doesn’t work on a web page, we’ll take a quick look at the first 100 values of the wind and pressure variables in storms. We can print these to the Console by extracting each of them with the $ operator, using the [ construct to subset the first 100 elements of each vector: # first 100 values of atmospheric pressure storms$pressure[1:100] ## [1] 1005 1004 1003 1001 997 995 987 988 988 990 990 993 993 994 ## [15] 995 995 992 990 988 984 982 984 989 993 995 996 997 1000 ## [29] 997 990 992 992 993 1019 1019 1018 1017 1016 1013 1011 1009 1007 ## [43] 1004 1001 997 997 997 997 996 995 993 991 990 989 1012 1012 ## [57] 1012 1011 1011 1011 1010 1010 1006 1008 1009 1010 1009 1006 1006 1005 ## [71] 1004 999 999 997 991 995 997 995 994 994 995 996 997 997 ## [85] 998 998 999 999 1000 1000 1001 1002 1003 1005 1005 1009 1008 1008 ## [99] 1008 1007 # first 100 values of wind speed storms$wind[1:100] ## [1] 30 30 35 40 50 60 65 65 65 60 60 45 30 35 35 40 40 45 45 45 50 50 50 ## [24] 45 40 40 40 40 40 40 40 35 35 20 20 20 25 25 30 30 30 35 40 60 60 55 ## [47] 50 50 50 50 50 50 45 40 25 25 25 30 30 30 30 30 35 35 35 40 40 45 45 ## [70] 45 45 50 50 55 60 60 60 55 55 55 50 50 50 50 50 50 50 50 50 50 50 50 ## [93] 50 50 50 25 30 30 30 30 Notice that even though pressure is continuous variables it looks like a discrete variable because it has only been measured to the nearest whole millibar. Similarly, wind is only measured to the nearest 5 mph. These differences reflect the limitations of the methodology used to measure each variable, e.g. measuring wind speed is hard because it varies so much in space and time. This illustrates an important idea: we can’t just look at the values a numeric variable takes in a sample to determine whether it is discrete or continuous. In one sense the pressure variable is a discrete variable because of the way it was measured, even though we know that atmospheric pressure is really continuous. Whether we treat it as continuous or discrete is an analysis decision. These sorts of distinctions often don’t matter too much when we’re exploring data, but they can matter when we’re deciding how to analyse it statistically. We have to make a decision about how to classify a variable based on knowledge of its true nature and the measurement process. For example, imagine that we were only able to measure wind speed to the nearest 25 mph. In this situation we would only “see” a few different categories of wind speed, so it might be sensible to treat the wind variable as an ordinal, categorical variable. 20.2 Graphical summaries We only looked at the first 100 values of the wind and pressure variables because the storms data set is too large to look everything at once. It’s hard to say much about the sample distribution of these two variables by just looking at such a small subset of values. If the data set has been sorted, these might not even be representative of the wider sample. What else might we do? One useful tool is ‘binning’. The idea behind binning a variable is very simple. It involves two steps. First, we take the set of possible values of our numeric variable and divide this into a equal sized, non-overlapping intervals. We can use any interval size we like, as long as it is large enough to span at least two observations some of the time, though in practice some choices are more sensible than others. We then have to work out how many values of the our variable fall inside each bin. The resulting set of counts tells us quiet a lot about the sample distribution. Let’s see how this works with an example. Binning is very tedious to do by hand, but as we might expect, there are a couple of base R function that can do this for us: cut and table. Here’s how to use these to bin the pressure variable into intervals of 10 mbar: presure_bins &lt;- cut(storms$pressure, breaks = seq(900, 1020, by = 5), right = FALSE) table(presure_bins) ## presure_bins ## [900,905) [905,910) [910,915) [915,920) [920,925) [925,930) ## 0 1 2 2 7 4 ## [930,935) [935,940) [940,945) [945,950) [950,955) [955,960) ## 14 23 35 44 47 39 ## [960,965) [965,970) [970,975) [975,980) [980,985) [985,990) ## 79 81 156 127 170 240 ## [990,995) [995,1000) [1000,1005) [1005,1010) [1010,1015) [1015,1020) ## 252 291 466 515 134 18 We won’t explain how cut and table work as we only need to understand the output. The output of table is a named numeric vector. The names of each element describe an interval, and the corresponding values are the observation counts in that interval. What does this tell us? It shows that most pressure observations associated with storm systems are round about 1000 mbar. Values higher than 1000 mbar are rare, but a range of values below this are possible, with lower and lower values becoming less frequent. These binned data tell us quite a lot about the sample distribution of pressure. It’s still difficult to perceive the information in this output when it is presented as a series of numbers. What we really need is some kind of visualisation to help us interpret these numbers. This is what a histogram provides. Histograms are designed to summarise the sample distribution of a variable by showing the counts of binned data as a series of bars. The position and width of each bar corresponds to an interval and the height shows the count. Here’s a histogram that corresponds to the binned data we just made: This gives a clear summary of the sample distribution of pressure. It reveals: 1) the most common values, which are just above 1000 mbar; 2) the range of the data, which is about 100 mbar; and 3) the shape of the distribution, which is asymmetric, with a tendency toward low values. We used ggplot2 to make that histogram. We could do this by building a new data set with the binned data, and then use this with ggplot2 to construct the histogram manually. There is a much easier way to achieve the same result though. Rather than do it one one step with a single R expression, we will break the process up into two steps, storing the the ggplot2 object as we build it. The first step uses the ggplot function with aes to set up the default data and aesthetic mapping: plt_hist &lt;- ggplot(storms, aes(x = pressure)) This is no different than the extended scatter plot example we stepped through earlier. The only difference is that a histogram requires only one aesthetic mapping. We supplied the argument x = pressure to aes because we want to display the map intervals associated with pressure to the x axis. We don’t need to supply an aesthetic mapping for the y axis because ggplot2 is going to handle this for us. The second step adds a layer to the plt_hist object. We need to find the right geom_XX function to do this. Unsurprisingly, this is called geom_histogram: plt_hist &lt;- plt_hist + geom_histogram() summary(plt_hist) ## data: name, year, month, day, hour, lat, long, pressure, wind, ## type, seasday [2747x11] ## mapping: x = pressure ## faceting: &lt;ggproto object: Class FacetNull, Facet&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map: function ## map_data: function ## params: list ## render_back: function ## render_front: function ## render_panels: function ## setup_data: function ## setup_params: function ## shrink: TRUE ## train: function ## train_positions: function ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet&gt; ## ----------------------------------- ## geom_bar: na.rm = FALSE ## stat_bin: binwidth = NULL, bins = NULL, na.rm = FALSE, pad = FALSE ## position_stack Look at the text of the summary of the added layer below the ----. This shows that geom_histogram adds a stat to the layer, the stat_bin. What this means is that ggplot2 is going to take the raw pressure data and bin it for us. Everything we need to plot a histogram is now set up. Here’s the resulting plot: plt_hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The resulting plot is not quite the same as the example we saw above because it uses different bins. It’s a good idea to play around with the bin size to arrive at an informative histogram. We set the properties of the geom_histogram to tweak this kind of thing—the binwidth argument adjusts the width of the bins used. Let’s construct the histogram again with 7 mbar wide bins, as well as adjust the colour scheme and axis labels a bit: ggplot(storms, aes(x = pressure)) + geom_histogram(binwidth = 7, fill = &quot;steelblue&quot;, colour=&quot;darkgrey&quot;, alpha = 0.8) + xlab(&quot;Atmospheric Pressure (mbar)&quot;) + ylab(&quot;Count&quot;) Whether or not that colour scheme is an improvement is a matter of taste. Mostly we wanted to demonstrate how the fill, colour, and alpha arguments change the output. Notice that the effect of increasing the bin width is to ‘smooth’ the histogram, i.e. this version looks less jagged than the last. We can use pretty much the same R code to produce a histogram summarising the wind speed sample distribution: ggplot(storms, aes(x = wind)) + geom_histogram(binwidth = 10, fill = &quot;steelblue&quot;, colour=&quot;darkgrey&quot;, alpha = 0.8) + xlab(&quot;Wind Speed (mph)&quot;) + ylab(&quot;Count&quot;) The only things that changed in this example were the aesthetic mapping and the bin width, which we set to 10. It reveals that the wind speed during a storm tends to be about 40 mph, though the range of wind speeds is about 100 mph and the shape of the distribution is asymmetric. We have to choose the bin widths carefully. Remember that wind speed is measured to the nearest 5 mph. This means we should choose a bin width that is a multiple of 5 to produce a meaningful histogram. Look what happens if we set the bin width to 3: ggplot(storms, aes(x = wind)) + geom_histogram(binwidth = 3, fill = &quot;steelblue&quot;, colour=&quot;darkgrey&quot;, alpha = 0.8) + xlab(&quot;Wind Speed (mph)&quot;) + ylab(&quot;Count&quot;) We end up with gaps in the histogram because some intervals do not include multiples of 5. This is not a good histogram because it fails to reliably summarise the distribution. Similar problems would occur if we chose a bin width that is greater than, but not a multiple of 5, because different bins would cover a different number of values that make up the wind variable. The take home message is that we have to know our data in order to produce meaningful summaries of it. We’ll finish up this subsection by briefly reviewing one alternative to the histogram. Histograms are good for visualising sample distributions when we have a reasonable sample size (at least dozens, and ideally, hundreds of observations). They aren’t very effective when the sample is quite small. In this ‘small data’ situation it’s better to use something called a dot plot8. Let’s use dplyr to extract a small(ish) subset of the storms data: storms_small &lt;- storms %&gt;% filter(year == 1998, type == &quot;Hurricane&quot;) This just extracts the subset of hurricane observations from 1998. The ggplot2 code to make a dot plot with these data is very similar to the histogram case: ggplot(storms_small, aes(x = pressure)) + geom_dotplot(binwidth = 2) + xlab(&quot;Atmospheric Pressure (mbar)&quot;) + ylab(&quot;Count&quot;) Here, each observation in the data adds one dot, and dots that fall into the same bin are stacked up on top of one another. The resulting plot displays the same information about a sample distribution as a histogram, but it tends to be more informative when there are relatively few observations. 20.3 Descriptive statistics So far we’ve been describing the properties of sample distributions in very general terms, using phrases like ‘most common values’ and ‘the range of the data’ without really saying what we mean. Statisticians have devised specific terms to describe these kinds of properties, as well as different descriptive statistics to quantify them. The two that matter most are the central tendency and the dispersion: A measure of central tendency describes a typical (‘central’) value of a distribution. Most people know at least one measure of central tendency. The “average” that they calculated at school is the arithmetic mean of a sample. There are many different measures of central tendency, each with their own pros and cons. Take a look at the Wikipedia to see the most common ones. Among these, the median is the one that is used most often in exploratory analyses. A measure of dispersion describes how spread out a distribution is. Dispersion measures quantify the variability or scatter of a variable. If one distribution is more dispersed than another it means that in some sense it encompasses a wider range of values. What this means in practice depends on the kind of measure we’re working with. Basic statistics courses tend to focus on the variance, and its square root, the standard deviation. There are others though. 20.3.1 Measuring central tendency There are two descriptive statistics that are typically used to describe the central tendency of the sample distribution of numeric variables. The first is the arithmetic mean of a sample. People often say ‘empirical mean’, ‘sample mean’ or just ‘the mean’ when referring to the arithmetic sample mean. This is fine, but keep in mind that there are other kinds of mean (e.g. the harmonic mean and the geometric mean)9. How do we calculate the arithmetic sample mean of a variable? Here’s the mathematical definition: \\[ \\bar{x} = \\frac{1}{N}\\sum\\limits_{i=1}^{N}{x_i} \\] We need to define the terms to make sense of this. The \\(\\bar{x}\\) stands for the arithmetic sample mean. The \\(N\\) in the right hand side of this expression is the sample size, i.e. the number of observations in a sample. The \\(x_i\\) refer to the set of values the variable takes in the sample. The \\(i\\) is an index used to reference each observation: the first observation has value \\(x_1\\), the second has value \\(x_2\\), and so on, up to the last value, \\(x_N\\). Finally, the \\(\\Sigma_{i=1}^{N}\\) stands for summation (‘adding up’) from \\(i = 1\\) to \\(N\\). Most people have used this formula at some point even though they may not have realised it. The mean function in R will calculate the arithmetic mean for us: mean(storms$wind) ## [1] 54.68329 This tells us that the arithmetic sample mean of wind speed is 55 mph. How useful is this? One limitation of the arithmetic mean is that it is affected by the shape of a distribution. It’s very sensitive to the extremes of a sample distribution. This is why, for example, it does not make much sense to look at the mean income of workers in a country to get a sense of what a ‘typical’ person earns. Income distribution are highly asymmetric, and those few who are lucky enough to earn very good salaries tend to shift the mean upward and well past anything that is really ‘typical’. The sample mean is also strongly affected by the presence of ‘outliers’. It’s difficult to give a precise definition of outliers—the appropriate definition depends on the context—but roughly speaking, these are unusually large or small values. Because the sample mean is sensitive to the shape of a distribution and the presence of outliers we often prefer a second measure of central tendency: the sample median. The median of a sample is the number separating the upper half from the lower half10. We can find the sample median in R with the median function: median(storms$wind) ## [1] 50 The sample median of wind speed is 50 mph. This is still to the right of the most common values of wind speed, but it shifted less than the mean. 20.3.1.1 What about ‘the mode’? What does the phrase “the most common values” (e.g. of wind speed) really mean when describing a distribution? In fact, this is an indirect reference to something called the mode of the distribution. The mode of a distribution is essentially its peak, i.e. it locates the most likely value of a variable. Notice that we didn’t use the phrase ‘sample mode’. It’s easy to calculate the mode of a theoretical distribution. Unfortunately, it’s not a simple matter to reliably estimate the mode of a sample from such a distribution. If a numeric variable is discrete, and we have a lot of data, we can sometimes arrive at an estimate of the mode by tabulating the number of observations in each numeric category. Although in truth wind speed is a continuous variable, it is only measured to the nearest 5mph in the storms data set. Therefore, it looks like a discrete variable. We can use the table function here to tabulate the number of observations at each value: table(storms$wind) ## ## 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 ## 3 122 168 345 234 212 228 208 136 154 189 136 119 94 61 76 40 57 ## 105 110 115 120 125 130 135 140 145 150 155 ## 26 30 42 27 20 8 5 1 2 2 2 The names of each element in the resulting vector are the recorded wind speeds and the corresponding values are the associated counts of each value. The most common wind speed is 30 mph, with 345 observations. The categories either side of this (25 and 35 mph) contain much lower counts. This provides a fairly good indication that the mode of the wind distribution is about 30mph. Tabulating the counts in of numeric categories to identify the likely mode is only sensible when a numerical variable is genuinely discrete, or looks discrete as a result of how it was measured. Even then, there is no guarantee that this approach will produce a sensible estimate of the ‘true’ mode. If a variable is continuous then tabulating counts simply does not work. Methods exist to estimate a mode from a sample, but they are not simple. Nonetheless, it’s important to know what the mode represents and to be able to identify its approximate value by inspecting a histogram. 20.3.2 Measuring dispersion There are many ways to quantify the dispersion of a sample distribution. The most important quantities from the standpoint of statistics are the sample variance and standard deviation. The sample variance (\\(s^2\\)) is ‘the sum of squared deviations’ (i.e. the differences) of each observation from the sample mean, divided by the sample size minus one. Here’s the mathematical definition: \\[ s^2 = \\frac{1}{N-1}\\sum\\limits_{i=1}^{N}{(x_i-\\bar{x})^2} \\] The meaning of these terms is the same as for the sample mean. The \\(\\bar{x}\\) is the sample mean, the \\(N\\) is the sample size, and the \\(x_i\\) refers to the set of values the variable takes. We don’t have to actually apply this formula in R. There’s a function to calculate the sample variance: var(storms$wind) ## [1] 668.1444 What does that number actually mean? Variances are always non-negative. A small variance indicates that observations tend to be close to the mean (and to one another), while a high variance indicates that observations are very spread out. A variance of zero only occurs if all values are identical. However, it is difficult to interpret whether a sample variance is really “small” or “large” because the calculation involves squared deviations. For example, changing the measurement scale of a variable by 10 involves a 100-fold change (102) in the variance. The variance is a important quantity in statistics that crops up over and over again. Many common statistical tools use changes in variance to formally compare how well different models describe a data set. However, it is very difficult to interpret variances, which is why we seldom use them in exploratory work. A better statistic is to describe sample dispersion is a closely related quantity called the standard deviation of the sample, usually denoted \\(s\\). The standard deviation is the square root of the variance. We calculate it using the sd function: sd(storms$wind) ## [1] 25.84849 Why do we prefer the standard deviation over the variance? Consider the wind speed again. The standard deviation of the wind speed sample is 26. Take another look at the wind speed histogram. This shows that the wind speed measurements span about 5 standard deviations. If we had instead measured wind speed in kilometers per hour (kph), the standard deviation of the sample would be 42, because 1 mph ~ 1.6 kph. If we plot the histogram of wind speed in kph it is still the case that the data are spanned by about 5 standard deviations. The variance on the other hand increases from to approximately 668 to 1730, a factor of 1.62. This is the reason we often use the standard deviation compare dispersion: it reflects the dispersion we perceive in the data. The sample standard deviation is not without problems though. Like the sample mean, it is sensitive to the shape of a distribution and the presence of outliers. A measure of dispersion that is more robust to these kinds of features is the interquartile range. What are quartiles? We need to know what a quartile is to understand the interquartile range. Three quartiles are defined for any sample. These divide the data into four equal sized groups, from the set of smallest numbers up to the set of largest numbers. The second quartile (Q2) is the median, i.e. it divides the data into an upper and lower half. The first quartile (Q1) is the number that divides the lower 50% of values into two equal sized groups. The third quartile (Q3) is the number that divides the upper 50% of values into two equal sized groups. The quartiles also have other names. The first quartile is sometimes called the lower quartile, or the 25th percentile; the second quartile (the median) is the 50th percentile; and the third quartile is also called the upper quartile, or the 75th percentile. The interquartile range (IQR) is defined as the difference between the third and first quartile. This means the IQR contains the middle 50% of values of a variable. Obviously, the more spread out the data are, the larger the IQR will be. The reason we prefer to use IQR to measure dispersion is that it only depends on the data in the “middle” of a sample distribution. This makes it robust to the presence of outliers. We can use the IQR function to find the interquartile range of the wind variable: IQR(storms$wind) ## [1] 35 The IQR is used as the basis for a useful data summary plot called a ‘box and whiskers’ plot. We’ll see how to construct this in a later next chapter. 20.3.3 Skewness A well-defined hierarchy has been defined to describe and quantify the shape of distributions. It’s essential to know about the first two, central tendency and dispersion, because these are the basis of many standard analyses. The next most important aspect of a distribution is its skewness (or just ‘skew’). Skewness describes the asymmetry of a distribution. Just as with central tendency and dispersion, there are many different ways to quantify the skewness of a sample distribution. These are quite difficult to interpret because their interpretation depends on other features of a distribution. We’ll just explore skewness in the simplest case: the skewness of a unimodal distribution. A unimodal distribution is one that has a single peak. We can never say for certain that a sample distribution is unimodal or not—unimodality is really a property of theoretical distributions—but with enough data and a sensible histogram we can at least say that a distribution is ‘probably’ unimodal. The histograms we produced to describe the sample distributions of wind and pressure certainly appear to be unimodal. Each has a single, distinct peak. These two unimodal distributions are also asymmetric—they exhibit skewness. The pressure distribution is said to be skewed to the left because it has a long ‘tail’ that spreads out in this direction. In contrast, we say that the wind distribution is skewed to the right, because it has a long ‘tail’ that spreads out to right. Left skewness and right skewness are also called negative and positive skew, respectively. A sample distribution that looks symmetric is said to have (approximately) zero skew11. The reason we care about skewness is that many common statistical models assume that the distributions we’re sampling from, after controlling for other variables, are not skewed. This is an issue for a statistics course. For now we just need to understand what skewness means and be able to describe distributions in terms of left (negative) and right (positive) skew. Not to be confused with the ‘Cleveland dot plot’. A standard dot plot summarises a sample distribution. The Cleveland dot plot is something quite different, which summarises the frequencies of a categorical variable. It’s meant to serve as a simple alternative to bar charts and pie charts.↩ There is also a very important distinction to be made between the sample mean and the (unobserved) population mean, but we can ignore this distinction for now as we are only concerned with samples. The distinction matters when thinking about statistical models and tests↩ If we have an even sample size, exactly half the data are larger than the median and half the data are smaller than the median. The sample median is then half way between the largest value of the lower half and the smallest value of the upper half. If a sample has an odd number of observations the sample median is just the value of the observation that divides the remaining data into two equal sized high- and low-value sets.↩ Strictly speaking the terms “negative” and “positive” are reserved for situations where we have calculated a quantitative measure of skew. However, they are often used informally in verbal descriptions of skewness.↩ "],
["exploring-categorical-variables.html", "Chapter 21 Exploring categorical variables 21.1 Understanding categorical variables 21.2 Graphical summaries of categorical variables", " Chapter 21 Exploring categorical variables This chapter will consider how to go about exploring the sample distribution of a categorical variable. Using the storms data from the nasaweather package (remember to load and attach the package), we’ll review some basic descriptive statistics and visualisations that are appropriate for categorical variables. 21.1 Understanding categorical variables Exploring categorical variables is generally simpler than working with numeric variables because we have fewer options, or at least life is simpler if we only require basic summaries. We’ll work with the year and type variables in storms to illustrate the key ideas. Which kind of categorical variable is type? There are four storm categories in type. We can use the unique function to print these for us: unique(storms$type) ## [1] &quot;Tropical Depression&quot; &quot;Tropical Storm&quot; &quot;Hurricane&quot; ## [4] &quot;Extratropical&quot; The first question we should ask is, is type an ordinal or nominal variable? It’s hard to know how to classify type without knowing something about tropical storms. Some googling indicates that type can reasonably be considered an ordinal variable: a tropical depression is the least severe storm and a hurricane is the most severe class; in between are extra tropical and tropical storm categories. What about the year variable? Years are obviously ordered from early to late and we might be interested in how some aspect of our data changes over time. In this case we might consider treating year either as a numeric variable, or perhaps as an ordinal categorical variable. Alternatively, if the question is simply, ‘do the data vary from one year to the next’ without any concern for trends, it’s perfectly reasonable to treat year as a nominal categorical variable. This illustrates an important idea: the classification of a variable will often depend on the objectives of an analysis. The classification of a variable matters because it influences how we choose to summarise it, how we interpret its relationship with other variables, and whether a specific statistical model is appropriate for our data or not. Fortunately, our choice of classification is less important when we are just trying to summarise the variable numerically or graphically. For now, let’s assume that it’s fine to treat year as a categorical variable. 21.1.1 Numerical summaries When we calculate summaries of categorical variables we are aiming to describe the sample distribution of the variable, just as with numeric variables. The general question we need to address is, ‘what are the relative frequencies of different categories?’ We need to understand which categories are common and which are rare. Since a categorical variable takes a finite number of possible values, the simplest thing to do is tabulate the number of occurances of each type. We’ve seen how the table function is used to do this: table(storms$type) ## ## Extratropical Hurricane Tropical Depression ## 412 896 513 ## Tropical Storm ## 926 This shows that the number of observations associated with hurricanes and tropical storms are about equal, that the number of observations associated with extratropical and tropical systems is similar, and the former pair of categories are more common than the latter. This indicates that in general, storm systems in Central America spend relatively more time in the more severe classes. Raw frequencies give us information about the rates of occurance of different categories in a dataset. However, it’s difficult to compare raw counts across different data sets if the sample sizes vary (which they usually do). This is why we often convert counts to proportions. To do this, we have to divide each count by the total count across all categories. This is easy to do in R because it’s vectorised: type_counts &lt;- table(storms$type) type_counts / sum(type_counts) ## ## Extratropical Hurricane Tropical Depression ## 0.1499818 0.3261740 0.1867492 ## Tropical Storm ## 0.3370950 So about 2/3 of observations are associated with hurricanes and tropical storms, with a roughly equal split, and the remaining 1/3 associated with less severe storms. What about measuring the central tendency of a categorical sample distribution? Various measures exist, but these tend to be less useful than those used to describe numeric variables. We can find the sample mode of ordinal and nominal variables easily though (in contrast to numeric variables, where it is difficult to define). This is just the most common category. For example, the tropical storm category is the modal value of the type variable. Only just though. The proportion of tropical storm observations is 0.34, while the proportion of hurricane observations is 0.33. These are very similar, and it’s not hard to imagine that modal observation might have been the hurricane category in a different sample. The sample mode is sensitive to chance variation when two categories occur at similar frequencies. It is possible to calculate a sample median of a categorical variable, but only for the ordinal case. The median value is the one that lies in the middle of an ordered set of values—it makes no sense to talk about “the middle” of a set of nominal values that have no inherent order. Unfortunately, even for ordinal variables the sample median is not precisely defined. Imagine that we’re working with a variable with only two categories: ‘big’ vs. ‘small’, and exactly 50% of the values are ‘small’ value and 50% are large. What is the median in this case? Because the median is not always well-defined, the developers of base R have chosen not to implement a function to calculate the median of ordinal variables (a few packages contain functions to do this though). Be careful with median Unfortunately, if we apply the median function to a character vector it will give us an answer, e.g. median(storms$type) will spit something out. It is very likely to give us the wrong answer though. R has no way of knowing which categories are “high” and which are “low”, so just sorts the elements of type alphabetically and then finds the middle value of this vector. If we really have to find the median value of an ordinal value we can do it by first converting the categories to integers— assigning 1 to the lowest category, 2 to the next lowest, and so on— and then use the median function to find out which value is the median. 21.2 Graphical summaries of categorical variables The most common graphical tool used to summarise a categorical variable is a bar chart. A bar chart (or bar graph) is a plot that presents summaries of grouped data with rectangular bars. The lengths of the bars is proportional to the values they represent. When summarising a single categorical variable, the length of the bars should show the raw counts or proportions of each category. Constructing a bar graph to display the counts is very easy with ggplot2. We will do this for the type variable. As always, we start by using the ggplot function to construct a graphical object containing the necessary default data and aesthetic mapping. bar_plt &lt;- ggplot(storms, aes(x = type)) We’ve called the object bar_plt, for obvious reasons. Notice that we only need to define one aesthetic mapping: we mapped type to the x axis. This produces a bar plot with vertical bars. From here we follow the usual ggplot2 workflow, meaning the next step is to add a layer using one of the geom_XX functions. There are two functions we can use to create bar charts in ggplot, geom_bar and geom_col. By default geom_col counts the number of observations in each category, whilst geom_bar plots the actual numbers in the data frame. In this case as we want the number of storms of each type we will use geom_bar: bar_plt &lt;- bar_plt + geom_bar() summary(bar_plt) ## data: name, year, month, day, hour, lat, long, pressure, wind, ## type, seasday [2747x11] ## mapping: x = type ## faceting: &lt;ggproto object: Class FacetNull, Facet&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map: function ## map_data: function ## params: list ## render_back: function ## render_front: function ## render_panels: function ## setup_data: function ## setup_params: function ## shrink: TRUE ## train: function ## train_positions: function ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet&gt; ## ----------------------------------- ## geom_bar: width = NULL, na.rm = FALSE ## stat_count: width = NULL, na.rm = FALSE ## position_stack Look at the layer information below ----. The geom_bar function sets the stat to “count”. Counting a categorical variable is analogous to binning a numeric variable. The only difference is that there is no need to specify bin widths because type is categorical, i.e. ggplot2 will sum up the number of observations associated with every category of type. Here’s the resulting figure: bar_plt This is the same summary information we produced using the table function, only now it’s presented in graphical form. We can customise this bar graph if needed with functions like xlab and ylab, and by setting various properties inside geom_bar. For example: ggplot(storms, aes(x = type)) + geom_bar(fill = &quot;orange&quot;, width = 0.7) + xlab(&quot;Storm Type&quot;) + ylab(&quot;Number of Observations&quot;) The only new thing here is that we used the width argument of geom_bar to make the bars a little narrower than the default. There is one slight problem with this graph: the order in which the different groups is presented does not reflect the ordinal scale. This occurs because ggplot2 does not “know” that we want to treat type as a ordinal variable. There is no way for ggplot2 to “guess” the appropriate order, so it uses the alphabetical ordering of the category names to set the order of the bars. To fix this we need to customise the scale associated with the ‘x’ aesthetic. We can start by making a short character vector containing all the category names in the focal variable, ensuring these are listed in the order they need to be plotted in: ords &lt;- c(&quot;Tropical Depression&quot;, &quot;Extratropical&quot;, &quot;Tropical Storm&quot;, &quot;Hurricane&quot;) Keep an eye on the spelling too—R is not forgiving of spelling errors. We use this with the limits argument of the scale_x_discrete function to fix the ordering: ggplot(storms, aes(x = type)) + geom_bar(fill = &quot;orange&quot;, width = 0.7) + scale_x_discrete(limits = ords) + xlab(&quot;Storm Type&quot;) + ylab(&quot;Number of Observations&quot;) We had to use one of the scale_x_YY functions here because we needed to change the way an aesthetic appears. We use scale_x_discrete because ‘discrete’ is gplot2-speak for ‘categorical’, which is what we have mapped to the ‘x’ aesthetic. What else might we change? The categories of type have quite long names, meaning the axis labels are all bunched together. One way to fix this is to make the labels smaller or rotate them via the ‘themes’ system. Here’s an alternative solution: just flip the x and y axes to make a horizontal bar chart. We can do this with the coord_flip function (this is new): ggplot(storms, aes(x = type)) + geom_bar(fill = &quot;orange&quot;, width = 0.7) + scale_x_discrete(limits = ords) + coord_flip() + xlab(&quot;Storm Type&quot;) + ylab(&quot;Number of Observations&quot;) "],
["relationships-between-two-variables.html", "Chapter 22 Relationships between two variables 22.1 Associations between numeric variables 22.2 Associations between categorical variables 22.3 Categorical-numerical associations", " Chapter 22 Relationships between two variables This chapter is about exploring the associations between pairs of variables in a sample. These are called bivariate associations. An association is any relationship between two variables that makes them dependent, i.e. knowing the value of one variable gives us some information about the possible values of the second variable. The main goal of this chapter is to show how to use descriptive statistics and visualisations to explore associations among different kinds of variables. 22.1 Associations between numeric variables 22.1.1 Descriptive statistics Statisticians have devised various different ways to quantify an association between two numeric variables in a sample. The common measures seek to calculate some kind of correlation coefficient. The terms ‘association’ and ‘correlation’ are closely related; so much so that they are often used interchangeably. Strictly speaking correlation has a narrower definition: a correlation is defined by a metric (the ‘correlation coefficient’) that quantifies the degree to which an association tends to a certain pattern. The most widely used measure of correlation is Pearson’s correlation coefficient (also called the Pearson product-moment correlation coefficient). Pearson’s correlation coefficient is something called the covariance of the two variables, divided by the product of their standard deviations. The mathematical formula for the Pearson’s correlation coefficient applied to a sample is: \\[ r_{xy} = \\frac{1}{N-1}\\sum\\limits_{i=1}^{N}{\\frac{x_i-\\bar{x}}{s_x} \\frac{y_i-\\bar{y}}{s_y}} \\] We’re using \\(x\\) and \\(y\\) here to refer to each of the variables in the sample. The \\(r_{xy}\\) denotes the correlation coefficient, \\(s_x\\) and \\(s_y\\) denote the standard deviation of each sample, \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means, and \\(N\\) is the sample size. Remember, a correlation coefficient quantifies the degree to which an association tends to a certain pattern. In the case of Pearson’s correlation coefficient, the coefficient is designed to summarise the strength of a linear (i.e. ‘straight line’) association. We’ll return to this idea in a moment. Pearson’s correlation coefficient takes a value of 0 if two variables are uncorrelated, and a value of +1 or -1 if they are perfectly related. ‘Perfectly related’ means we can predict the exact value of one variable given knowledge of the other. A positive value indicates that high values in one variable is associated with high values of the second. A negative value indicates that high values of one variable is associated with low values of the second. The words ‘high’ and ‘low’ are relative to the arithmetic mean. In R we can use the cor function to calculate Pearson’s correlation coefficient. For example, the Pearson correlation coefficient between pressure and wind is given by: cor(storms$wind, storms$pressure) ## [1] -0.9254911 This is negative, indicating wind speed tends to decline with increasing pressure. It is also quite close to -1, indicating that this association is very strong. We saw this in the Introduction to ggplot2 chapter when we plotted atmospheric pressure against wind speed. The Pearson’s correlation coefficient must be interpreted with care. Two points are worth noting: Because it is designed to summarise the strength of a linear relationship, Pearson’s correlation coefficient will be misleading when this relationship is curved, or even worse, hump-shaped. Even if the relationship between two variables really is linear, Pearson’s correlation coefficient tells us nothing about the slope (i.e. the steepness) of the relationship. If those last two statements don’t make immediate sense, take a close look at this figure: This shows a variety of different relationships between pairs of numeric variables. The numbers in each subplot are the Pearson’s correlation coefficients associated with the pattern. Consider each row: The first row shows a series of linear relationships that vary in their strength and direction. These are all linear in the sense that the general form of the relationship can be described by a straight line. This means that it is appropriate to use Pearson’s correlation coefficient in these cases to quantify the strength of association, i.e. the coefficient is a reliable measure of association. The second row shows a series of linear relationships that vary in their direction, but are all examples of a perfect relationship—we can predict the exact value of one variable given knowledge of the other. What these plots show is that Pearson’s correlation coefficient measures the strength of association without telling us anything the steepness of the relationship. The third row shows a series of different cases where it is definitely inappropriate to Pearson’s correlation coefficient. In each case, the variables are related to one another in some way, yet the correlation coefficient is always 0. Pearson’s correlation coefficient completely fails to flag the relationship because it is not even close to being linear. 22.1.1.1 Other measures of correlation What should we do if we think the relationship between two variables is non-linear? We should not use Pearson correlation coefficient to measure association in this case. Instead, we can calculate something called a rank correlation. The idea is quite simple. Instead of working with the actual values of each variable we ‘rank’ them, i.e. we sort each variable from lowest to highest and the assign the labels ‘first, ’second’, ‘third’, etc. to different observations. Measures of rank correlation are based on a comparison of the resulting ranks. The two most popular are Spearman’s \\(\\rho\\) (‘rho’) and Kendall’s \\(\\tau\\) (‘tau’). We won’t examine the mathematical formula for each of these as they don’t really help us understand them much. We do need to know how to interpret rank correlation coefficients though. The key point is that both coefficients behave in a very similar way to Pearson’s correlation coefficient. They take a value of 0 if the ranks are uncorrelated, and a value of +1 or -1 if they are perfectly related. Again, the sign tells us about the direction of the association. We can calculate both rank correlation coefficients in R using the cor function again. This time we need to set the method argument to the appropriate value: method = &quot;kendall&quot; or method = &quot;spearman&quot;. For example, the Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\) measures of correlation between pressure and wind are given by: cor(storms$wind, storms$pressure, method = &quot;kendall&quot;) ## [1] -0.7627645 cor(storms$wind, storms$pressure, method = &quot;spearman&quot;) ## [1] -0.9025831 These roughly agree with the Pearson correlation coefficient, though Kendall’s \\(\\tau\\) seems to suggest that the relationship is weaker. Kendall’s \\(\\tau\\) is often smaller than Spearman’s \\(\\rho\\) correlation. Although Spearman’s \\(\\rho\\) is used more widely, it is more sensitive to errors and discrepancies in the data than Kendall’s \\(\\tau\\). 22.1.2 Graphical summaries Correlation coefficients give us a simple way to summarise associations between numeric variables. They are limited though, because a single number can never summarise every aspect of the relationship between two variables. This is why we always visualise the relationship between two variables. The standard graph for displaying associations among numeric variables is a scatter plot, using horizontal and vertical axes to plot two variables as a series of points. We saw how to construct scatter plots using ggplot2 in the [Introduction to ggplot2] chapter so we won’t step through the details again. There are a few other options beyond the standard scatter plot. Specifically, ggplot2 provides a couple of different geom_XX functions for producing a visual summary of relationships between numeric variables in situations where over-plotting of points is obscuring the relationship. One such example is the geom_count function: ggplot(storms, aes(x = pressure, y = wind)) + geom_count(alpha = 0.5) The geom_count function is used to construct a layer in which data are first grouped into sets of identical observations. The number of cases in each group is counted, and this number (‘n’) is used to scale the size of points. Take note—it may be necessary to round numeric variables first (e.g. via mutate) to make a usable plot if they aren’t already discrete. Two further options for dealing with excessive over-plotting are the geom_bin_2d and geom_hex functions. The the geom_bin_2d divides the plane into rectangles, counts the number of cases in each rectangle, and then uses the number of cases to assign the rectangle’s fill colour. The geom_hex function does essentially the same thing, but instead divides the plane into regular hexagons. Note that geom_hex relies on the hexbin package, so this need to be installed to use it. Here’s an example of geom_hex in action: ggplot(storms, aes(x = pressure, y = wind)) + geom_hex(bins = 25) Notice that this looks exactly like the ggplot2 code for making a scatter plot, other than the fact that we’re now using geom_hex in place of geom_point. 22.2 Associations between categorical variables 22.2.1 Numerical summaries Numerically exploring associations between pairs of categorical variables is not as simple as the numeric variable case. The general question we need to address is, “do different combinations of categories seem to be under or over represented?” We need to understand which combinations are common and which are rare. The simplest thing we can do is ‘cross-tabulate’ the number of occurrences of each combination. The resulting table is called a contingency table. The counts in the table are sometimes referred to as frequencies. The xtabs function (= ‘cross-tabulation’) can do this for us. For example, the frequencies of each storm category and month combination is given by: xtabs(~ type + month, data = storms) ## month ## type 6 7 8 9 10 11 12 ## Extratropical 27 38 23 149 129 42 4 ## Hurricane 3 31 300 383 152 25 2 ## Tropical Depression 22 59 150 156 84 42 0 ## Tropical Storm 31 123 247 259 204 61 1 The first argument sets the variables to cross-tabulate. The xtabs function uses R’s special formula language, so we can’t leave out that ~ at the beginning. After that, we just provide the list of variables to cross-tabulate, separated by the + sign. The second argument tells the function which data set to use. This isn’t a dplyr function, so the first argument is not the data for once. What does this tell us? It shows us how many observations are associated with each combination of values of type and month. We have to stare at the numbers for a while, but eventually it should be apparent that hurricanes and tropical storms are more common in August and September (month ‘8’ and ‘9’). More severe storms occur in the middle of the storm season—perhaps not all that surprising. If both variables are ordinal we can also calculate a descriptive statistic of association from a contingency table. It makes no sense to do this for nominal variables because their values are not ordered. Pearson’s correlation coefficient is not appropriate here. Instead, we have to use some kind of rank correlation coefficient that accounts for the categorical nature of the data. Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\) are designed for numeric data, so they can’t be used either. One measure of association that is appropriate for categorical data is Goodman and Kruskal’s \\(\\gamma\\) (“gamma”). This behaves just like the other correlation coefficients we’ve looked at: it takes a value of 0 if the categories are uncorrelated, and a value of +1 or -1 if they are perfectly associated. The sign tells us about the direction of the association. Unfortunately, there isn’t a base R function to compute Goodman and Kruskal’s \\(\\gamma\\), so we have to use a function from one of the packages that implements it (e.g. the GKgamma function in the vcdExtra package) if we need it. 22.2.2 Graphical summaries Bar charts can be used to summarise the relationship between two categorical variables. The basic idea is to produce a separate bar for each combination of categories in the two variables. The lengths of these bars is proportional to the values they represent, which is either the raw counts or the proportions in each category combination. This is the same information displayed in a contingency table. Using ggplot2 to display this information is not very different from producing a bar graph to summarise a single categorical variable. Let’s do this for the type and year variables in storms, breaking the process up into two steps. As always, we start by using the ggplot function to construct a graphical object containing the necessary default data and aesthetic mapping: bar_plt &lt;- ggplot(storms, aes(x = year, fill = type)) Notice that we’ve included two aesthetic mappings. We mapped the year variable to the x axis, and the storm category (type) to the fill colour. We want to display information from two categorical variables, so we have to define two aesthetic mappings. The next step is to add a layer using geom_bar (we want a bar plot) and display the results: bar_plt &lt;- bar_plt + geom_bar() bar_plt This is called a stacked bar chart. Each year has its own bar (x = year), and each bar has been divided up into different coloured segments, the length of which is determined by the number of observations associated with each storm type in that year (fill = type). We have all the right information in this graph, but it could be improved. Look at the labels on the x axis. Not every bar is labelled. This occurs because year is stored as a numeric vector in storms, yet we are treating it as a categorical variable in this analysis—ggplot2 has no way of knowing this of course. We need a new trick here. We need to convert year to something that won’t be interpreted as a number. One way to do this is to convert year to a character vector12. Once it’s in this format, ggplot2 will assume that year is a categorical variable. We can convert a numeric vector to a character vector with the as.character function. We could transform year inside aes ‘on the fly’, or alternatively, we can use the mutate function to construct a new version of storms containing the character version of year. We’ll do the latter so that we can keep reusing the new data frame: storms_alter &lt;- mutate(storms, year = as.character(year)) We must load and attach dplyr to make this work. The new data frame storms_alter is identical to storms, except that year is now a character vector. Now we just need to construct and display the ggplot2 object again using this new data frame: ggplot(storms_alter, aes(x = year, fill = type)) + geom_bar() That’s an improvement. However, the ordering of the storm categories is not ideal because the order in which the different groups are presented does not reflect the ordinal scale we have in mind for storm category. We saw this same problem in the Exploring categorical variables chapter—ggplot2 treats does not ‘know’ the correct order of the type categories. Time for a new trick. We need to somehow embed the information about the required category order of type into our data. It turns out that R has a special kind of augmented vector, called a factor, that’s designed to do just this. We make use of this we need to know how to convert something into a factor. We use the factor function, setting its levels argument to be a vector of category names in the correct order: # 1. make a vector of storm type names in the required order storm_names &lt;- c(&quot;Tropical Depression&quot;, &quot;Extratropical&quot;, &quot;Tropical Storm&quot;, &quot;Hurricane&quot;) # 2. now convert year to a character and type to a factor storms_alter &lt;- storms %&gt;% mutate(year = as.character(year), type = factor(type, levels = storm_names)) This may look a little confusing at first glance, but all we did here was create a vector of ordered category names called storm_names, and then use mutate to change type to a factor using the ordering implied by storm_names. Just be careful with the spelling—the values in storm_names must match those in type. We did this with dplyr’s mutate function, again calling the modified data set storms_alter. Once we’ve applied the factor trick we can remake the bar chart: # 3. make the bar plot ggplot(storms_alter, aes(x = year, fill = type)) + geom_bar() Factors Factors are very useful. They crop up all the time in R. Unfortunately, they are also a pain to work with and a frequent source of errors. A complete treatment of factors would require a whole new chapter, so to save space, we’ve just shown one way to work with them via the factor function. This is enough to solve the reordering trick required to get ggplot2 to work the way we want it to, but there’s a lot more to learn about factors. A stacked bar chart is the default produced by geom_bar. One problem with this kind of chart is that it can be hard to spot associations among the two categorical variables. If we want to know how they are associated it’s often better to plot the counts for each combination of categories side-by-side. This isn’t hard to do. We switch to a side-by-side bar chart by assigning a value of &quot;dodge&quot; to the position argument of geom_bar: ggplot(storms_alter, aes(x = year, fill = type)) + geom_bar(position = &quot;dodge&quot;) + labs(x = &quot;Year&quot;, y = &quot;Number of Observations&quot;, fill = &quot;Storm Category&quot;) The position = &quot;dodge&quot; argument says that we want the bars to ‘dodge’ one another along the x axis so that they are displayed next to one another. We snuck in one more tweak. Remember, we can use labs to set the labels of any aesthetic mapping we’ve defined—we used it here to set the label of the aesthetic mapping associated with the fill colour and the x/y axes. This final figure shows that on average, storm systems spend more time as hurricanes and tropical storms than tropical depressions or extratropical systems. Other than that, the story is a little messy. For example, 1997 was an odd year, with few storm events and relatively few hurricanes. 22.3 Categorical-numerical associations We’ve seen how to summarise the relationship between a pair of variables when they are of the same type: numeric vs. numeric or categorical vs. categorical. The obvious next question is, “How do we display the relationship between a categorical and numeric variable?” As usual, there are a range of different options. 22.3.1 Descriptive statistics Numerical summaries can be constructed by taking the various ideas we’ve explored for numeric variables (means, medians, etc), and applying them to subsets of data defined by the values of the categorical variable. This is easy to do with the dplyr group_by and summarise pipeline. We won’t review it here though, because we’re going to do this in the next chapter. 22.3.2 Graphical summaries The most common visualisation for exploring categorical-numerical relationships is the ‘box and whiskers plot’ (or just ‘box plot’). It’s easier to understand these plots once we’ve seen an example. To construct a box and whiskers plot we need to set ‘x’ and ‘y’ axis aesthetics for the categorical and numeric variable, and we use the geom_boxplot function to add the appropriate layer. Let’s examine the relationship between storm category and atmospheric pressure: ggplot(storms_alter, aes(x = type, y = pressure)) + geom_boxplot() + xlab(&quot;Storm category&quot;) + ylab(&quot;Pressure (mbar)&quot;) It’s fairly obvious why this is called a box and whiskers plot. Here’s a quick overview of the component parts of each box and whiskers: The horizontal line inside the box is the sample median. This is our measure of central tendency. It allows us to compare the most likely value of the numeric variable across the different categories. The boxes display the interquartile range (IQR) of the numeric variable in each category, i.e. the middle 50% of observations in each group according to their rank. This allows us to compare the spread of the numeric values in each category. The vertical lines that extend above and below each box are the “whiskers”. The interpretation of these depends on which kind of box plot we are making. By default, ggplot2 produces a traditional Tukey box plot. Each whisker is drawn from each end of the box (the upper and lower quartiles) to a well-defined point. To find where the upper whisker ends we have to find the largest observation that is no more than 1.5 times the IQR away from the upper quartile. The lower whisker ends at the smallest observation that is no more than 1.5 times the IQR away from the lower quartile. Any points that do not fall inside the whiskers are plotted as an individual point. These may be outliers, although they could also be perfectly consistent with the wider distribution. The resulting plot compactly summarises the distribution of the numeric variable within each of the categories. We can see information about the central tendency, dispersion and skewness of each distribution. In addition, we can get a sense of whether there are potential outliers by noting the presence of individual points outside the whiskers. What does the above plot tell us about atmospheric pressure and storm type? It shows that pressure tends to display negative skew in all four storm categories, though the skewness seems to be higher in tropical storms and hurricanes. The pressure values of tropical depression, tropical storm, and hurricane histograms overlap, though not by much. The extratropical storm system seems to be something ‘in between’ a tropical storm and a tropical depression. 22.3.3 Alternatives to box and whiskers plots Box and whiskers plots are a good choice for exploring categorical-numerical relationships. They provide a lot of information about how the distribution of the numeric variable changes across categories. Sometimes we may want to squeeze even more information about these distributions into a plot. One way to do this is to make multiple histograms (or dot plots, if we don’t have much data). We already know how to make a histogram, and we have seen how aesthetic properties such as colour and fill are used to distinguish different categories of a variable in a layer. This suggests that we can overlay more than one histogram on a single plot. Let’s use this idea to see how the sample distribution of wind speed (wind) differs among the storm classes: ggplot(storms_alter, aes(x = wind, fill = type)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.6, binwidth = 5) + xlab(&quot;Wind Speed (mph)&quot;) We define two mappings: the continuous variable (wind) is mapped to the x axis, and the categorical variable (type) is mapped to the the fill colour. Notice that we also set the position argument to &quot;identity&quot;. This tells ggplot2 not to stack the histograms on top of one another. Instead, they are allowed to overlap. It’s for this reason that we also made them semi-transparent by setting the alpha argument. Plotting several histograms in one layer like this places a lot of information in one plot, but it can be hard to make sense of this when the histograms overlap a lot. If the overlapping histograms are too difficult to interpret we might consider producing a separate one for each category. We’ve already seen a quick way to do this. Faceting works well here: ggplot(storms_alter, aes(x = wind)) + geom_histogram(alpha = 0.8, binwidth = 5) + xlab(&quot;Wind Speed (mph)&quot;) + facet_wrap(~ type, ncol = 4) We can see quite a lot in this plot and the last. The tropical depression, tropical storm, and hurricane histograms do not overlap (with a few minor exceptions). These three storm categories are obviously defined with respect to wind speed. Perhaps they represent different phases of one underlying physical phenomenon? The extratropical storm system seems to be something altogether different. In fact, an extratropical storm is a different kind of weather system from the other three. It can turn into a tropical depression (winds &lt; 39 mph) or a subtropical storm (winds &gt; 39 mph), but only a subtropical can turn into a hurricane. We’re oversimplifying, but the point is that the simple ordinal scale that we envisaged for the type variable is probably not very sensible. It’s not really true that an extratropical is “greater than” a subtropical depression (or vice versa). We should probably have characterised type as a nominal variable, although this designation ignores the fact that three of the storm types have a clear ordering. The take home message is that we have to understand our data before we start to really analyse it. This is why exploratory data analysis is so important. The alternative is to convert it to something called a factor. A factor is a special type of vector used by R to encode categorical variables. These are very useful, but we don’t use them in this book because they can be a bit tricky to work with.↩ "],
["building-in-complexity.html", "Chapter 23 Building in complexity 23.1 Multivariate relationships 23.2 Comparing descriptive statistics", " Chapter 23 Building in complexity 23.1 Multivariate relationships We examined various plots that summarise associations between two variables in the last chapter. How do we explore relationships between more than two variables in a single graph? That is, how do we explore multivariate associations? It’s difficult to give a concrete answer to this question, because it depends on the question we’re trying to address, the kinds of variables we’re working with, and to a large extent, our creativity and aptitude with an advanced graphing framework like ggplot2. Nonetheless, we already know enough about how ggplot2 works to build some fairly sophisticated visualisations. There are two ways to add additional information to a visualisation: Define aesthetic mappings to allow the properties of a layer to depend on the different values of one or more variable. Use faceting to construct a multipanel plot according to the values of categorical variables. We can adopt both of these approaches at the same time, meaning we can get information form 4-6 variables into a single graph if we need to (though this does not always produce an easy-to-read plot). We’ve already seen these two approaches used together in the Introduction to ggplot2 chapter. We’ll look at one more example to illustrate the approach again. We want to understand how the sample distribution of wind speed during a storm varies over the course of a year. We also want to visualise how this differs among storm categories. One way to do this is to produce a stacked histogram for each month of the year, where the colour of the stacked histograms changes with respect to storm category. We do this using the facet_wrap function to specify separate panels for each month, colouring the histograms by the type variable. Stacking the histograms happens by default: ggplot(storms_alter, aes(x = wind, fill = type)) + geom_histogram(binwidth = 15) + xlab(&quot;Wind Speed (mph)&quot;) + ylab(&quot;Count&quot;) + labs(fill = &quot;Storm Type&quot;) + facet_wrap(~ month, ncol = 3) Notice that we’re using storms_alter from the last chapter, the version of storms where the type variable was converted to a factor. We haven’t used any new tricks here though. We just set a couple of aesthetics and used faceting to squeeze many histograms onto one plot. It mostly shows that if we’re planning a holiday in Central America we should probably avoid travelling from August to October… 23.2 Comparing descriptive statistics Until now we have been focusing on plots that display either the raw data (e.g. scatter plots), or a summary of the raw data that captures as much detail as possible (e.g. histograms and box plots). We’ve tended to treat descriptive statistics like the sample mean as ‘a number’ to be examined in isolation. These are often placed in the text of a report or in a table. However, there’s nothing to stop us visualising a set of means (or any other descriptive statistics) and a figure is much more informative than than a table. Moreover, many common statistical tools focus on a few aspects of sample distributions (e.g. means and variances) so it’s a good idea to plot these. We need to know how construct graphs that display such summaries. Let’s start with a simple question: how does the (arithmetic) mean wind speed vary across different types of storm? One strategy is to produce a bar plot in which the lengths of the bars represent the mean wind speed in each category. There are two different ways to produce this with ggplot2. The first is simplest, but requires a new ggplot2 trick. When we add a layer using geom_bar we have to set two new arguments. The first is stat = &quot;summary&quot;. This tells ggplot2 not to plot the raw values of the y aesthetic mapping, but instead, to construct a summary of the ‘y’ variable. The second argument is fun.y = mean. This tells ggplot2 how to summarise this variable. The part on the right hand side can be any R function that takes a vector of values and returns a single number. Obviously we want the mean function. See how this works in practice: ggplot(storms_alter, aes(x = type, y = wind)) + geom_bar(stat = &quot;summary&quot;, fun.y = mean) + coord_flip() + xlab(&quot;Storm Category&quot;) + ylab(&quot;Mean Wind Speed (mph)&quot;) We also flipped the coordinates here with coord_flip to make this a horizontal bar plot. We’ve seen this before in the Exploring categorical variables chapter. The only new idea is our use of the stat and fun.y arguments. The second way to build a bar plot showing some kind of summary statistic breaks the problem into two steps. In the first step we have to calculate whatever it is we want to display, i.e. the category-specific mean in this case. This information needs to be stored in a data frame or tibble so dplyr is the best tool to use for this: storms_sum &lt;- storms_alter %&gt;% group_by(type) %&gt;% summarise(mean_wind = mean(wind)) storms_sum ## # A tibble: 4 x 2 ## type mean_wind ## &lt;fctr&gt; &lt;dbl&gt; ## 1 Tropical Depression 27.35867 ## 2 Extratropical 40.06068 ## 3 Tropical Storm 47.32181 ## 4 Hurricane 84.65960 We used group_by and summarise to calculate the set of means, which we called mean_wind. The second step uses the new data frame (called storms_sum) as the default data in a new graphical object, sets x and y aesthetic mappings from type and mean_wind, and adds a layer with geom_col: mean.plt &lt;- ggplot(storms_sum, aes(x = type, y = mean_wind)) + geom_col() + coord_flip() + xlab(&quot;Storm Category&quot;) + ylab(&quot;Mean Wind Speed (mph)&quot;) mean.plt The result is the same as the last plot. Note that we have used geom_col instead of geom_bar here. Remember here that the default behaviour of geom_bar is to count the observations in each category. Using the geom_col function tells it that the information in mean_wind must be plotted ‘as is’ instead. Which approach is better? The first approach is more compact though. We recommend the second long-winded way approach for new users because it separates the summary calculations from the plotting. This way, as long as we’re comfortable with dplyr, we can get away with remembering less about how ggplot2 works. It also makes it a bit easier to fix mistakes, as we can first check whether the right information is in the summary data frame, before we worry about plotting it. The two-step method is easy to extend to different kinds of plots as well. An example will help to clarify what we mean by this. Remember the wind speed vs. atmospheric pressure scatter plots we first produced? One criticism of those plots is that they don’t really summarise differences among storm events. We plotted all the data, which means a storm system that lasts a long time contributes relatively more points to the plot than a short storm. Why not plot the storm-specific means instead? We know how to do this using dplyr and ggplot2. First construct a new data frame containing the means: storms_means &lt;- storms_alter %&gt;% group_by(name) %&gt;% summarise(wind = mean(wind), pressure = mean(pressure)) Then it is just a matter producing a scatter plot with the new data frame. There are no new tricks to learn: ggplot(storms_means, aes(x = pressure, y = wind)) + geom_point(alpha = 0.8) + xlab(&quot;Atmospheric Pressure (mbar)&quot;) + ylab(&quot;Wind Speed (mph)&quot;) "],
["doing-more-with-ggplot2.html", "Chapter 24 Doing more with ggplot2 24.1 Adding error bars 24.2 Adding text to plots 24.3 Customising text 24.4 Saving plots 24.5 Panel plots", " Chapter 24 Doing more with ggplot2 Throughout this block we have learnt a range of ways to plot our data using ggplot2. Here we provide a few more examples of ways you may wish to customise your plots. You will not be examined on the material in this chapter, however it will be helpful when you have to make plots for other modules, such as during your Level 1 projects. We will use the storms data set again from the nasaweather package. As in the Relationships between two variables chapter we will reorder the levels of the type variable so that they get increasingly fierce. # 1. make a vector of storm type names in the required order storm_names &lt;- c(&quot;Tropical Depression&quot;, &quot;Extratropical&quot;, &quot;Tropical Storm&quot;, &quot;Hurricane&quot;) # 2. now convert type to a factor storms_alter &lt;- storms %&gt;% mutate(type = factor(type, levels = storm_names)) 24.1 Adding error bars In the Building in Complexity chapter we learnt how to make a bar chart showing the means of our data. However, we generally want to show how variable the data are as well as the central tendency (e.g. mean) of the data. To do this we can include error bars showing for example the standard deviation or standard error of the mean. We’ll demonstrate this using the storms data set again, by plotting the means and standard errors of wind speed for each storm type. We start by calculating the means and standard deviations for each group. storms_sum &lt;- storms_alter %&gt;% group_by(type) %&gt;% summarise(mean_wind = mean(wind), std = sd(wind)) storms_sum ## # A tibble: 4 x 3 ## type mean_wind std ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Tropical Depression 27.35867 3.520588 ## 2 Extratropical 40.06068 13.246997 ## 3 Tropical Storm 47.32181 11.092150 ## 4 Hurricane 84.65960 18.790856 We can now use this data frame to make the plot, using geom_col to plot the means and the unsurprisingly named geom_errorbar to add the error bars. ggplot(storms_sum, aes(x=type, y = mean_wind)) + # Plot the means geom_col(fill = &quot;orange&quot;) + # Add the error bars geom_errorbar(aes(ymin = mean_wind - std, ymax = mean_wind + std), width = 0.1) + # Flip the axes round to prevent labels overlapping coord_flip() + # Use a more professional theme theme_classic(base_size = 12) + # Change the axes labels xlab(&quot;Storm Category&quot;) + ylab(&quot;Mean Wind Speed (mph)&quot;) The ymin and ymax arguments of the geom_errorbar function give the lower and upper limits of error bars. Here, we have plotted the mean +/- 1 standard deviation. Note that we can change the width of the error bars using the width argument. Also remember if you’re including error bars on a plot that you MUST specify in the figure legend what they show (e.g. standard deviation, standard error of the mean, 95% confidence intervals). 24.2 Adding text to plots There may be some cases in which you want to add text to plots, for example to show the sample size for each group or to show which categories are significantly different from each other if you’ve performed a statistical test (we’ll come back to this at Level 2). Here we’re going to add a label for each bar on our bar chart. To do this we start by adding the labels that we want to use to the data frame. For example, here we will calculate the mean (using the mean function) and the sample size (using the function n) for each group. storms_sum &lt;- storms %&gt;% group_by(type) %&gt;% summarise(mean_wind = mean(wind), samp = n()) storms_sum ## # A tibble: 4 x 3 ## type mean_wind samp ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Extratropical 40.06068 412 ## 2 Hurricane 84.65960 896 ## 3 Tropical Depression 27.35867 513 ## 4 Tropical Storm 47.32181 926 Then we can add the text showing the sample size to our plot using the function geom_text. ggplot(storms_sum, aes(x = type, y = mean_wind)) + # Add the bars geom_col(fill = &quot;orange&quot;) + # Flip the axes round coord_flip() + # Change the axes labels xlab(&quot;Storm Category&quot;) + ylab(&quot;Mean Wind Speed (mph)&quot;) + # Add the text geom_text(aes(label = samp, y = 10)) + # Use a more professional theme theme_classic(base_size = 12) 24.3 Customising text Sometimes we may want to change the appearance of the text on the plot. For example, sometimes if the axis labels are quite long they may be bunched together or overlap each other, making it difficult to read them. We saw this before in the Exploring Categorical Variables chapter. ggplot(storms_alter, aes(x = type)) + geom_bar(fill = &quot;orange&quot;, width = 0.7) + xlab(&quot;Storm Type&quot;) + ylab(&quot;Number of Observations&quot;) Here it is very difficult to read the categories on the \\(x\\) axis as the text is overlapping. In the Exploring Categorical Variables chapter we saw one way to deal with this, by using coord_flip to rotate the axes. An alternative to this is to change the size of text - a simple way to do this is to use the base_size argument within a ggplot theme_XX function as follows: ggplot(storms, aes(x = type)) + geom_bar(fill = &quot;orange&quot;, width = 0.7) + xlab(&quot;Storm Type&quot;) + ylab(&quot;Number of Observations&quot;) + theme_classic(base_size = 10) The base_size argument changes the size of all of the text within the plot. It is also possible to rotate the labels themselves rather than the whole plot. Here, we use the angle argument of the element_text function again inside theme. ggplot(storms, aes(x = type)) + geom_bar(fill = &quot;orange&quot;, width = 0.7) + xlab(&quot;Storm Type&quot;) + ylab(&quot;Number of Observations&quot;) + theme(axis.text.x = element_text(angle = 90)) Here we used the argument ‘axis.text.x’ so that only the labels on the \\(x\\) axis were rotated. 24.4 Saving plots When using RStudio plots can be saved using the Export button. However, such plots are often pixelated. R also has a range of functions that can be used to save plots. When making figures with ggplot we can use the ggsave function. For example, here we will create a scatter plot using the storms data set again. ggplot(storms, aes(x = pressure, y = wind)) + # Add the points geom_point() + # Change the axis labels labs(x=&quot;Atmospheric pressure (mbar)&quot;, y = &quot;Wind speed (mph)&quot;) Once you’re happy with the plot you can use the ggsave function to save it as follows: ggsave(&quot;Stormsplot.pdf&quot;, height = 5, width = 5) The first argument that this function takes is the name of the file that you will save. By default ggsave will save the last plot that you’ve made. You can also provide the name of a plot as the second argument to the function if you have assigned it a name. Note that R will save the plot to your working directory (you can change where the plot is saved to using the path argument in the ggsave function). Note that if you do not specify the width and height arguments to ggsave it will use the current size of your plotting window. You can also add the ggsave function on to the code for a specific plot ggplot(storms, aes(x = pressure, y = wind)) + # Add the points geom_point() + # Change the axis labels labs(x=&quot;Atmospheric pressure (mbar)&quot;, y = &quot;Wind speed (mph)&quot;) + # Save the figure ggsave(&quot;Stormsplot.pdf&quot;, height = 5, width = 5) 24.5 Panel plots We have already seen how the facet_wrap function can be used to produce multiple panels in the Introduction to ggplot2 chapter. This function can be used where you want to make multiple plots each showing a different level of a factor. However, sometimes you may wish to present a multi-panel plot using different variables in the different panels. There are multiple ways to do this, we’re going to show you one using the cowplot package. First make sure that this package is installed (if you haven’t used it before) and loaded (every time you use it). Then make the individual plots that you want to include your multi-panel plot using ggplot as normal. For example we might want to look at a) the relative frequency of different storm types occuring and b) the mean wind speed associated with each storm type. First we make these two plots that we want to include in the panel and assign these to names. plta &lt;- ggplot(storms, aes(x = type)) + geom_bar(fill = &quot;orange&quot;) + xlab(&quot;Storm Type&quot;) + ylab(&quot;Number of Observations&quot;) + theme_classic(base_size = 10) pltb &lt;- ggplot(storms_sum, aes(x=type, y = mean_wind)) + # Plot the means geom_col(fill = &quot;orange&quot;) + # Change the axes labels xlab(&quot;Storm Category&quot;) + ylab(&quot;Mean Wind Speed (mph)&quot;) + theme_classic(base_size = 10) Then we can use the plot_grid function from the cowplot package to create the multi-panel plot. plot_grid(plta, pltb, nrow = 1, labels = c(&quot;auto&quot;), label_size = 10) The plot_grid function allows the panel to be customised easily, for example by changing the number of plots in each row (nrow argument) and including labels for each panel (labels argument). We can then use the ggsave function to save our multi-panel plot as before. # Create the multi-panel plot plot_grid(plta, pltb, nrow = 1, labels = c(&quot;auto&quot;), label_size = 10) + # Save it ggsave(&quot;Stormsplot.pdf&quot;, height = 4, width = 8) "]
]
